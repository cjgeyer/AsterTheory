
\chapter{Families}

\section{Bernoulli}
\label{sec:bernoulli}

A random variable is \emph{Bernoulli} if its possible values are zero and one.
In other words, every Bernoulli random variable is zero-or-one-valued,
and vice versa.

This is the \emph{rationale} for the distribution, any dichotomous (two-valued)
random variable can be coded as Bernoulli.

This is a \emph{discrete} random variable.

This is a special case of the binomial distribution, which we do next.

\section{Binomial}
\label{sec:binomial}

A random variable is \emph{binomial} if it is the sum of IID Bernoulli
random variables.  Hence the Bernoulli distribution is the binomial
distribution for sample size one (for one term in the sum).

The \emph{probability mass function} is
\begin{equation} \label{eq:binomial-pmf}
   f(y) = \binom{n}{y} p^y (1 - p)^{n - y}, \qquad y = 0, 1, \ldots, n,
\end{equation}
where $p$ is the \emph{usual parameter}, the probability that any of the
$n$ Bernoulli random variables in the sum is equal to one.

The \emph{mean} and \emph{variance} are
\begin{align*}
   E(y) & = n p
   \\
   \var(y) & = n p (1 - p)
\end{align*}

This is an \emph{exponential family}.  From \eqref{eq:binomial-pmf}
the log likelihood is
$$
   l(\theta) = y \log(p) + (n - y) \log(1 - p)
   = y \cdot \log\left(\frac{p}{1 - p}\right) + n \log(1 - p)
$$
from which we see that we have an exponential family with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log\left(\frac{p}{1 - p}\right)
$$
The right-hand side is so important that it is given a name.  The
$\logit$ function (pronounced low-jit) is given by
$$
   \logit(p) = \log\left(\frac{p}{1 - p}\right), \qquad 0 < p < 1.
$$
Its inverse function is
$$
   \logit^{-1}(\theta) = \frac{e^\theta}{1 + e^\theta},
   \qquad - \infty < \theta < \infty.
$$

The \emph{cumulant function} is
\begin{align*}
   c(\theta)
   & =
   - n \log(1 - p)
   \\
   & =
   - n \log\left(1 - \frac{e^\theta}{1 + e^\theta}\right)
   \\
   & =
   - n \log\left(\frac{1}{1 + e^\theta}\right)
   \\
   & =
   n \log\left(1 + e^\theta\right)
\end{align*}
Note that, as required for any sum of IID random variables, the cumulant
function for sample size $n$ is $n$ times the cumulant function for
sample size one (Section~\ref{sec:iid} above).

We check that this has the correct derivatives
$$
   c'(\theta) = \frac{n e^\theta}{1 + e^\theta} = n p
$$
and
$$
   c''(\theta)
   =
   \frac{n e^\theta}{1 + e^\theta}
   - \frac{n e^\theta e^\theta}{(1 + e^\theta)^2}
   =
   \frac{n e^\theta}{1 + e^\theta}
   \left[ 1 - \frac{e^\theta}{1 + e^\theta} \right]
   =
   n p (1 - p)
$$

The \emph{mean value parameter} is $\xi = n p$.

The \emph{canonical parameter space} is the range of the $\logit$ function,
which is the whole real line, $- \infty < \theta < \infty$.

The \emph{mean value parameter space} is $n$ times the domain
of the $\logit$ function $0 < \xi < n$.

Theorem~\ref{th:completion-fundamental} says limiting conditional models
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[0, n]$, and its
boundary consists of two points $0$ and $n$.

Thus there are two limiting conditional models, one of which contains only
the distribution concentrated at zero and one of which contains only
the distribution concentrated at $n$.

In one-dimensional space there are only two directions.  Every positive
vector points in the same direction and gives the same LCM.  Every negative
vector points in the same direction and gives the same LCM.  (And, of course,
the zero vector points in no direction and gives the original model back
as the LCM corresponding to it.)

As discussed in Theorem~\ref{th:cumfun-lcm} above and its following comments,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.

So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y = 0)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log (1 - p)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log \left(\frac{1}{1 + e^\theta}\right)
   \\
   & =
   0
\end{align*}
and
\begin{align*}
   c_{+ 1}(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y = n)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log (p)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log \left(\frac{e^\theta}{1 + e^\theta}\right)
   \\
   & =
   n \theta
\end{align*}

\emph{Addition rule:}
the sum of $m$ independent and identically distributed
binomial random variables with sample size $n$ and usual parameter $p$
has the binomial distribution with sample size $m n$ and usual parameter $p$.

Hence if $y_{p(j)} \longrightarrow y_j$ is a binomial arrow for sample size $n$
the conditional distribution of $y_j$ given $y_{p(j)}$ is binomial
for sample size $n y_{p(j)}$.

This family is not implemented in either R package \code{aster} or
R package \code{aster2}.  Only the $n = 1$ special case, the Bernoulli
family is implemented.

\section{Poisson}
\label{sec:poisson}

A random variable is \emph{Poisson} if it has
the \emph{probability mass function}
\begin{equation} \label{eq:poisson-pmf}
   f(y) = \frac{\xi^y}{y !} e^{- \xi}, \qquad y = 0, 1, 2, \ldots,
\end{equation}
where $\xi$ is the \emph{usual parameter}, which turns out to be the
mean and variance of the distribution, hence also the mean-value parameter.

This is a \emph{discrete} random variable.

There are two rationales for this distribution, both so closely related
that they are almost one rationale.  First, the Poisson distribution is
an approximation to the $\text{binomial}(n, p)$ distribution when $n$
is very large and $p$ is very small and the mean $n p$ is moderate sized.
An example is a lottery.  Every week millions, sometimes hundreds of millions
of tickets are sold (that's $n$), the probability of any one ticket winning
is very small --- for example, for the Powerball lottery, the probability is
one over 292,201,338 (as we write this, the rules change from time to time) ---
(that's $p$), and $n p$ is moderate sized.  In weeks where the jackpot is small
and few tickets are sold, there are still tens of millions of tickets sold,
so $n p$ is less than one but not very small.  In weeks where the jackpot
is large, there may be many hundreds of millions
of tickets sold, so $n p$ is greater
than one and multiple winners are expected (they split the jackpot among them).
But regardless, the distribution of the number of winners is well approximated
by the $\text{Poisson}(n p)$ distribution.

Before we can discuss the second rationale, we discuss the \emph{addition rule:}
the sum of independent Poisson random variables is again Poisson.
It is not required that the independent Poisson random variables be
identically distributed.  Since the expectation of a sum is the sum of the
expectations, the sum of independent Poisson random variables having
means $\xi_1$, $\ldots,$ $\xi_n$ has
the $\text{Poisson}(\xi_1 + \cdots + \xi_n)$ distribution.

It follows (not obviously, but the derivation can be found in books
about spatial point processes) that the sum of $n$ independent Bernoulli
random variables is well approximated by a Poisson distribution provided
$n$ is very large and the means of all of the Bernoulli random variables
are very much smaller than the mean of the Poisson random variable.
Again, if the means of the Bernoulli random variables
are $\xi_1$, $\ldots,$ $\xi_n$, then the mean of the Poisson random variable
is $\xi_1 + \cdots + \xi_n$.  So we are assuming that each $\xi_i$ is very
much smaller than the sum.  To return to our lottery example, it does not
matter that each player is playing the same game.  So long as the expectation
of any one ticket winning is negligible compared to the expected number of
winners (for all tickets), the distribution of the number of winners will
be approximately Poisson.

Let's take a biological example.  Suppose we are counting ants, and we
have divided up the region in which we are counting ants with a very fine
grid.  If our grid is fine enough, the probability of counting more than one
ant in a grid cell will be negligible, perhaps impossible (if our grid cells
are so small that more than one ant could not fit).  Then the number of
ants in any one cell is a Bernoulli (zero-or-one-valued) random variable,
and the number of ants in any region that contains a very large number
of grid cells is very well approximated by the Poisson distribution.
If we take the limit as the size of the grid cells goes to zero we
get exact Poisson distributions.  Except that we forgot to mention
independence.  This assumes the Bernoulli random variables are independent,
that where one ant is has nothing whatsoever to do with where any other ant
is.  If we can accept this independence assumption, then the count of
ants in any region of any size large enough to have a moderate sized
expected number of ants can be assumed Poisson.

Now we abstract away from ants to be counting any things in regions of
any dimension.  The number of stars visible to the naked eye in a region
of sky, the number of raisins in slice of carrot cake, the number of white
blood cells in a drop of blood on a microscope slide, the number of ants
in a square meter region of your back yard, the number of leaves on a tree,
the number of calls arriving at a call center in a specified time interval,
and many other things can be assumed Poisson.

The independence assumption is crucial, pheromone trails and perhaps other
phenomena may make our counts of ants noticeably non-Poisson.  But if it
can be plausibly asserted that the probability of any one thing being counted
is independent of all the other things counted or not counted, then the
distribution of the total count is Poisson.

And even if the distribution of a count random variable fails to be exactly
Poisson due to some failure of the independence assumption, the Poisson
distribution may still may be a pretty good approximation (or may fail badly
if the independence assumption is grossly wrong).

As stated above, the \emph{mean} and \emph{variance} are
\begin{align*}
   E(y) & = \xi
   \\
   \var(y) & = \xi
\end{align*}

This is an \emph{exponential family}.  From \eqref{eq:poisson-pmf}
the log likelihood is
$$
   l(\theta) = y \log(\xi) - \xi
$$
(the term $\log(y !)$ can be dropped because it does not contain the
parameter), from which we see that we have an exponential family with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(\xi),
$$
so
$$
   \xi = e^{\theta}.
$$

The \emph{cumulant function} is
$$
   c(\theta) = \xi = e^\theta
$$
We check that this has the correct derivatives (and this is trivial)
$$
   c'(\theta) = e^\theta = \xi
$$
and
$$
   c''(\theta) = e^\theta = \xi
$$

The \emph{mean value parameter} is also the usual parameter $\xi$.

The \emph{canonical parameter space} is the range of the $\log$ function,
which is the whole real line, $- \infty < \theta < \infty$.

\begin{sloppypar}
The \emph{mean value parameter space} is the domain of the log function
\mbox{$0 < \xi < \infty$}.
\end{sloppypar}

\begin{sloppypar}
\emph{Thinning rule:} in the following graph
$$
\begin{CD}
   y_1 @>\text{Poi}>> y_2 @>\text{Ber}>> y_3
\end{CD}
$$
the conditional distribution of $y_3$ given $y_1$ (both arrows combined)
is $\text{Poisson}(\xi_3 \xi_2)$.  A thinned Poisson process is another
Poisson process, where ``thinning'' means we take each ``point'' counted
and accept or reject it independently with the same probability.
\end{sloppypar}

As discussed at the end of the preceding section, LCM
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[0, \infty)$, and its
boundary consists of the single point $0$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at zero.

Also as discussed at the end of the preceding section,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y = 0)
   \\
   & =
   e^\theta 
   +
   \log (e^{- \xi})
   \\
   & =
   e^\theta 
   - \xi
   \\
   & =
   0
\end{align*}
So, again as in the preceding section, the cumulant function for the LCM
concentrated at zero is the zero function.

As mentioned in Section~\ref{sec:infinitely-divisible} above,
the Poisson distribution is infinitely divisible.
This is easily verified from its cumulant function.
For any positive real number $r$
$$
   r c(\theta) = r e^{\theta} = e^{\theta + \log(r)}
$$
is a cumulant function.  In fact, it is a cumulant function for the Poisson
family.  One log likelihood for the Poisson family is
$$
   l(\theta) = y \theta - e^\theta
$$
but if we make the substitution $\theta = \psi + \log(r)$ we get
$$
   l(\psi) = y \psi + y \log(r) - e^{\psi + \log(r)}
$$
and we can drop the term that does not contain the new parameter $\psi$
obtaining
$$
   l(\psi) = y \psi - e^{\psi + \log(r)}
$$
and we see this has exponential family form with canonical statistic $y$,
canonical parameter $\psi$,
and cumulant function $c(\psi) = e^{\psi + \log(r)}$.

This is just a special case of the fact,
noted without proof in Section~\ref{sec:define-expfam},
that adding a constant to a canonical parameter gives
another canonical parameter.

Another way of thinking about this fact is that our new parameterization
just puts an offset $\log(r)$ in the exponential family.
But we know from Section~\ref{sec:canonical-affine-submodel} above
that canonical affine submodels of full exponential families are again
exponential families.

Note that in going from Section~\ref{sec:bernoulli}
to Section~\ref{sec:binomial} we just went from the family having cumulant
function $c(\theta) = 1 + e^\theta$ to the family
having cumulant function $n c(\theta)$,
something we know from Section~\ref{sec:iid} above is always valid.
So we might think that we would need another section to go from the
family having cumulant function $c(\theta) = e^\theta$ to the family
having cumulant function $r c(\theta)$, which is valid only when the
family is infinitely divisible.  But we have just found that that does
not give us a new family, but rather the same old Poisson family
(with an offset), so we do not need a new section for a new family.

\section{Zero-Truncated Poisson}
\label{sec:zero-truncated-poisson}

The \emph{zero-truncated Poisson} distribution is the Poisson distribution
conditioned on being nonzero.

The \emph{rationale} is that it can be used to incorporate zero-inflated
Poisson random variables into aster models.

This is a \emph{discrete} distribution.

If $f$ is the PMF of the Poisson distribution, then the PMF of
the zero-truncated Poisson distribution is
\begin{equation} \label{eq:zero-truncated-poisson-pmf-in-terms-of-poisson}
   g(y) = \frac{f(y)}{1 - f(0)}, 
   \qquad y = 1, 2, \ldots,
\end{equation}
that is, if $m$ is the mean of the untruncated Poisson distribution, then
the PDF of the zero-truncated Poisson distribution is
\begin{equation} \label{eq:zero-truncated-poisson-pmf}
   g(y) = \frac{m^y e^{- m}}{y ! (1 - e^{- m})}, 
   \qquad y = 1, 2, \ldots.
\end{equation}

Since this is not a ``brand name distribution'' the mean and variance
cannot just be looked up.  In aid of this calculation we prove a rather
trivial general theorem.
\begin{theorem} \label{th:truncated-mean-variance}
Suppose $X$ is a nonnegative-integer-valued random variable,
and $Y$ is the corresponding zero-truncated random variable.  Then
$$
   E(Y^k) = E(X^k) / \Pr(X > 0)
$$
for any positive integer $k$.
\end{theorem}
\begin{proof}
For this proof let $f$ denote the PMF of $X$ and $g$ the PMF of $Y$, so
the relationship between the two is given
by \eqref{eq:zero-truncated-poisson-pmf-in-terms-of-poisson}
even though we are no longer assuming $X$ is Poisson.  Then
\begin{align*}
   E(Y^k)
   & =
   \sum_{y = 1}^\infty y^k g(y)
   \\
   & =
   \frac{1}{1 - f(0)} \sum_{x = 1}^\infty x^k f(x)
   \\
   & =
   \frac{1}{1 - f(0)} \sum_{x = 0}^\infty x^k f(x)
   \\
   & =
   \frac{E(X)}{1 - f(0)}
   \\
   & =
   \frac{E(X)}{\Pr(X > 0)}
\end{align*}
where the third equality is the fact that the $x = 0$ term in the sum
is equal to zero.
\end{proof}
Together with
\begin{align*}
   \var(Y) & = E(Y^2) - E(Y)^2
   \\
   E(Y^2) & = \var(Y) + E(Y)^2
\end{align*}
which are well known from elementary probability theory, we can use the
theorem to calculate the mean and variance of zero-truncated random variables.

For the Poisson distribution, we have $E(X) = \var(X) = m$
so $E(X^2) = m + m^2$, so
\begin{equation} \label{eq:mean-of-zero-truncated-Poisson}
   E(Y)
   =
   \frac{E(X)}{\Pr(X > 0)}
   =
   \frac{m}{1 - e^{- m}}
\end{equation}
and
\begin{equation} \label{eq:variance-of-zero-truncated-Poisson}
\begin{split}
   \var(Y)
   & =
   E(Y^2) - E(Y)^2
   \\
   & =
   \frac{E(X^2)}{\Pr(X > 0)} - \left(\frac{E(X)}{\Pr(X > 0)}\right)^2
   \\
   & =
   \frac{m + m^2}{1 - e^{- m}} - \left( \frac{m}{1 - e^{- m}} \right)^2
\end{split}
\end{equation}
% Mathematica
% dist = PoissonDistribution[m]
% f[y_] = PDF[dist, y]
% g[y_] = f[y] / (1 - f[0])
% Sum[ g[y], {y, 1, Infinity} ]
% moo = Sum[ y g[y], {y, 1, Infinity} ]
% moo - m / (1 - f[0])
% Simplify[%]
% voo = Sum[ (y - moo)^2 g[y], {y, 1, Infinity} ]
% voo - ((m + m^2) / (1 - f[0]) - (m / (1 - f[0]))^2)
% Simplify[%]

This is an \emph{exponential family}.
From \eqref{eq:zero-truncated-poisson-pmf} the log likelihood is
$$
   l(\theta) = y \log(m) - m - \log(1 - e^{- m})
$$
(the term $\log(y !)$ can be dropped because it does not contain the
parameter), from which we see that we have an exponential family with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(m),
$$
so
$$
   m = e^{\theta},
$$
the relation between $\theta$ and $m$ being the same as for the Poisson
distribution.

But the \emph{usual parameter} $m$ is not the \emph{mean value parameter},
which is
\begin{equation} \label{eq:zero-truncated-poisson-theta-to-xi}
   \xi = \frac{m}{1 - f(0)} = \frac{m}{1 - e^{- m}}
   = \frac{\exp(\theta)}{1 - \exp(- \exp(\theta))}
\end{equation}
as we know from general exponential family theory,
%%%%%%%%%% NEED BACKWARD REFERENCE to mean value parameterization %%%%%%%%%%
the mapping $\theta \mapsto \xi$ given by the formula above is
strictly increasing and invertible and both it and its inverse mapping
$\xi \to \theta$ are infinitely differentiable.  But in this case
the inverse mapping $\xi \to \theta$ seems to have no closed-form expression.
% can Mathematica find closed-form expression?
% using stuff from above
% foo[m_] = moo
% bar[theta_] = foo[Exp[theta]]
% Solve[ bar[theta] == xi, theta ]
% Nope!  It has no clue.
The map $\xi \to \theta$ is what is called a \emph{link function} in the
terminology of generalized linear models (GLM).  The failure of some
families to have link functions in useful form is one reason why aster
model theory and practice never mentions link functions.  They make
sense for some families but not others.

The \emph{cumulant function} is
\begin{equation} \label{eq:zero-truncated-poisson-cumfun}
   c(\theta) = m + \log(1 - e^{- m})
   = e^\theta + \log(1 - \exp(- \exp(\theta)))
\end{equation}
We check that this has the correct derivatives
\begin{align*}
   c'(\theta)
   & =
   e^\theta 
   +
   \frac{\exp(- \exp(\theta)) \exp(\theta)}{1 - \exp(- \exp(\theta))}
   \\
   & =
   m + \frac{m e^{- m}}{1 - e^{- m}}
   \\
   & =
   \frac{m}{1 - e^{- m}}
\end{align*}
and
\begin{align*}
   c''(\theta)
   & =
   e^\theta 
   +
   \frac{\exp(- \exp(\theta)) \exp(\theta)}{1 - \exp(- \exp(\theta))}
   -
   \frac{\exp(- \exp(\theta)) \exp(\theta)^2}{1 - \exp(- \exp(\theta))}
   \\
   & \quad
   -
   \frac{\exp(- \exp(\theta))^2 \exp(\theta)^2}{(1 - \exp(- \exp(\theta)))^2}
   \\
   & =
   m + \frac{m e^{- m}}{1 - e^{- m}}
   - \frac{m^2 e^{- m}}{1 - e^{- m}}
   - \frac{m^2 e^{- 2 m}}{(1 - e^{- m})^2}
\end{align*}
% Mathematica
% voo - (m + m Exp[- m] / (1 - Exp[- m]) - m^2 Exp[- m] / (1 - Exp[- m]) -
%     m^2 Exp[- m]^2 / (1 - Exp[- m])^2)
% Simplify[%]
and this does simplify to be equal to our other expression for variance.

Two other formulas for the variance are also useful \citep{geyer-3701}.
\begin{subequations}
\begin{align}
   \var(y) & = \xi (1 + m - \xi)
   \label{eq:first-convenient-variance-formula}
   \\
   & = \xi (1 - \xi e^{- m})
   \label{eq:second-convenient-variance-formula}
\end{align}
\end{subequations}
% Mathematica
% voo - moo (1 + m - moo)
% Simplify[%]
% voo - moo (1 - moo Exp[- m])
% Simplify[%]
As $\theta \to - \infty$ and $m \to 0$ the mean value parameter $\xi$ converges
(using L'Hospital's rule) to
$$
   \lim_{m \to 0} \frac{m}{1 - e^{- m}} = \lim_{m \to 0} \frac{1}{e^{m}} = 1
$$
and \eqref{eq:first-convenient-variance-formula} shows the variance converges
to zero as $m \to 0$ and $\xi \to 1$.
As $\theta \to \infty$ and $m \to \infty$ the mean value parameter $\xi$
is approximately equal to $m$ because $f(0) = e^{- m}$ is approximately zero.
Then $\xi e^{- m}$ is small compared to one, and
and \eqref{eq:second-convenient-variance-formula} shows the variance is
also approximately equal to $\xi \approx m$.

As we said above, the \emph{mean value parameter} $\xi$ is not
the usual parameter $m$.

As can be seen from that fact that \eqref{eq:zero-truncated-poisson-cumfun}
is finite for all $\theta$,
the \emph{canonical parameter space} is
the whole real line, $- \infty < \theta < \infty$.

As we saw when discussing variance formulas, $\xi \to 1$ as $m \to 0$.
Thus the lower end of the mean value parameter space is one.
And from $m$ being the mean of a Poisson distribution so $m$ has
no upper bound, and from $m \approx \xi$ when either is large, we see that
$\xi$ also has no upper bound.  Thus
the \emph{mean value parameter space} is $1 < \xi < \infty$.

As discussed at the end of the two preceding sections, LCM
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[1, \infty)$, and its
boundary consists of the single point $1$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at one.

Also as discussed at the end of the two preceding sections,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y = 1)
   \\
   & =
   m + \log(1 - e^{- m})
   +
   \log \left(\frac{m e^{- m}}{1 - e^{- m}}\right)
   \\
   & =
   \log(m)
   \\
   & =
   \theta
\end{align*}
So the cumulant function of the distribution concentrated at one is the
identity function.

This just happens to agree with the $n = 1$ case for the binomial
distribution (Section~\ref{sec:binomial} above), but it need not have.
It all depends on how we defined the cumulant functions for these families
in the first place.  We could have added different arbitrary constants
to the cumulant functions of these families and they would still be
cumulant functions.

\section{Normal Location}

\begin{sloppypar}
The univariate normal distribution has \emph{probability density function}
(PDF)
\begin{equation} \label{eq:normal-pdf}
   f(y) = \frac{1}{\sqrt{2 \pi} \sigma} e^{- \frac{(y - \xi)^2}{2 \sigma^2}},
   \qquad - \infty < y < \infty.
\end{equation}
\end{sloppypar}

This is a \emph{continuous} random variable;
except when incorporated into an aster model, it is a mixture of discrete
and continuous.  For a normal-location arrow, when the predecessor is zero
the conditional distribution of the successor is the degenerate random variable
concentrated at zero, which is discrete, and when the predecessor is greater
than zero, the conditional distribution of the successor is continuous.

The \emph{rationale} is the celebrated central limit theorem,
or more precisely, theorems, because there are many variants.
In non-technical terms these theorems say that a random variable that is the sum
of a large number of random variables that are not too dependent,
not too heavy tailed, and not too unequal in size will be well approximated
by a normal distribution.  (If the random variable in question is the
sum of a large number of independent random variables, then Lindeberg's
central limit theorem using Lindeberg's condition specifies what
``not too heavy tailed, and not too unequal in size'' means.
If the random variable in question is the
sum of the components of a dependent stochastic process, then various
stationary process central limit theorems, Markov chain central limit theorems,
and the martingale central limit theorem, give various notions of what
``not too dependent'' means.)
\begin{quotation}
Everybody believes in the law of errors, the experimenters because they
think it is a mathematical theorem, the mathematicians because they
think it is an experimental fact.
\\
\hspace*{\fill} --- Lippman, quoted by Poincar\'{e}, quoted by \citet{cramer}
\end{quotation}
``The law of errors'' is an old name for the normal distribution.
It has also been named after de Moivre, Laplace, and Gauss.
The term ``normal distribution'' was popularized by K. Pearson in the early
twentieth century.  Like the term ``law of errors'' it builds into the name
the idea that it is the main, principle, or only distribution for random data.
Also note the Lippman quote is sarcastic.  Justification
for this belief was always known to be shaky.
(Harald Cram\'{e}r and Henri Poincar\'{e} are, of course, famous.  It is
unclear who the Monsieur Lippman was that Poincar\'{e} attributed this to.)

Since the nonparametrics revolution \citep{hollander-wolfe-chicken},
the exploratory data analysis revolution \citep{tukey},
the bootstrap revolution \citep{efron-tibshirani,davison-hinkley},
and the robustness revolution \citep{huber-ronchetti,hampel-et-al}
no user of statistics aware of these developments wants to blindly
assume normality, especially when it can be demonstrated to be grossly
incorrect using any of these tools.  But the normal distribution may
fit data well, so it continues to be used.  It just is no longer considered
the only distribution for data, as it was before 1950 (mostly, there was
the chi-square test for contingency tables).

The other rationale for this distribution (which has nothing to do with
aster models) is that the usual assumption of homoscedastic normal errors
for linear models makes the distribution of point estimates exactly normal
and the distribution of various test statistics exactly $t$ or exactly $F$.
This rationale is often attributed to Gauss and is why the normal distribution
is sometimes called Gaussian, because Gauss independently co-invented the
method of least squares and more-or-less gave this rationale (more-or-less
because his discussion was Bayesian rather than frequentist), but of course
this was a century before the $t$ and $F$ distributions were invented.

The \emph{mean} and \emph{variance} are
\begin{align*}
   E(y) & = \xi
   \\
   \var(y) & = \sigma^2
\end{align*}
When used in R package \code{aster} every family must be a one-parameter
exponential family of distributions, so when we consider this as such a family
we must pick one parameter to be treated as unknown
and the other parameter to be treated as known.
Because the location parameter $\xi$ is the mean value parameter,
we pick this to be the unknown parameter.

With this understanding, the log likelihood is
$$
   l(\theta) = - \frac{(y - \xi)^2}{2 \sigma^2}
$$
(the term $\sqrt{2 \pi} \sigma$ can be dropped because it does not contain the
unknown parameter $\xi$.  If we expand the quadratic, we get
$$
   l(\theta)
   =
   - \frac{y^2}{2 \sigma^2}
   + \frac{y \xi}{\sigma^2}
   - \frac{\xi^2}{2 \sigma^2}
$$
and can now drop another term not containing $\xi$ obtaining
$$
   l(\theta)
   =
   \frac{y \xi}{\sigma^2} - \frac{\xi^2}{2 \sigma^2}
$$
from which we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \frac{\xi}{\sigma^2}
$$
so the \emph{mean value parameter} is
$$
   \xi = \sigma^2 \theta
$$
The \emph{cumulant function} is
$$
   c(\theta) = \frac{\xi^2}{2 \sigma^2}
   = \frac{\sigma^2 \theta^2}{2}
$$
We check that this has the correct derivatives
$$
   c'(\theta) = \sigma^2 \theta = \xi
$$
and
$$
   c''(\theta) = \sigma^2
$$

\emph{Addition rule:} the sum of $n$ independent and identically distributed
normal random variables with mean $\xi$ and variance $\sigma^2$ has the normal
distribution with mean $n \xi$ and variance $n \sigma^2$.

\emph{General Addition rule:} any sum of independent
normal random variables is again normal
(identically distributed is not required), but this has no application
in aster model theory.

There are no limit degenerate distributions.
This is because the boundary of the closed convex support,
which is the interval $(- \infty, + \infty)$ is empty.
We can never observe data on the boundary.

\section{Negative Binomial}
\label{sec:negative-binomial}

\subsection{Basics}
\label{sec:negative-binomial-basics}

According to the \code{help("NegBinomial")} in R, the negative binomial
distribution has \emph{probability mass function}
\begin{equation} \label{eq:negative-binomial-pmf}
   f(y) = \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!} p^\alpha (1-p)^y,
   \qquad y = 0, 1, 2, \ldots.
\end{equation}
where $\alpha > 0$ is the shape parameter and $0 < p \le 1$ is the
usual parameter (success probability).  The case $\alpha = 1$ is the
geometric distribution.

The \emph{first rationale} for this distribution is inverse sampling,
and for this rationale $\alpha$ must be a positive integer.
If one has an infinite
sequence of IID Bernoulli random variables with usual parameter $p$,
then the distribution of the number of observed zero outcomes before the
$\alpha$-th nonzero outcome is negative binomial with shape parameter $\alpha$
and usual parameter $p$, that is, if one observes $y$ successes in $n$ trials,
then the distribution of $y$ is binomial if $n$ was fixed and the distribution
of $n - y$ is negative binomial if $y$ was fixed.
But this rationale has nothing to do with aster models.

The \emph{second rationale} for this distribution is overdispersed Poisson.
This distribution arises as a mixture of Poisson distributions,
as is discussed below (Section~\ref{sec:mixture}).
This is the reason it is implemented in R package
\code{aster}.  For this rationale $\alpha$ can be any positive real number.

The \emph{mean} and \emph{variance} in terms of these parameters are
\begin{align*}
   E(y) & = \frac{\alpha (1 - p)}{p}
   \\
   \var(y) & = \frac{\alpha (1 - p)}{p^2}
\end{align*}

From \eqref{eq:negative-binomial-pmf} the log likelihood is
$$
   l(\theta)
   =
   \log \Gamma(\alpha + y) - \log \Gamma(\alpha) - \log(y!)
   + \alpha \log(p) + y \log(1-p).
$$
from which we can see that if $\alpha$ is considered an unknown parameter,
this is \emph{not} an exponential family, so we consider $\alpha$ known,
which means we can drop terms not containing $p$ obtaining
$$
   l(\theta)
   =
   y \log(1 - p) + \alpha \log(p) 
$$
from which we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(1 - p)
$$
and solving for $p$ gives
$$
   1 - p = e^\theta
$$
and
$$
   p = 1 - e^\theta
$$

The \emph{cumulant function} is
$$
   c(\theta)
   =
   - \alpha \log(p) 
   =
   - \alpha \log(1 - e^\theta)
$$
As $p$ goes from zero to one, $\theta$ goes from zero to $- \infty$ so
the formula above does not define the cumulant function on the whole real
line and equation (5) in \citet{geyer-gdor}, which is \eqref{eq:cumfun-expfam}
in this book, must be used
\begin{equation*}
   c(\theta) = c(\psi) +
   \log E_{\psi}\bigl( e^{y (\theta - \psi)} \bigr)
\end{equation*}
where $\psi$ is a fixed canonical parameter value, $\theta$ varies
over the whole real line, and the cumulant function has the value $\infty$
where the expectation does not exist.

Evaluating this we get, using the theorem associated with the negative
binomial distribution \citep{brand-name-distributions},
\begin{equation} \label{eq:negative-binomial-cumfun-derivation}
\begin{split}
   c(\theta)
   & =
   c(\psi) +
   \log \left( \sum_{y = 0}^\infty
   e^{y (\theta - \psi)} \cdot
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!} p^\alpha (1-p)^y
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   p^\alpha
   \sum_{y = 0}^\infty
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ (1-p) e^{\theta - \psi} \right]^y
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   p^\alpha
   \left[ 1 - (1-p) e^{\theta - \psi} \right]^{- \alpha}
   \right)
\end{split}
\end{equation}
where $p$ is the usual parameter value corresponding to canonical parameter
value $\psi$, that is, $\psi = \log(1 - p)$ and the formula is only valid
when the infinite sequence converges, which it does if and only if
$-1 < (1-p) e^{\theta - \psi} < +1$.

Now $1 - p = e^\psi$, so we can simplify $(1-p) e^{\theta - \psi} = e^\theta$.
So the convergence criterion is $e^\theta < 1$ or $\theta < 0$, and the
formula simplifies to
$$
   c(\theta)
   =
   c(\psi)
   +
   \alpha \log(p)
   -
   \alpha \log \left( 1 - e^\theta \right)
$$
but the formula only determines the cumulant function up to an arbitrary
constant (which does not matter) so we can take the cumulant function to be
\begin{equation} \label{eq:cumfun-negative-binomial}
   c(\theta)
   =
   \begin{cases}
   - \alpha \log(1 - e^\theta), & \theta < 0
   \\
   \infty, & \theta \ge 0
   \end{cases}
\end{equation}
So the full canonical parameter space is, as we guessed before,
\begin{equation} \label{eq:canonical-parameter-space-negative-binomial}
   \Theta = \set{ \theta \in \real : \theta < 0 }
\end{equation}
and \eqref{eq:cumfun-negative-binomial} agrees with what we derived just
from looking at the log likelihood wherever the function is finite.

Let's check that this cumulant function gives the correct mean and variance.
\begin{align*}
   c'(\theta)
   & =
   \frac{\alpha e^\theta}{1 - e^\theta}
   \\
   & =
   \frac{\alpha (1 - p)}{p}
   \\
   c''(\theta)
   & =
   \frac{d}{d \theta}
   \frac{\alpha e^\theta}{1 - e^\theta}
   \\
   & =
   \frac{\alpha e^\theta}{1 - e^\theta}
   -
   \frac{\alpha e^{2 \theta}}{(1 - e^\theta)^2}
   \\
   & =
   \frac{\alpha (1 - p)}{p}
   \left(
   1
   -
   \frac{1 - p}{p}
   \right)
   \\
   & =
   \frac{\alpha (1 - p)}{p^2}
\end{align*}
as we had already been told but now have derived
from exponential family theory.

The \emph{mean value parameter}
\begin{equation} \label{eq:negative-binomial-mean-value}
   \xi = \frac{\alpha (1 - p)}{p}
\end{equation}
is not the usual parameter $p$.  Solving for $p$ gives
\begin{equation} \label{eq:negative-binomial-usual}
   p = \frac{\alpha}{\alpha + \xi}
\end{equation}
% Mathematica
% Solve[ xi == alpha (1 - p) / p, p ]

The \emph{canonical parameter space}
is \eqref{eq:canonical-parameter-space-negative-binomial}
which is not the whole real line.

The \emph{mean value parameter space} is the range of the derivative of
the cumulant function $0 < \xi < \infty$.

As discussed at the end of the Sections~\ref{sec:binomial},
\ref{sec:poisson}, and~\ref{sec:zero-truncated-poisson}, LCM
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[0, \infty)$, and its
boundary consists of the single point $0$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at zero.

Also as discussed at the end of the those sections,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y = 0)
   \\
   & =
   - \alpha \log(1 - e^\theta)
   +
   \log \left(p^\alpha\right)
   \\
   & =
   0
\end{align*}
So the cumulant function of the family concentrated at zero is the zero
function, as we also found in Sections~\ref{sec:binomial} and~\ref{sec:poisson}
above.  But as mentioned at the end of Section~\ref{sec:zero-truncated-poisson}
above, this agreement just happened because of arbitrary choices of arbitrary
constants in cumulant functions.

\subsection{Negative Binomial as Mixture of Poisson}
\label{sec:mixture}

As stated above, one rationale for the negative binomial distribution is
that it is a mixture of Poisson distributions.  Let the conditional
distribution of $Y$ given $\mu$ be Poisson with mean $\mu$,
and let the marginal distribution of $\mu$ be $\text{Gamma}(\alpha, \lambda)$.
Then the marginal distribution of $Y$ is given by
\begin{align*}
   f(y)
   & =
   \int f(y \mid \mu) g(\mu) \, d \mu
   \\
   & =
   \int_0^\infty \frac{\mu^y e^{- \mu}}{y!} \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)} \mu^{\alpha - 1} e^{- \lambda \mu}
   \, d \mu
   \\
   & =
   \frac{1}{y!}
   \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)}
   \int_0^\infty \mu^{y + \alpha - 1} e^{- (1 + \lambda) \mu}
   \, d \mu
   \\
   & =
   \frac{1}{y!}
   \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)}
   \frac{\Gamma(y + \alpha)}{(1 + \lambda)^{y + \alpha}}
\end{align*}
using the theorem associated with the gamma distribution
\citep{brand-name-distributions}.

For this to be equal to \eqref{eq:negative-binomial-pmf} we need
$$
   \frac{1}{y!}
   \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)}
   \frac{\Gamma(y + \alpha)}{(1 + \lambda)^{y + \alpha}}
   =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!} p^\alpha (1-p)^y
$$
that is
$$
   \frac{\lambda^\alpha}{(1 + \lambda)^{y + \alpha}}
   =
   p^\alpha (1-p)^y
$$
or
$$
   \left(\frac{\lambda}{1 + \lambda}\right)^\alpha
   \left(\frac{1}{1 + \lambda}\right)^y
   =
   p^\alpha (1-p)^y
$$
which happens if and only if $p = \lambda / (1 + \lambda)$ and
$1 - p = 1 / (1 + \lambda)$ so $\lambda = p / (1 - p)$.

\subsection{Poisson as Limit of Negative Binomial}

Reparameterize the negative binomial distribution so the parameters
are $\alpha$ and $\xi$ so the usual parameter is
\eqref{eq:negative-binomial-usual}
and the PMF \eqref{eq:negative-binomial-pmf} becomes
\begin{align*}
   f(y)
   & =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left(\frac{\alpha}{\alpha + \xi}\right)^\alpha
   \left(\frac{\xi}{\alpha + \xi}\right)^y
   \\
   & =
   \frac{1}{y!}
   \left(\frac{\alpha}{\alpha + \xi}\right)^\alpha
   \left(\frac{\xi}{\alpha + \xi}\right)^y
   \prod_{k = 1}^y (\alpha + k - 1)
   \\
   & =
   \frac{\xi^y}{y!}
   \left(1 - \frac{\xi}{\alpha + \xi}\right)^\alpha
   \prod_{k = 1}^y \frac{\alpha + k - 1}{\alpha + \xi}
   \\
   & \to
   \frac{\xi^y}{y!} e^{- \xi}
\end{align*}
so as $\alpha \to \infty$ with $\xi$ fixed, we recover the Poisson
distribution.  But this does not happen if we let $\alpha \to \infty$
with some other parameter, such as $p$ or $\theta$, fixed.
(The limit $\left(1 - \frac{\xi}{\alpha + \xi}\right)^\alpha \to e^{- \xi}$
as $\alpha \to \infty$, which was used in the derivation above, follows
from $(1 + x / n)^n \to e^x$ as $n \to \infty$, which can be found
in any calculus book.)

The upshot of this section is that if the shape parameter $\alpha$
of a negative binomial distribution is large, then it is well approximated
by a Poisson distribution.  One only needs the negative binomial family
when the shape parameter is small.

The limit in this section is not like the limits in other sections of
this appendix.  In those other sections we took limits as the canonical
parameter of the exponential family went to plus or minus infinity.
Since the canonical parameter is considered unknown, this kind of
limit can arise in the process of maximum likelihood.
%%%%%%%%%% NEED BACKWARD REFERENCE to limiting conditional models %%%%%%%%%%
In this section we took a limit as the shape parameter $\alpha$ went
to infinity.
Since this parameter is considered known, this kind of
limit cannot arise in the process of maximum likelihood.

\section{Zero-Truncated Negative Binomial}

This section is just like Section~\ref{sec:zero-truncated-poisson} above
\emph{mutatis mutandis}.

The \emph{zero-truncated negative binomial} distribution is
the negative binomial distribution conditioned on being nonzero.

The \emph{rationale} is that it can be used to incorporate zero-inflated
negative binomial random variables into aster models.

This is a \emph{discrete} distribution.

\begin{sloppypar}
If $f$ is the PMF of the negative binomial distribution, then the PMF of
the zero-truncated negative binomial distribution is
\begin{equation}
\label{eq:zero-truncated-negative-binomial-pmf-in-terms-of-negative-binomial}
   g(y) = \frac{f(y)}{1 - f(0)},
   \qquad y = 1, 2, \ldots,
\end{equation}
that is, if $\alpha$ is the shape parameter and $p$ is the usual parameter
of the untruncated negative binomial distribution, then
the PDF of the zero-truncated negative binomial distribution is
\begin{equation} \label{eq:zero-truncated-negative-binomial-pmf}
   g(y) =
   \frac{\Gamma(\alpha + y) p^\alpha (1-p)^y}
   {\Gamma(\alpha) \, y! \, (1 - p^\alpha)},
   \qquad y = 1, 2, \ldots.
\end{equation}
\end{sloppypar}

Since this is not a ``brand name distribution'' the mean and variance
cannot just be looked up.
We still use Theorem~\ref{th:truncated-mean-variance}
and the comment following it to calculate the mean and the variance but now
\begin{align*}
   E(X) & = \frac{\alpha (1 - p)}{p}
   \\
   \var(X) & = \frac{\alpha (1 - p)}{p^2}
   \\
   E(X^2) & =
   \frac{\alpha (1 - p)}{p^2} + \left( \frac{\alpha (1 - p)}{p} \right)^2
   \\
   \Pr(X > 0) & = 1 - p^\alpha
\end{align*}
so
$$
   E(Y) = \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
$$
and
\begin{align*}
   \var(Y)
   & = 
   E(Y^2) - E(Y)^2
   \\
   & = 
   \frac{E(X^2)}{\Pr(X > 0)} - \left(\frac{E(X)}{\Pr(X > 0)}\right)^2
   \\
   & =
   \frac{1}{1 - p^\alpha}
   \left[ \frac{\alpha (1 - p)}{p^2} + \left( \frac{\alpha (1 - p)}{p}
   \right)^2 \right]
   -
   \left( \frac{\alpha (1 - p)}{p (1 - p^\alpha)} \right)^2
\end{align*}
% Mathematica
% dist = NegativeBinomialDistribution[alpha,p]
% f[y_] = PDF[dist, y]
% g[y_] = f[y] / (1 - f[0])
% Sum[ g[y], {y, 1, Infinity} ]
% moo = Sum[ y g[y], {y, 1, Infinity} ]
% moo - alpha (1 - p) / (p (1 - p^alpha))
% Simplify[%]
% voo = Sum[ (y - moo)^2 g[y], {y, 1, Infinity} ]
% voo - (1 / (1 - p^alpha) (alpha (1 - p) / p^2 + (alpha (1 - p) / p)^2) -
%     (alpha (1 - p) / (p (1 - p^alpha)))^2)
% Simplify[%]

With the assumptions of Section~\ref{sec:negative-binomial} above
($\alpha$ known and $p$ unknown) this is an exponential family.
From \eqref{eq:zero-truncated-negative-binomial-pmf} the log likelihood is
$$
   l(\theta)
   =
   \log \Gamma(\alpha + y) + \alpha \log(p) + y \log(1-p)
   - \log \Gamma(\alpha) - \log(y!) - \log(1 - p^\alpha)
$$
and we may drop terms that do not contain the unknown parameter $p$ obtaining
$$
   l(\theta)
   =
   y \log(1-p) + \alpha \log(p) - \log(1 - p^\alpha)
$$
from which we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(1 - p)
$$
(the same as for the untruncated negative binomial distribution)
and solving for $p$ gives
$$
   p = 1 - e^\theta
$$
(the same as for the untruncated negative binomial distribution).

The \emph{cumulant function} is
$$
   c(\theta)
   =
   - \alpha \log(p) + \log(1 - p^\alpha)
   =
   - \alpha \log(1 - e^\theta) + \log(1 - (1 - e^\theta)^\alpha)
$$
As in Section~\ref{sec:negative-binomial} this does not define the
cumulant function on the whole real line so we use
\begin{align*}
   c(\theta)
   & =
   c(\psi) +
   \log \left( \sum_{y = 1}^\infty
   e^{y (\theta - \psi)} \cdot
   \frac{\Gamma(\alpha + y) p^\alpha (1-p)^y}
   {\Gamma(\alpha) \, y! \, (1 - p^\alpha)}
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   \frac{p^\alpha}{1 - p^\alpha}
   \sum_{y = 1}^\infty
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ (1-p) e^{\theta - \psi} \right]^y
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   \frac{p^\alpha}{1 - p^\alpha}
   \left\{
   - 1 +
   \sum_{y = 0}^\infty
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ (1-p) e^{\theta - \psi} \right]^y
   \right\}
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   \frac{p^\alpha}{1 - p^\alpha}
   \left\{
   - 1 +
   \bigl[ 1 - (1-p) e^{\theta - \psi} \bigr]^{- \alpha}
   \right\}
   \right)
\end{align*}
where the last equality is the theorem associated with the negative
binomial distribution \citep{brand-name-distributions}
and where, as in \eqref{eq:negative-binomial-cumfun-derivation}, $p$ is
the usual parameter that goes with $\psi$ not $\theta$ so $p$ is a known
constant and $(1-p) e^{\theta - \psi} = e^\theta$ and the infinite series
converges if and only if $\theta < 0$.
Thus our formula simplifies to
$$
   c(\theta) = c(\psi) + \log\left( \frac{p^\alpha}{1 - p^\alpha} \right)
   +
   \log\left(- 1 + (1 - e^\theta)^{- \alpha} \right)
$$
and we may drop the terms that do not contain $\theta$ obtaining
$$
   c(\theta)
   =
   \begin{cases}
   \log\left((1 - e^\theta)^{- \alpha} - 1\right), & \theta < 0 \\
   \infty, & \theta \ge 0
   \end{cases}
$$
With some rearrangement, this agrees with what we deduced from looking
at the log likelihood.
% Mathematica
% ( - alpha Log[1 - Exp[theta]] + Log[1 - (1 - Exp[theta])^alpha] ) -
%     Log[(1 - Exp[theta])^(- alpha) - 1]
% Simplify[%]
% FullSimplify[%]
% PowerExpand[%]
%
% check
%
% dist = NegativeBinomialDistribution[alpha, 1 - E^theta]
% f[y_] = PDF[dist, y]
% g[y_] = f[y] / (1 - f[0])
% m[t_] = Sum[ Exp[y t] g[y], {y, 1, Infinity} ]
% k[t_] = PowerExpand[Log[m[t]]]
%
% c[theta_] = Log[- 1 + (1 - E^theta)^(- alpha) ]

Let's check that this cumulant function gives the correct mean and variance.
\begin{align*}
   c'(\theta)
   & =
   \frac{- \alpha (1 - e^\theta)^{- \alpha - 1} (- e^\theta)}
   {(1 - e^\theta)^{- \alpha} - 1}
   \\
   & =
   \frac{\alpha (1 - e^\theta)^{- \alpha - 1} e^\theta}
   {(1 - e^\theta)^{- \alpha} - 1}
   \\
   & =
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
   \\
   & =
   \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   \\
   c''(\theta)
   & =
   \frac{d}{d \theta}
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
   \\
   & =
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
   +
   \frac{\alpha e^\theta e^\theta}{(1 - e^\theta)^2 [1 - (1 - e^\theta)^\alpha]}
   \\
   & \qquad
   - \frac{\alpha^2 e^\theta e^\theta (1 - e^\theta)^{\alpha - 1}
   }{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]^2}
   \\
   & =
   \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   +
   \frac{\alpha (1 - p)^2}{p^2 (1 - p^\alpha)}
   -
   \frac{\alpha^2 (1 - p)^2 p^{\alpha - 2}}{(1 - p^\alpha)^2}
\end{align*}
% Mathematica
% (1 / 1 - p^alpha (alpha (1 - p) / p^2 + (alpha (1 - p) / p)^2) -
%     (alpha (1 - p) / (p (1 - p^alpha)))^2) -
%     (alpha (1 - p) / (p (1 - p^alpha)) +
%     alpha (1 - p)^2 / (p^2 (1 - p^alpha)) -
%     alpha^2 p^alpha (1 - p)^2 / (p^2 (1 - p^alpha)^2))
% Simplify[%]
% FullSimplify[%]
% PowerExpand[%]
%
% Oops!
%
% c[theta_] = Log[(1 - Exp[theta])^(- alpha) - 1]
% cp[theta_] = D[c[theta], theta]
% cp[theta_] = Simplify[cp[theta]]
% cpp[theta_] = D[cp[theta], theta]
% cpp[Log[1 - p]]
%
% alpha (1 - p) / (p (1 - p^alpha)) + alpha (1 - p)^2 / (p^2 (1 - p^alpha)) -
%     alpha^2 (1 - p)^2 p^(alpha - 2) / (1 - p^alpha)^2 - cpp[Log[1 - p]]
% Simplify[%]
% alpha (1 - p) / (p (1 - p^alpha)) + alpha (1 - p)^2 / (p^2 (1 - p^alpha)) -
%     alpha^2 (1 - p)^2 p^(alpha - 2) / (1 - p^alpha)^2 - voo
% Simplify[%]
And after some rearrangement $c''(\theta)$ agrees with the variance
calculated above.

The \emph{mean value parameter}
\begin{equation} \label{eq:zero-truncated-negative-binomial-mean-value}
   \xi
   =
   \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   =
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
\end{equation}
is not the usual parameter $p$.  As with zero-truncated Poisson,
we find that the inverse mapping $\xi \to \theta$ has no closed-form
expression.
% Mathematica
% Solve[ xi == cpp[theta], theta ]
As stated in Section~\ref{sec:zero-truncated-poisson} above
we know from general exponential family theory
that this mapping $\xi \to \theta$ is
%%%%%%%%%% NEED BACKWARD REFERENCE to mean value parameterization %%%%%%%%%%
strictly increasing, invertible, and infinitely differentiable.
But in this case the inverse mapping $\xi \to \theta$ seems to have
no closed-form expression.
Also as stated in Section~\ref{sec:zero-truncated-poisson} above,
this means that this family does not have what GLM theory calls a link function
in any useful form.

The full canonical parameter space is
\eqref{eq:canonical-parameter-space-negative-binomial}
as it was for the negative binomial.

Taking the limit in
\eqref{eq:zero-truncated-negative-binomial-mean-value}
as $\theta \to - \infty$ and $p \to 1$ we see that
$$
   \lim_{p \uparrow 1} \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   =
   \lim_{p \uparrow 1} \frac{- \alpha}{1 - p^\alpha - p \alpha p^{\alpha - 1}}
   =
   1
$$
(using L'Hospital's rule).
Taking the limit in
\eqref{eq:zero-truncated-negative-binomial-mean-value}
as $\theta \to \infty$ and $p \to 0$ we see that $\xi \to \infty$ in
this case.  And since we know from general exponential family theory
that $c'(\theta)$ is a continuous increasing function, it follows
that the \emph{mean value parameter space} is $1 < \xi < \infty$.

As discussed at the end of the Sections~\ref{sec:binomial},
\ref{sec:poisson}, \ref{sec:zero-truncated-poisson},
and~\ref{sec:negative-binomial-basics},
LCM are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[1, \infty)$, and its
boundary consists of the single point $1$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at one.

Also as discussed at the end of the those sections,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y = 1)
   \\
   & =
   \log\left((1 - e^\theta)^{- \alpha} - 1\right)
   +
   \log\left(\frac{\alpha p^\alpha (1 - p)}{1 - p^\alpha}\right)
   \\
   & =
   \log\left(p^{- \alpha} - 1\right)
   +
   \log\left(\frac{\alpha p^\alpha (1 - p)}{1 - p^\alpha}\right)
   \\
   & =
   \log\left(\frac{1 - p^\alpha}{p^\alpha}\right)
   +
   \log\left(\frac{\alpha p^\alpha (1 - p)}{1 - p^\alpha}\right)
   \\
   & =
   \log(\alpha) + \log(1 - p)
   \\
   & =
   \log(\alpha) + \theta
\end{align*}
% Mathematica
% dist = NegativeBinomialDistribution[alpha,p]
% f[y_] = PDF[dist, y]
% g[y_] = f[y] / (1 - f[0])
% Sum[ g[y], {y, 1, Infinity} ]
%
% c[theta_] = Log[(1 - Exp[theta])^(- alpha) - 1]
% cm1[theta_] = c[theta] + Log[g[1]]
% p = 1 - E^theta
%
% $Assumptions = alpha > 0 && theta < 0
% cm1[theta]
% PowerExpand[%]
% FullSimplify[%]
%
% Limit[theta + s - c[theta + s], s -> -Infinity]

Thus we see that unlike in Sections~\ref{sec:binomial}
and~\ref{sec:zero-truncated-poisson}, the cumulant function for
the family concentrated at one is not the identity function but
rather the identity function plus a constant function.

\section{Multivariate Bernoulli}
\label{sec:multivariate-bernoulli}

A random vector is \emph{multivariate Bernoulli} if it always has exactly
one component equal to one and the rest of its components are equal to zero.

This is the \emph{rationale} for the distribution.
In categorical data analysis, where we have IID individuals that can take
values in a finite number of categories, the result for each individual
can be coded as multivariate Bernoulli.  The category corresponding to the
component of the random vector that is equal to one says which category
the individual is in.

This is a \emph{discrete} random vector.

This is a special case of the multinomial distribution, which we do next.

This family is implemented in R package \code{aster2}.
When incorporated in an aster model, this family is a \emph{dependence group}.
Because it is
multivariate, it cannot be implemented in R package \code{aster},
which allows only univariate families.  Although R package \code{aster2}
calls this family ``multinomial'' created by the family function
\code{fam.multinomial}, it is not the general multinomial described in
the following section, but the multivariate Bernoulli described in this
section, which is the $n = 1$ special case of the general multinomial.

\section{Multinomial}
\label{app:sec:multinomial}

This family is multi-dimensional and hence cannot be implemented in R package
\code{aster}.

When IID individuals (a simple random sample) are classified into mutually
exclusive and exhaustive categories (every individual falls in exactly one
category), the vector of category counts has
the \emph{multinomial distribution}.
This is one \emph{rationale} for this distribution.

The other \emph{rationale} is that when the sample size (predecessor)
is equal to one, this family serves as a $k$-way switch if there are $k$
categories
(and this $n = 1$ special case is the multivariate Bernoulli family
described in the preceding section).
%%%%%%%%%% NEED BACKWARD REFERENCE to k-way switch %%%%%%%%%%

This is the distribution of a random vector, not a random variable.

The Bernoulli and binomial distributions are related to the multinomial
distribution, but the multinomial distribution with two categories is
still a two-dimensional random vector, so it is not Bernoulli or binomial,
which are one-dimensional distributions (of random variables).
If $Y$ is a Bernoulli random variable, then
$(Y, 1 - Y)$ is a multivariate Bernoulli random vector.
If $Y$ is a binomial random variable with sample size $n$, then
$(Y, n - Y)$ is a multinomial random vector with sample size $n$.

This is a \emph{discrete} random vector.

The \emph{probability mass function} is
\begin{equation} \label{eq:multinomial-pmf}
   f(y) = \frac{n !}{\prod_{i \in I} y_i!} \prod_{i \in I} p_i^{y_i},
   \qquad y \in S,
\end{equation}
where $p$ is the \emph{usual parameter vector}, which is a probability
vector satisfying
\begin{gather*}
   p_i > 0
   \\
   \sum_{i \in I} p_i = 1
\end{gather*}
where $y$ is the the vector of counts (nonnegative integers) satisfying
\begin{subequations}
\begin{gather}
   y_i \ge 0
   \label{eq:multinomial-canonical-statistic-constraint-positivity}
   \\
   \sum_{i \in I} y_i = n
   \label{eq:multinomial-canonical-statistic-constraint-sum-to-n}
\end{gather}
\end{subequations}
where $n$ is the sample size (the number of IID individuals classified),
where $S$ is the sample space for $y$, the set
\begin{equation} \label{eq:multinomial-canonical-sample-space}
   S = \bigset{ y \in \nats^I : \sum_{i \in I} y_i = n},
\end{equation}
where $\nats$ denotes the \emph{natural numbers} $\{0, 1, 2, 3, \ldots\}$,
and where we have chosen the index set of $y$ and $\theta$ to be an
arbitrary finite set $I$ rather than $\{1, \ldots, k\}$ for some $k$
to fit in with the conventions of aster models (in an aster model $I$
would be a subset of nodes of the aster graph comprising a multinomial
dependence group, see Section~\ref{sec:subvector} for vectors and subvectors).

The \emph{mean vector} and \emph{variance matrix} have components
\begin{subequations}
\begin{alignat}{2}
   E(Y_i) & = n p_i
   \label{eq:mean-vector-components}
   \\
   \var(Y_i) & = n p_i (1 - p_i)
   \label{eq:variance-matrix-components-diagonal}
   \\
   \cov(Y_i, Y_j) & = - n p_i p_j, & \qquad & i \neq j
   \label{eq:variance-matrix-components-off-diagonal}
\end{alignat}
\end{subequations}
The mean of a random vector $Y$ is the vector whose components are the
means of the components of $Y$.  Here $E(Y) = \xi$ and $\xi_i = n p_i$.
The variance of a random vector $Y$ is the matrix whose components are the
covariances of the components of $Y$.  Here $\var(Y) = M$ has components
$m_{i j}$ which are given by \eqref{eq:variance-matrix-components-diagonal}
when $i = j$ and by \eqref{eq:variance-matrix-components-off-diagonal} when
$i \neq j$.

This is an \emph{exponential family}.  From \eqref{eq:multinomial-pmf}
the log likelihood is
\begin{equation} \label{eq:multinomial-logl-try-one}
   l(\theta) = \sum_{i \in I} y_i \log(p_i)
\end{equation}
from which we see that we have an exponential family with
\emph{canonical statistic vector} $y$ and \emph{canonical parameter vector}
$\theta$ having components
\begin{equation} \label{eq:multinomial-canonical-try-one}
   \theta_i = \log(p_i), \qquad i \in I.
\end{equation}

Trying to read the cumulant function off of \eqref{eq:multinomial-logl-try-one}
seems to say $c(\theta)$ is the constant function everywhere equal to zero,
and this is correct because the $p_i$ must sum to one, and as we shall see
the cumulant function does have the value zero
when
\begin{equation} \label{eq:multinomial-canonical-constraint-try-one}
   \sum_{i \in I} e^{\theta_i} = 1.
\end{equation}

But we want the cumulant function defined on the whole vector space where
$\theta$ lives, so we must use equation (5) in \citet{geyer-gdor},
which is \eqref{eq:cumfun-expfam} in this book,
\begin{align*}
   c(\theta)
   & =
   c(\psi)
   +
   \log E_\psi\left\{ e^{\sum_{i \in I} y_i (\theta_i - \psi_i)} \right\}
   \\
   & =
   c(\psi) + \log \sum_{y \in S}
   e^{\sum_{i \in I} y_i (\theta_i - \psi_i)}
   \cdot
   \frac{n !}{\prod_{i \in I} y_i!} \prod_{i \in I} p_i^{y_i}
   \\
   & =
   c(\psi) + \log \sum_{y \in S}
   \frac{n !}{\prod_{i \in I} y_i!}
   \prod_{i \in I} (p_i e^{\theta_i - \psi_i})^{y_i}
   \\
   & =
   c(\psi) + \log \left( \sum_{i \in I} p_i e^{\theta_i - \psi_i} \right)^n
   \\
   & =
   c(\psi) + n \log \left( \sum_{i \in I} p_i e^{\theta_i - \psi_i} \right)
\end{align*}
where the last equality is the multinomial theorem also called the theorem
associated with the multinomial distribution \citep{brand-name-distributions}.
Here $\psi$ is a possible value of the canonical parameter vector (held fixed)
and $p$ is the usual parameter vector corresponding to it so $p_i = e^{\psi_i}$
and $p_i e^{\theta_i - \psi_i} = e^{\theta_i}$.  So dropping $c(\psi)$,
which is an arbitrary constant, we obtain
\begin{equation} \label{eq:multinomial-cumfun}
   c(\theta) = n \log \left( \sum_{i \in I} e^{\theta_i} \right).
\end{equation}
Since this is finite for all vectors $\theta$, the full canonical parameter
space is the whole vector space $\real^I$ where $\theta$ lives.

This gives a log likelihood valid on the whole vector space where
$\theta$ lives
\begin{equation} \label{eq:multinomial-logl}
\begin{split}
   l(\theta) & = \inner{y, \theta} - c(\theta)
   \\
   & = \left( \sum_{i \in I} y_i \theta_i \right)
   - n \log \left( \sum_{i \in I} e^{\theta_i} \right)
\end{split}
\end{equation}

We check that \eqref{eq:multinomial-cumfun} has the correct derivatives
\begin{subequations}
\begin{align}
   \frac{\partial c(\theta)}{\partial \theta_i}
   & = 
   \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
   \label{eq:multinomial-cumfun-first-derivatives}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_i^2}
   & = 
   \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
   -
   \frac{n e^{\theta_i} e^{\theta_i}}
   {\left( \sum_{j \in I} e^{\theta_j} \right)^2}
   \label{eq:multinomial-cumfun-second-derivatives-diagonal}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_i \theta_j}
   & = 
   -
   \frac{n e^{\theta_i} e^{\theta_j}}
   {\left( \sum_{k \in I} e^{\theta_k} \right)^2}
   \label{eq:multinomial-cumfun-second-derivatives-off-diagonal}
\end{align}
\end{subequations}
But here we have a problem that this does not even make sense with our
previous notion of canonical parameters, which, recall, was defined
by \eqref{eq:multinomial-canonical-try-one} but only subject to the
constraint \eqref{eq:multinomial-canonical-constraint-try-one}.
So we do not have a notion of what the map $\theta \to p$ should be
for values of $\theta$ that do not satisfy the
constraint \eqref{eq:multinomial-canonical-constraint-try-one}.

We solve this problem by using the fundamental relationship between
cumulant functions and means and variances
(Section~\ref{sec:mean-variance-cumulant} above), which says the
derivatives above have to give means and covariances.
Thus \eqref{eq:multinomial-cumfun-first-derivatives} must
give the correct mean values
$$
   \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}} = n p_i
$$
so
\begin{equation} \label{eq:multinomial-canonical-to-usual}
   p_i = \frac{e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}, \qquad i \in I,
\end{equation}
gives the correct mapping between our new canonical parameters (now
$\theta$ can be any vector in $\real^I$) and the usual parameters.

This function is not invertible.  If one adds the same constant
to all of the $\theta_i$, then the value of $p_i$ does not change
$$
   \frac{e^{\theta_i + c}}{\sum_{j \in I} e^{\theta_j + c}}
   =
   \frac{e^c e^{\theta_i}}{e^c \sum_{j \in I} e^{\theta_j}}
   =
   \frac{e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
$$

This illustrates another thing wrong with the concept of the link function:
it forces the canonical parameterization to be identifiable even when this
is inadvisable.  Using our choice of parameterization here there can be
no link function because the map $\theta \to \xi$ is not one-to-one, so
its inverse mapping does not exist (an that inverse mapping is supposed to
be the ``link'' function).

We also clear up a mystery left hanging and check
that \eqref{eq:multinomial-cumfun} does indeed evaluate to zero when the
constraint \eqref{eq:multinomial-canonical-constraint-try-one} holds.

Having made the identification \eqref{eq:multinomial-canonical-to-usual}
we see that \eqref{eq:multinomial-cumfun-second-derivatives-diagonal}
and \eqref{eq:multinomial-cumfun-second-derivatives-off-diagonal}
do give the correct variances and covariances
\begin{align*}
   \frac{\partial^2 c(\theta)}{\partial \theta_i^2}
   & =
   \var(y_i) = n p_i (1 - p_i)
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_i \partial \theta_j}
   & =
   \cov(y_i, y_j) = - n p_i p_j
\end{align*}

The canonical parameter space is the whole vector space where $\theta$ lives.

The mean value parameter vector, as stated above, is the vector $\xi$ having
components $\xi_i = n p_i$.  In case $n = 1$, the mean value parameter vector
is the usual parameter vector.  Otherwise, not.

The mean value parameter space is the relative interior of the
convex hull of $S$
\begin{equation} \label{eq:multinomial-mean-value-parameter-space}
   \Xi
   =
   \bigset{ \xi \in \real^I : \xi_i > 0, i \in I \opand
   \sum_{j \in I} \xi_j = n }
\end{equation}
To see this we note that $\theta_i = \log(\xi_i)$ always defines a point
in the canonical sample space so long as the $\xi_i$ are strictly positive.
And that point maps via \eqref{eq:multinomial-canonical-to-usual} to
$$
   p_i = \frac{e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
   =
   \frac{\xi_i}{\sum_{j \in I} \xi_j} = \frac{\xi_i}{n}
$$
which makes the $p_i$ sum to one.
So every point in $\Xi$ is a mean value parameter vector value,
and, conversely, every point $\theta \in \real^I$ maps to a value $\xi$
that satisfies $\xi_i > 0$ for all $i$ and $\sum_{i \in I} \xi_i = n$.

\begin{theorem} \label{th:multinomial-lcm}
The closed convex support of the multinomial family is
$$
   C = \bigset{ \xi \in \real^I : \xi_i \ge 0, i \in I \opand
   \sum_{j \in I} \xi_j = n }
$$
the closure \eqref{eq:multinomial-mean-value-parameter-space}.
The support function is given by
\begin{equation} \label{eq:multinomial-support-function}
   \sigma_C(\delta) = n \max(\delta),
\end{equation}
where
$$
   \max(\delta) = \max_{i \in I} \delta_i.
$$
Define the hyperplane
$$
   H_\delta = \set{ y \in \real^I : \inner{y, \delta} = \sigma_C(\delta) },
$$
which is \eqref{eq:complete-fundamental-hyperplane} with the index set $I$
instead of $J$.
Then the cumulant function for the LCM conditioned on $H_\delta$ is
\begin{equation} \label{eq:multinomial-cumfun-lcm}
   c_\delta(\theta)
   =
   n \log \left(
   \sum_{\substack{i \in I \\ \delta_i = \max(\delta)}} e^{\theta_i} \right).
\end{equation}
If we define
$$
   G = \set{ i \in I : \delta_i = \max(\delta) },
$$
then under the LCM conditioned on $H_\delta$, the family of distributions
for $y_G$ is multinomial with sample size $n$
and $y_{I \setminus G} = 0$ almost surely.
\end{theorem}
\begin{proof}
For any $i \in I$ define the vector $v^{(i)}$ (we use the temporary notation
of superscript in parenthesis to denote a sequence of vectors)
having $v^{(i)}_i = n$ and $v^{(i)}_j = 0$ for $j \in I \setminus \{ i \}$.
Clearly, each such $v^{(i)}$ is in
the sample space \eqref{eq:multinomial-canonical-sample-space} and
has probability $p_i^n > 0$.  Thus it must be in any support.
Now the formula
$$
   C = \bigset{ \sum_{i \in I} p_i v^{(i)} : p_i \ge 0, i \in I \opand
   \sum_{j \in I} p_j = 1 }
$$
shows that $C$ is the convex hull of the vectors $v^{(i)}$.  Thus $C$ must
be contained in the closed convex support.  On the other hand, $C$ contains
$S$, so $C$ is the smallest closed convex set containing $S$.  Hence
$C$ is the closed convex support.

We now claim that any vector $v$ having $v_j = 0$ when
$\delta_j < \max(\delta)$ maximizes $\inner{\fatdot, \delta}$ over $C$.
We have
$$
   \inner{v, \delta}
   =
   \sum_{\substack{i \in I \\ \delta_i = \max(\delta)}} v_i \delta_i
   =
   \max(\delta)
   \sum_{\substack{i \in I \\ \delta_i = \max(\delta)}} v_i
   =
   \max(\delta)
   \sum_{i \in I} v_i
   =
   n \max(\delta)
$$
and for any $x \in C$ we have
$$
   \inner{v, \delta}
   =
   \sum_{i \in I} v_i \delta_i
   \le
   \max(\delta)
   \sum_{i \in I} v_i
   =
   n \max(\delta)
$$
Thus \eqref{eq:multinomial-support-function} is correct.

Now if $y \in C$ such that
\begin{equation} \label{eq:multinomial-lcm-foo}
   \inner{y, \delta} = \sigma_C(\delta) = n \max(\delta)
\end{equation}
we must have, as we have already seen, $y_j = 0$ if $\delta_j < \max(\delta)$.
Conversely, if $y \in C$ and $y_j = 0$ if $\delta_j < \max(\delta)$,
then \eqref{eq:multinomial-lcm-foo} holds.
Hence conditioning the original model on $Y \in H_\delta$ is the same as
conditioning on $Y_{I \setminus G} = 0$.  Then the usual formulas
for conditionals for multinomials show that $Y_G$ is multinomial
with sample size $n$, as asserted.

It only remains to calculate the cumulant function for the LCM using
\eqref{eq:cumfun-lcm}.
\begin{align*}
   c_\delta(\theta)
   & =
   c(\theta) + \log \Pr\nolimits_\theta(Y \in H_\delta)
   \\
   & =
   n \log \left( \sum_{i \in I} e^{\theta_i} \right)
   +
   \log \left( \sum_{i \in G} p_i \right)^n
   \\
   & =
   n \log \left( \sum_{i \in I} e^{\theta_i} \right)
   +
   n \log \left( \sum_{i \in G} p_i \right)
   \\
   & =
   n \log \left( \sum_{i \in I} e^{\theta_i} \right)
   +
   n \log \left(
   \frac{\sum_{i \in G} e^{\theta_i}}{\sum_{i \in I} e^{\theta_i}} \right)
   \\
   & =
   n \log \left( \sum_{i \in G} e^{\theta_i} \right)
\end{align*}
\end{proof}

\section{Normal Location-Scale}

The univariate normal distribution is curious in that it remains an exponential
family even if we consider both parameters unknown, but then
the dimensions of the canonical statistic vector and the canonical parameter
vector must match.  So if the canonical parameter vector is going to be
two-dimensional so must be the canonical statistic vector.

Let's see how that happens.  We already have the probability density
function \eqref{eq:normal-pdf} of the normal distribution.
Now if we write down the log likelihood not dropping any terms that contain
either parameter we get
\begin{equation} \label{eq:logl-normal-location-scale}
   l(\theta)
   =
   - \log(\sigma) - \frac{(x - \nu)^2}{2 \sigma^2}
   =
   - \log(\sigma) - \frac{x^2}{2 \sigma^2}
   + \frac{x \nu}{\sigma^2}
   - \frac{\nu^2}{2 \sigma^2}
\end{equation}
where for reasons to be discussed presently we have changed the notation
for the random variable from $y$ to $x$ and the notation
for the mean from $\xi$ to $\nu$.

As was discussed in Section~\ref{sec:define-expfam} in the main text,
there is some freedom in choosing the canonical statistic vector and
the canonical parameter vector we must have the terms containing both
data and parameters in exponential family form, that is,
$$
   - \frac{x^2}{2 \sigma^2} + \frac{x \nu}{\sigma^2}
   =
   y_1 \theta_1 + y_2 \theta_2
$$
but that still allows lots of choices.
We could, for example, choose $y_1 = x$ or $y_1 = x^2$ or $y_1 = - x^2 / 2$.
And each such choice forces a different choice of the corresponding
canonical parameter.

The choice made in the implementation in R package \code{aster2} is
\begin{align*}
   y_1 & = x,
   \\
   y_2 & = x^2,
   \\
   \theta_1 & = \frac{\nu}{\sigma^2}
   \\
   \theta_2 & = - \frac{1}{2 \sigma^2}
\end{align*}
We have had many examples where the usual parameters are not the canonical
parameters.  Here is our first example where the usual statistics are not
the canonical statistics (the usual statistic is one canonical statistic
but not the other).

In terms of the canonical parameters, the usual parameters are
\begin{align*}
   \sigma^2 & = - \frac{1}{2 \theta_2}
   \\
   \nu & = \theta_1 \sigma^2 = - \frac{\theta_1}{2 \theta_2}
\end{align*}

The first canonical parameter is unrestricted $- \infty < \theta_1 < \infty$
but the second canonical parameter is restricted $- \infty < \theta_2 < 0$.
Thus our guess at the cumulant function from looking at the log likelihood
\begin{equation} \label{eq:cumfun-normal-location-scale}
\begin{split}
   c(\theta)
   & =
   \log(\sigma) + \frac{\nu^2}{2 \sigma^2}
   \\
   & =
   \frac{1}{2} \log\left(- \frac{1}{2 \theta_2}\right)
   +
   \left( - \frac{\theta_1}{2 \theta_2} \right)^2 \cdot \frac{1}{2}
   \cdot (- 2 \theta_2)
   \\
   & =
   \frac{1}{2} \log\left(- \frac{1}{2 \theta_2}\right)
   -
   \frac{\theta_1^2}{4 \theta_2} 
\end{split}
\end{equation}
(this agrees with the cumulant function for this family
in R package \code{aster2}).

Notice that in going from the PDF to the log likelihood
\eqref{eq:logl-normal-location-scale}, on a factor of $1 / \sqrt{2 \pi}$
was dropped, so the PDF of the family (with respect to Lebesgue measure)
can now be written
$$
   f_\theta(x)
   =
   \frac{1}{\sqrt{2 \pi}} e^{x \theta_1 + x^2 \theta_2 - c(\theta)}
$$
and the fact that PDF integrate to one gives
$$
   \int e^{x \theta_1 + x^2 \theta_2} \, d x
   =
   \sqrt{2 \pi} e^{c(\theta)}
$$

Because our guess at the cumulant function is not defined on a whole vector
space we use equation (5) in \citet{geyer-gdor},
which is \eqref{eq:cumfun-expfam}
in this book, as we have done several times before in this appendix
\begin{align*}
   c(\theta)
   & =
   c(\psi) + \log E_\psi
   \bigl( e^{y_1 (\theta_1 - \psi_1) + y_2 (\theta_2 - \psi_2)} \bigr)
   \\
   & =
   c(\psi) + \log E_\psi
   \bigl( e^{y_1 (\theta_1 - \psi_1) + y_2 (\theta_2 - \psi_2)} \bigr)
   \\
   & =
   c(\psi)
   +
   \log \left( \frac{1}{\sqrt{2 \pi}} \int
   e^{x (\theta_1 - \psi_1) + x^2 (\theta_2 - \psi_2)}
   e^{x \psi_1 + x^2 \psi_2) - c(\psi)} \, d x \right)
   \\
   & =
   \log \left( \frac{1}{\sqrt{2 \pi}} \int
   e^{x \theta_1 + x^2 \theta_2} \, d x \right)
\end{align*}
and, as we have already seen, the last expression is equal to $c(\theta)$ for
$\theta$ such that $\theta_2 < 0$.
In case $\theta_2 \ge 0$ the integrand is either constant or goes to infinity
as $x \to \infty$ or $x \to - \infty$ or both.  In any case the integral
does not exist.  Thus the canonical parameter space is indeed
\begin{equation} \label{eq:canonical-parameter-space-normal-location-scale}
   \Theta = \set{ \theta \in \real^2 : \theta_2 < 0 }.
\end{equation}

The canonical sample space for sample size one is
$$
   S = \set{ y \in \real^2 : y_1^2 = y_2 }
$$
which is a curve.  Thus any intersection with a tangent line consists of
a single point.  And, because this is a continuous distribution, single points
have probability zero.  Thus in the limiting conditional model theorem
(Theorem~\ref{th:completion-fundamental} above) we are in the case where
the probability of $Y \in H_\delta$ is equal to zero
(regardless of what $\delta$ is).  So there are no LCM for this family.

For sample size one, the data are on the curve $S$ with probability one.
So the MLE does not exist with probability one.  This is no surprise.
One cannot estimate two parameters from one variable $x$.

For sample size greater than one, the data are in the interior of the convex
hull of the curve $S$ with probability one.
So the MLE exists with probability one.  This is no surprise either.
One can estimate the two parameters of the normal distribution from
data $x_1$, $x_2$, $\ldots$, $x_n$ for $n \ge 2$.

So we don't need the theory of limiting conditional models for this family.
That theory is only needed for discrete families.

% Mathematica
%
% dist = NormalDistribution[moo, Sqrt[voo]]
% f[x_] = PDF[dist, x]
% Integrate[f[x], {x, -Infinity, Infinity}, Assumptions -> voo > 0]
% Integrate[x f[x], {x, -Infinity, Infinity}, Assumptions -> voo > 0]
% Integrate[(x - moo)^2 f[x], {x, -Infinity, Infinity}, Assumptions -> voo > 0]
%
% cumfun[theta1_, theta2_] = Integrate[Exp[x theta1 + x^2 theta2] f[x],
%     {x, -Infinity, Infinity}, Assumptions -> voo > 0]

We forgot to check that \eqref{eq:cumfun-normal-location-scale} has
the correct derivatives.  Let's do that now.
\begin{align*}
   \frac{\partial c(\theta)}{\partial \theta_1}
   & =
   -
   \frac{\theta_1}{2 \theta_2}
   \\
   \frac{\partial c(\theta)}{\partial \theta_2}
   & =
   -
   \frac{1}{2 \theta_2}
   +
   \frac{\theta_1^2}{4 \theta_2^2}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_1^2}
   & =
   - \frac{1}{2 \theta_2}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_1 \partial \theta_2}
   & =
   \frac{\theta_1}{2 \theta_2^2}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_2^2}
   & =
   \frac{1}{2 \theta_2^2}
   -
   \frac{\theta_1^2}{2 \theta_2^3}
\end{align*}
Translating these back to functions of $x$ and the original parameters,
they say
\begin{align*}
   E(X) & = \nu
   \\
   E(X^2) & = \sigma^2 + \nu^2
   \\
   \var(X) & = \sigma^2
   \\
   \cov(X, X^2) & = 2 \nu \sigma^2
   \\
   \var(X^2) & = 4 \nu^2 \sigma^2 + 2 \sigma^4
\end{align*}
The first three of these are well known.  The other two do check
(in Mathematica).
% Mathematica
% c[theta1_, theta2_] = (1/2) Log[- 1 / (2 theta2)] - theta1^2 / (4 theta2) 
% c1[theta1_, theta2_] = D[c[theta1, theta2], theta1]
% c2[theta1_, theta2_] = D[c[theta1, theta2], theta2]
% c11[theta1_, theta2_] = D[c1[theta1, theta2], theta1]
% c12[theta1_, theta2_] = D[c1[theta1, theta2], theta2]
% c22[theta1_, theta2_] = D[c2[theta1, theta2], theta2]
% c1[nu / sigma^2, - 1 / (2 sigma^2)]
% c2[nu / sigma^2, - 1 / (2 sigma^2)]
% c11[nu / sigma^2, - 1 / (2 sigma^2)]
% c12[nu / sigma^2, - 1 / (2 sigma^2)]
% c22[nu / sigma^2, - 1 / (2 sigma^2)]
%
% dist = NormalDistribution[nu, sigma]
% f[x_] = PDF[dist, x]
% $Assumptions = sigma > 0 && Element[nu, Reals]
% Integrate[f[x], {x, -Infinity, Infinity}]
% Integrate[(x - nu)(x^2 - nu^2 - sigma^2)f[x], {x, -Infinity, Infinity}]
% Integrate[(x^2 - nu^2 - sigma^2)^2 f[x], {x, -Infinity, Infinity}]

\section{Gamma Rate}

The gamma distribution has PDF
\begin{equation} \label{eq:gamma-pdf}
   f(y) = \frac{\lambda^\alpha}{\Gamma(\alpha)} y^{\alpha - 1} e^{- \lambda y},
   \qquad 0 < y < \infty,
\end{equation}
where $\alpha > 0$ and $\lambda > 0$ are parameters.

This is a \emph{continuous} random variable;
except when incorporated into an aster model, it is a mixture of discrete
and continuous.  For a gamma-rate arrow, when the predecessor is zero
the conditional distribution of the successor is the degenerate random variable
concentrated at zero, which is discrete, and when the predecessor is greater
than zero, the conditional distribution of the successor is continuous.

This is the most well known continuous distribution after
the normal distribution, and it has many rationales, but these rationales
do not seem to justify its inclusion in aster models, which is why
R packages \code{aster} and \code{aster2} do not include it.
This family includes the exponential distribution (case $\alpha = 1$)
and the chi-square distributions (case $\alpha = n / 2$ and $\lambda = 1 / 2$)
as special cases.

The main rationale is vague.  This is the most well-known distribution
with support $(0, \infty)$.  So it is a TTT (thing to do) when one wants such.

The \emph{mean} and \emph{variance} are
\begin{subequations}
\begin{align}
   E(y) & = \frac{\alpha}{\lambda}
   \label{eq:gamma-mean}
   \\
   \var(y) & = \frac{\alpha}{\lambda^2}
   \label{eq:gamma-var}
\end{align}
\end{subequations}

In this section, we treat this as a one-parameter family
with $\alpha$ known and $\lambda$ unknown.

With this understanding, the log likelihood is
$$
   l(\theta)
   = 
   \alpha \log(\lambda) - \lambda y
$$
(we have dropped terms that do not contain $\lambda$).
From this we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$,
\emph{canonical parameter} $\theta = - \lambda$, and
\emph{cumulant function}
\begin{equation} \label{eq:cumfun-gamma-rate}
   c(\theta) = - \alpha \log(\lambda) = - \alpha \log(- \theta).
\end{equation}

We check that this has the correct derivatives
$$
   c'(\theta) = \frac{\alpha}{- \theta} = \frac{\alpha}{\lambda}
$$
and
$$
   c''(\theta) = \frac{\alpha}{\theta^2} = \frac{\alpha}{\lambda^2}.
$$

Because our guess at the cumulant function is not defined on a whole vector
space we use equation (5) in \citet{geyer-gdor},
which is \eqref{eq:cumfun-expfam}
in this book, as we have done several times before in this appendix
\begin{align*}
   c(\theta)
   & =
   c(\psi) + \log E_\psi \bigl( e^{y (\theta - \psi)} \bigr)
   \\
   & =
   c(\psi) + \log \int e^{y (\theta - \psi)} f_\psi(y) \, d y
   \\
   & =
   c(\psi) + \log \int e^{y (\theta - \psi)}
   \frac{\lambda_\psi^\alpha}{\Gamma(\alpha)} y^{\alpha - 1}
   e^{- \lambda_\psi y} \, d y
\end{align*}
where $\lambda_\psi = - \psi$ so
\begin{align*}
   c(\theta)
   & =
   \log \int
   \frac{1}{\Gamma(\alpha)} y^{\alpha - 1} e^{y \theta} \, d y
\end{align*}
and this integral clearly exists if and only if $\theta < 0$ in which
case, by \eqref{eq:gamma-pdf} integrating to one,
we get \eqref{eq:cumfun-gamma-rate}, as we must.

Hence the full canonical parameter space of this family is
$$
   \Theta = \set{ \theta \in \real : \theta < 0 }.
$$

The \emph{mean value parameter} is
$$
   \xi = \frac{\alpha}{- \theta}
$$

\emph{Addition rule:} the sum of $n$ independent and identically distributed
gamma random variables with shape parameter $\alpha$ and rate parameter
$\lambda$ has the gamma distribution
with shape parameter $n \alpha$ and rate parameter $\lambda$.

\emph{General addition rule:} the sum of $n$ independent
gamma random variables with shape parameters $\alpha_1$, $\ldots,$ $\alpha_n$
and rate parameter $\lambda$ (same for all) has the gamma distribution
with shape parameter $\alpha_1 + \cdots + \alpha_n$
and rate parameter $\lambda$.

There are no limit degenerate distributions.
This is because the boundary of the closed convex support,
which is the interval $[0, \infty)$ is the point zero.
We can never observe data on the boundary, because continuous
distributions put probability zero at any point.

\section{Gamma Shape-Rate}

If we take the gamma distribution with PDF \eqref{eq:gamma-pdf}
to have both parameters unknown, then the log likelihood is
$$
   l(\theta)
   =
   \alpha \log(\lambda) - \log \Gamma(\alpha) + \alpha \log(x) - \lambda x
$$
(we have dropped terms that do not contain $\alpha$ or $\lambda$ and have
changed the data variable from $y$ to $x$).  This has the form of
a two-dimensional exponential family with \emph{canonical statistics}
$y_1 = x$ and $y_2 = \log x$, \emph{canonical parameters}
$\theta_1 = - \lambda$ and $\theta_2 = \alpha$, and
\emph{cumulant function}
\begin{equation} \label{eq:cumfun-gamma-shape-rate}
   c(\theta) = \log \Gamma(\alpha) - \alpha \log(\lambda)
   = \log \Gamma(\theta_2) - \theta_2 \log(- \theta_1)
\end{equation}
With this definition of canonical parameters and statistics, we can rewrite
the PDF as
\begin{equation} \label{eq:gamma-pdf-too}
\begin{split}
   f_\theta(y)
   & =
   \frac{\lambda^\alpha}{\Gamma(\alpha)} x^{\alpha - 1} e^{- \lambda x}
   \\
   & =
   x^{- 1} e^{\inner{y, \theta} - c(\theta)}
   \\
   & =
   y_1^{- 1} e^{\inner{y, \theta} - c(\theta)}
\end{split}
\end{equation}
Because our guess at the cumulant function is not defined on a whole vector
space we use equation (5) in \citet{geyer-gdor},
which is \eqref{eq:cumfun-expfam}
in this book, as we have done several times before in this appendix
\begin{align*}
   c(\theta)
   & =
   c(\psi) + \log E_\psi \bigl( e^{\inner{y, \theta - \psi}} \bigr)
   \\
   & =
   c(\psi) + \log \int e^{\inner{y, \theta - \psi}} f_\psi(y) \, d y
   \\
   & =
   c(\psi) + \log \int e^{\inner{y, \theta - \psi}}
   y_1^{- 1} e^{\inner{y, \psi} - c(\psi)} \, d x
   \\
   & =
   \log \int y_1^{- 1} e^{\inner{y, \theta}} \, d x
\end{align*}
We know that \eqref{eq:gamma-pdf-too} integrates if and only if
$\alpha > 0$ and $\lambda > 0$ \citep[Slides~27--30]{slides-5101-deck-6}.
It follows that the full canonical parameter space is
$$
   \Theta = \set{ \theta \in \real^2 : \theta_1 < 0 \opand \theta_2 > 0 }
$$
and in order that \eqref{eq:gamma-pdf-too} integrate to one when the integral
exists, the last integral above must be $c(\theta)$.

Let us check that \eqref{eq:cumfun-gamma-shape-rate} has the correct
derivatives
\begin{align*}
   \frac{\partial c(\theta)}{\partial \theta_1}
   & =
   - \frac{\theta_2}{\theta_1}
   \\
   \frac{\partial c(\theta)}{\partial \theta_2}
   & =
   \mydigamma(\theta_2) - \log(- \theta_1)
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_1^2}
   & =
   \frac{\theta_2}{\theta_1^2}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_1 \partial \theta_2}
   & =
   - \frac{1}{\theta_1}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_2^2}
   & =
   \trigamma(\theta_2)
\end{align*}
Translating these back to functions of $x$ and the original parameters,
they say
\begin{align*}
   E(X) & = \frac{\alpha}{\lambda}
   \\
   E\{\log(X)\} & = \mydigamma(\alpha) - \log(\lambda)
   \\
   \var(X) & = \frac{\alpha}{\lambda^2}
   \\
   \cov\{X, \log(X)\} & = \frac{1}{\lambda}
   \\
   \var\{\log(X)\} & = \trigamma(\alpha)
\end{align*}
where the digamma function is the first derivative of $\log \Gamma(\fatdot)$,
and the trigamma function is the second derivative of this function.

Two of these agree with the known formulas for the mean and variance
of a gamma random variable \eqref{eq:gamma-mean} and \eqref{eq:gamma-var}.
The others involve integrals we don't know how to do other than by
the method of cumulant functions (what we just did).
(But Mathematica knows how to do these integrals.)
% Mathematica
% dist = GammaDistribution[alpha, 1 / lambda]
% f[x_] = PDF[dist, x]
% Integrate[ f[x], {x, 0, Infinity}, Assumptions -> alpha > 0 && lambda > 0 ]
% moo = Integrate[ x f[x], {x, 0, Infinity},
%     Assumptions -> alpha > 0 && lambda > 0 ]
% voo = Integrate[ (x - moo)^2 f[x], {x, 0, Infinity},
%     Assumptions -> alpha > 0 && lambda > 0 ]
% loo = Integrate[ Log[x] f[x], {x, 0, Infinity},
%     Assumptions -> alpha > 0 && lambda > 0 ]
% uoo = Integrate[ (Log[x] - loo)^2 f[x], {x, 0, Infinity},
%     Assumptions -> alpha > 0 && lambda > 0 ]
% woo = Integrate[ (Log[x] - loo) (x - moo) f[x], {x, 0, Infinity},
%     Assumptions -> alpha > 0 && lambda > 0 ]

The mean value parameter is the two-dimensional vector $\xi$ having components
\begin{align*}
   \xi_1
   & =
   - \frac{\theta_2}{\theta_1}
   \\
   \xi_2
   & =
   \mydigamma(\theta_2) - \log(- \theta_1)
\end{align*}

The addition rules for the gamma distribution were given in the preceding
section (they do not depend on which parameters are considered known or
unknown).

The canonical sample space for sample size one is
$$
   S = \set{ y \in \real^2 : y_1 = e^{y_2} }
$$
which is a curve.  Thus any intersection with a tangent line consists of
a single point.  And, because this is a continuous distribution, single points
have probability zero.  Thus in the limiting conditional model theorem
(Theorem~\ref{th:completion-fundamental} above) we are in the case where
the probability of $Y \in H_\delta$ is equal to zero
(regardless of what $\delta$ is).  So there are no LCM for this family.

For sample size one, the data are on the curve $S$ with probability one.
So the MLE does not exist with probability one.  This is no surprise.
One cannot estimate two parameters from one variable $x$.

For sample size greater than one, the data are in the interior of the convex
hull of the curve $S$ with probability one.
So the MLE exists with probability one.  This is no surprise either.
One can estimate the two parameters of the normal distribution from
data $x_1$, $x_2$, $\ldots$, $x_n$ for $n \ge 2$.

So we don't need the theory of limiting conditional models for this family.
That theory is only needed for discrete families.

The last three paragraphs almost exactly repeat what was said about
the normal-location-scale family.  The reasoning applies to any continuous
distribution.  The details distinguishing one of these families from the
other do not matter in this argument.

\section{K-Truncated Families}

R package \code{aster} implements $k$-truncated Poisson and $k$-truncated
negative binomial for any nonnegative integer $k$.  Example~{3} in
\citet{aster2} used two-truncated negative binomial.  We no longer think
%%%%%%%%%% NEED BACKWARD REFERENCE to subsampling %%%%%%%%%%
using this family is the best way to do this example.  Moreover we know
of no other examples in life history analysis that need these families
(except for the zero-truncated ones already discussed).  Therefore
we omit further discussion of them.

