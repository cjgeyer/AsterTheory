
\chapter{Families}

\section{Bernoulli}
\label{sec:bernoulli}

A random variable is \emph{Bernoulli} if its possible values are zero and one.
In other words, every Bernoulli random variable is zero-or-one-valued,
and vice versa.

This is the \emph{rationale} for the distribution, any dichotomous (two-valued)
random variable can be coded as Bernoulli.

This is a \emph{discrete} random variable.

This is a special case of the binomial distribution, which we do next.

\section{Binomial}
\label{sec:binomial}

A random variable is \emph{binomial} if it is the sum of IID Bernoulli
random variables.  Hence the Bernoulli distribution is the binomial
distribution for sample size one (for one term in the sum).

The \emph{probability mass function} is
\begin{equation} \label{eq:binomial-pmf}
   f(y) = \binom{n}{y} p^y (1 - p)^{n - y}, \qquad y = 0, 1, \ldots, n,
\end{equation}
where $p$ is the \emph{usual parameter}, the probability that any of the
$n$ Bernoulli random variables in the sum is equal to one.

The \emph{mean} and \emph{variance} are
\begin{align*}
   E(y) & = n p
   \\
   \var(y) & = n p (1 - p)
\end{align*}

This is an \emph{exponential family}.  From \eqref{eq:binomial-pmf}
the log likelihood is
$$
   l(\theta) = y \log(p) + (n - y) \log(1 - p)
   = y \cdot \log\left(\frac{p}{1 - p}\right) + n \log(1 - p)
$$
from which we see that we have an exponential family with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log\left(\frac{p}{1 - p}\right)
$$
The right-hand side is so important that it is given a name.  The
$\logit$ function (pronounced low-jit) is given by
$$
   \logit(p) = \log\left(\frac{p}{1 - p}\right), \qquad 0 < p < 1.
$$
Its inverse function is
$$
   \logit^{-1}(\theta) = \frac{e^\theta}{1 + e^\theta},
   \qquad - \infty < \theta < \infty.
$$

The \emph{cumulant function} is
\begin{align*}
   c(\theta)
   & =
   - n \log(1 - p)
   \\
   & =
   - n \log\left(1 - \frac{e^\theta}{1 + e^\theta}\right)
   \\
   & =
   - n \log\left(\frac{1}{1 + e^\theta}\right)
   \\
   & =
   n \log\left(1 + e^\theta\right)
\end{align*}
We check that this has the correct derivatives
$$
   c'(\theta) = \frac{n e^\theta}{1 + e^\theta} = n p
$$
and
$$
   c''(\theta)
   =
   \frac{n e^\theta}{1 + e^\theta}
   - \frac{n e^\theta e^\theta}{(1 + e^\theta)^2}
   =
   \frac{n e^\theta}{1 + e^\theta}
   \left[ 1 - \frac{e^\theta}{1 + e^\theta} \right]
   =
   n p (1 - p)
$$

The \emph{mean value parameter} is $\xi = n p$.

The \emph{canonical parameter space} is the range of the $\logit$ function,
which is the whole real line, $- \infty < \theta < \infty$.

The \emph{mean value parameter space} is $n$ times the domain
of the $\logit$ function $0 < \xi < n$.

Theorem~\ref{th:completion-fundamental} says limiting conditional models
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[0, n]$, and its
boundary consists of two points $0$ and $n$.

Thus there are two limiting conditional models, one of which contains only
the distribution concentrated at zero and one of which contains only
the distribution concentrated at $n$.

It is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  It is true (Section~\ref{sec:define-expfam} above)
that a cumulant function plus an affine function is another cumulant function.
But it could be very confusing if we do not use the formula derived from
taking limits in the original model.

So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \Pr\nolimits_\theta(Y = 0)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log (1 - p)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log \left(\frac{1}{1 + e^\theta}\right)
   \\
   & =
   0
\end{align*}
and
\begin{align*}
   c_{+ 1}(\theta)
   & =
   c(\theta) + \Pr\nolimits_\theta(Y = n)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log (p)
   \\
   & =
   n \log\left(1 + e^\theta\right)
   +
   n \log \left(\frac{e^\theta}{1 + e^\theta}\right)
   \\
   & =
   n \theta
\end{align*}

The sum of $m$ independent and identically distributed
binomial random variables with sample size $n$ and usual parameter $p$
has the binomial distribution with sample size $m n$ and usual parameter $p$.

Hence if $y_{p(j)} \longrightarrow y_j$ is a binomial arrow for sample size $n$
the conditional distribution of $y_j$ given $y_{p(j)}$ is binomial
for sample size $n y_{p(j)}$.

This family is not implemented in either R package \code{aster} or
R package \code{aster2}.  Only the $n = 1$ special case, the Bernoulli
family is implemented.

\section{Poisson}
\label{sec:poisson}

A random variable is \emph{Poisson} if it has
the \emph{probability mass function}
\begin{equation} \label{eq:poisson-pmf}
   f(y) = \frac{\xi^y}{y !} e^{- \xi}, \qquad y = 0, 1, 2, \ldots,
\end{equation}
where $\xi$ is the \emph{usual parameter}, which turns out to be the
mean and variance of the distribution.

This is a \emph{discrete} random variable.

There are two rationales for this distribution, both so closely related
that they are almost one rationale.  First, the Poisson distribution is
an approximation to the $\text{binomial}(n, p)$ distribution when $n$
is very large and $p$ is very small and the mean $n p$ is moderate sized.
An example is a lottery.  Every week millions, sometimes hundreds of millions
of tickets are sold; that's $n$, the probability of any one ticket winning
is very small, for example, for the Powerball lottery, the probability is
one over 292,201,338 (as we write this, the rules change from time to time);
that's $p$; so $n p$ is moderate sized.  In weeks where the jackpot is small
and few tickets are sold, there are still tens of millions of tickets sold,
so $n p$ is less than one but not really small.  In weeks where the jackpot
is large, there may be many hundreds of millions
of tickets sold, so $n p$ is greater
than one and multiple winners are expected (they split the jackpot among them).
But regardless, the distribution of the number of winners is well approximated
by the $\text{Poisson}(n p)$ distribution.

Before we can discuss the second rationale, we discuss the \emph{addition rule:}
the sum of independent Poisson random variables is again Poisson.
It is not required that the independent Poisson random variables be
identically distributed.  Since the expectation of a sum is the sum of the
expectations, the sum of independent Poisson random variables having
means $\xi_1$, $\ldots,$ $\xi_n$ has
the $\text{Poisson}(\xi_1 + \cdots + \xi_n)$ distribution.

It follows (not obviously, but the derivation can be found in books
about spatial point processes) that the sum of $n$ independent Bernoulli
random variables is well approximated by a Poisson distribution provided
$n$ is very large and the means of all of the Bernoulli random variables
are very much smaller than the mean of the Poisson random variable.
Again, if the means of the Bernoulli random variables
are $\xi_1$, $\ldots,$ $\xi_n$, then the mean of the Poisson random variable
is $\xi_1 + \cdots + \xi_n$.  So we are assuming that each $\xi_i$ is very
much smaller than the sum.  To return to our lottery example, it does not
matter that each player is playing the same game.  So long as the expectation
of any one ticket winning is negligible compared to the expected number of
winners (for all tickets), the distribution of the number of winners will
be approximately Poisson.

Let's take a biological example.  Suppose we are counting ants, and we
have divided up the region in which we are counting ants with a very fine
grid.  If our grid is fine enough, the probability of counting more than one
ant in a grid cell will be negligible, perhaps impossible (if our grid cells
are so small that more than one ant could not fit).  Then the number of
ants in any one cell is a Bernoulli (zero-or-one-valued) random variable,
and the number of ants in any region that contains a very large number
of grid cells is very well approximated by the Poisson distribution.
If we take the limit as the size of the grid cells goes to zero we
get exact Poisson distributions.  Except that we forgot to mention
independence.  This assumes the Bernoulli random variables are independent,
that where one ant is has nothing whatsoever to do with where any other ant
is.  If we can accept this independence assumption, then the count of
ants in any region of any size large enough to have a moderate sized
expected number of ants can be assumed Poisson.

Now we abstract away from ants to be counting any things in regions of
any dimension.  The number of stars visible to the naked eye in a region
of sky, the number of raisins in slice of carrot cake, the number of white
blood cells in a drop of blood on a microscope slide, the number of ants
in a square meter region of your back yard, the number of leaves on a tree,
the number of calls arriving at a call center in a specified time interval,
and many other things can be assumed Poisson.

The independence assumption is crucial, pheromone trails and perhaps other
phenomena may make our counts of ants noticeably non-Poisson.  But if it
can be plausibly asserted that the probability of any one thing being counted
is independent of all the other things counted or not counted, then the
distribution of the total count is Poisson.

And even if the distribution of a count random variable fails to be exactly
Poisson due to some failure of the independence assumption, the Poisson
distribution may still may be a pretty good approximation (or may fail badly
if the independence assumption is grossly wrong).

As stated above, the \emph{mean} and \emph{variance} are
\begin{align*}
   E(y) & = \xi
   \\
   \var(y) & = \xi
\end{align*}

This is an \emph{exponential family}.  From \eqref{eq:poisson-pmf}
the log likelihood is
$$
   l(\theta) = y \log(\xi) - \xi
$$
(the term $\log(y !)$ can be dropped because it does not contain the
parameter), from which we see that we have an exponential family with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(\xi),
$$
so
$$
   \xi = e^{\theta}.
$$

The \emph{cumulant function} is
$$
   c(\theta) = \xi = e^\theta
$$
We check that this has the correct derivatives (and this is trivial)
$$
   c'(\theta) = e^\theta = \xi
$$
and
$$
   c''(\theta) = e^\theta = \xi
$$

The \emph{mean value parameter} is also the usual parameter $\xi$.

The \emph{canonical parameter space} is the range of the $\log$ function,
which is the whole real line, $- \infty < \theta < \infty$.

\begin{sloppypar}
The \emph{mean value parameter space} is the domain of the log function
\mbox{$0 < \xi < \infty$}.
\end{sloppypar}

\begin{sloppypar}
\emph{Thinning rule:} in the following graph
$$
\begin{CD}
   y_1 @>\text{Poi}>> y_2 @>\text{Ber}>> y_3
\end{CD}
$$
the conditional distribution of $y_3$ given $y_1$ (both arrows combined)
is $\text{Poisson}(\xi_3 \xi_2)$.  A thinned Poisson process is another
Poisson process, where ``thinning'' means we take each ``point'' counted
and accept or reject it independently with the same probability.
\end{sloppypar}

As discussed at the end of the preceding section, LCM
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[0, \infty)$, and its
boundary consists of the single point $0$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at zero.

Also as discussed at the end of the preceding section,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \Pr\nolimits_\theta(Y = 0)
   \\
   & =
   e^\theta 
   +
   \log (e^{- \xi})
   \\
   & =
   e^\theta 
   - \xi
   \\
   & =
   0
\end{align*}
So, again as in the preceding section, the cumulant function for the LCM
concentrated at zero is the zero function.

As mentioned in Section~\ref{sec:infinitely-divisible} above,
the Poisson distribution is infinitely divisible.
This is easily verified from its cumulant function.
For any positive real number $r$
$$
   r c(\theta) = r e^{\theta} = e^{\theta + \log(r)}
$$
is a cumulant function.  In fact, it is a cumulant function for the Poisson
family.  One log likelihood for the Poisson family is
$$
   l(\theta) = y \theta - e^\theta
$$
but if we make the substition $\theta = \psi + \log(r)$ we get
$$
   l(\psi) = y \psi + y \log(r) - e^{\psi + \log(r)}
$$
and we can drop the term that does not contain the new parameter $\psi$
obtaining
$$
   l(\psi) = y \psi - e^{\psi + \log(r)}
$$
and we see this has exponential family form with canonical statistic $y$,
canonical parameter $\psi$ anc cumulant function $c(\psi) = e^{\psi + \log(r)}$.

This is just a special case of the fact,
noted without proof in Section~\ref{sec:define-expfam},
that adding a constant to a canonical parameter gives
another canonical parameter.

Another way of thinking about this fact is that our new parameterization
just puts an offset $\log(r)$ in the exponential family.
But we know from Section~\ref{sec:canonical-affine-submodel} above
that canonical affine submodels of full exponential families are again
exponential families.

Note that in going from Section~\ref{sec:bernoulli}
to Section~\ref{sec:binomial} we just went from the family having cumulant
function $c(\theta) = 1 + e^\theta$ to the family
having cumulant function $n c(\theta)$,
something we know from Section~\ref{sec:iid} above is always valid.
So we might think that we would need another section to go from the
family having cumulant function $c(\theta) = e^\theta$ to the family
having cumulant function $r c(\theta)$, which is valid only when the
family is infinitely divisible.  But we have just found that that does
not give us a new family, but rather the same old Poisson family
(with an offset), so we do not need a new section for a new family.

\section{Zero-Truncated Poisson}
\label{sec:zero-truncated-poisson}

The \emph{zero-truncated Poisson} distribution is the Poisson distribution
conditioned on being nonzero.

The \emph{rationale} is that it can be used to incorporate zero-inflated
Poisson random variables into aster models.

This is a \emph{discrete} distribution.

If $f$ is the PMF of the Poisson distribution, then the PMF of
the zero-truncated Poisson distribution is
\begin{equation} \label{eq:zero-truncated-poisson-pmf-in-terms-of-poisson}
   g(y) = \frac{f(y)}{1 - f(0)}, 
   \qquad y = 1, 2, \ldots,
\end{equation}
that is, if $m$ is the mean of the untruncated Poisson distribution, then
the PDF of the zero-truncated Poisson distribution is
\begin{equation} \label{eq:zero-truncated-poisson-pmf}
   g(y) = \frac{m^y e^{- m}}{y ! (1 - e^{- m})}, 
   \qquad y = 1, 2, \ldots.
\end{equation}

Since this is not a ``brand name distribution'' the mean and variance
cannot just be looked up.  The \emph{mean} is
\begin{equation} \label{eq:mean-of-zero-truncated-in-terms-of-untruncated}
\begin{split}
   E(y)
   & =
   \sum_{y = 1}^\infty y g(y)
   \\
   & =
   \frac{1}{1 - f(0)} \sum_{y = 1}^\infty y f(y)
   \\
   & =
   \frac{1}{1 - f(0)} \sum_{y = 0}^\infty y f(y)
   \\
   & =
   \frac{m}{1 - f(0)}
\end{split}
\end{equation}
and
\begin{equation}
\label{eq:second-ordinary-moment-of-zero-truncated-in-terms-of-untruncated}
\begin{split}
   E(y^2)
   & =
   \sum_{y = 1}^\infty y^2 g(y)
   \\
   & =
   \frac{1}{1 - f(0)} \sum_{y = 1}^\infty y^2 f(y)
   \\
   & =
   \frac{1}{1 - f(0)} \sum_{y = 0}^\infty y^2 f(y)
   \\
   & =
   \frac{m + m^2}{1 - f(0)}
\end{split}
\end{equation}
because the infinite sum in the next-to-last line is $E(y^2)$ when $y$
is Poisson, and $E(y^2) = \var(y) + E(y)^2$ for any random variable.
Hence the \emph{variance} is
\begin{align*}
   \var(y)
   & =
   E(y^2) - E(y)^2
   \\
   & =
   \frac{m + m^2}{1 - f(0)} - \left( \frac{m}{1 - f(0)} \right)^2
\end{align*}
% Mathematica
% dist = PoissonDistribution[m]
% f[y_] = PDF[dist, y]
% g[y_] = f[y] / (1 - f[0])
% Sum[ g[y], {y, 1, Infinity} ]
% moo = Sum[ y g[y], {y, 1, Infinity} ]
% moo - m / (1 - f[0])
% Simplify[%]
% voo = Sum[ (y - moo)^2 g[y], {y, 1, Infinity} ]
% voo - ((m + m^2) / (1 - f[0]) - (m / (1 - f[0]))^2)
% Simplify[%]

This is an \emph{exponential family}.
From \eqref{eq:zero-truncated-poisson-pmf} the log likelihood is
$$
   l(\theta) = y \log(m) - m - \log(1 - e^{- m})
$$
(the term $\log(y !)$ can be dropped because it does not contain the
parameter), from which we see that we have an exponential family with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(m),
$$
so
$$
   m = e^{\theta},
$$
the relation between $\theta$ and $m$ being the same as for the Poisson
distribution.

But the \emph{usual parameter} $m$ is not the \emph{mean value parameter},
which is
\begin{equation} \label{eq:zero-truncated-poisson-theta-to-xi}
   \xi = \frac{m}{1 - f(0)} = \frac{m}{1 - e^{- m}}
   = \frac{\exp(\theta)}{1 - \exp(- \exp(\theta))}
\end{equation}
as we know from general exponential family theory,
%%%%%%%%%% NEED BACKWARD REFERENCE to mean value parameterization %%%%%%%%%%
the mapping $\theta \mapsto \xi$ given by the formula above is
strictly increasing and invertible and both it and its inverse mapping
$\xi \to \theta$ are infinitely differentiable.  But in this case
the inverse mapping $\xi \to \theta$ seems to have no closed-form expression.
% can Mathematica find closed-form expression?
% using stuff from above
% foo[m_] = moo
% bar[theta_] = foo[Exp[theta]]
% Solve[ bar[theta] == xi, theta ]
% Nope!  It has no clue.
The map $\xi \to \theta$ is what is called a \emph{link function} in the
terminology of generalized linear models (GLM).  The failure of some
families to have link functions in useful form is one reason why aster
model theory and practice never mentions link functions.  They make
sense for some families but not others.

The \emph{cumulant function} is
\begin{equation} \label{eq:zero-truncated-poisson-cumfun}
   c(\theta) = m + \log(1 - e^{- m})
   = e^\theta + \log(1 - \exp(- \exp(\theta)))
\end{equation}
We check that this has the correct derivatives
\begin{align*}
   c'(\theta)
   & =
   e^\theta 
   +
   \frac{\exp(- \exp(\theta)) \exp(\theta)}{1 - \exp(- \exp(\theta))}
   \\
   & =
   m + \frac{m e^{- m}}{1 - e^{- m}}
   \\
   & =
   \frac{m}{1 - e^{- m}}
\end{align*}
and
\begin{align*}
   c''(\theta)
   & =
   e^\theta 
   +
   \frac{\exp(- \exp(\theta)) \exp(\theta)}{1 - \exp(- \exp(\theta))}
   -
   \frac{\exp(- \exp(\theta)) \exp(\theta)^2}{1 - \exp(- \exp(\theta))}
   \\
   & \quad
   -
   \frac{\exp(- \exp(\theta))^2 \exp(\theta)^2}{(1 - \exp(- \exp(\theta)))^2}
   \\
   & =
   m + \frac{m e^{- m}}{1 - e^{- m}}
   - \frac{m^2 e^{- m}}{1 - e^{- m}}
   - \frac{m^2 e^{- 2 m}}{(1 - e^{- m})^2}
\end{align*}
% Mathematica
% voo - (m + m Exp[- m] / (1 - Exp[- m]) - m^2 Exp[- m] / (1 - Exp[- m]) -
%     m^2 Exp[- m]^2 / (1 - Exp[- m])^2)
% Simplify[%]
and this does simplify to be equal to our other expression for variance.

Two other formulas for the variance are also useful \citep{geyer-3701}.
\begin{subequations}
\begin{align}
   \var(y) & = \xi (1 + m - \xi)
   \label{eq:first-convenient-variance-formula}
   \\
   & = \xi (1 - \xi e^{- m})
   \label{eq:second-convenient-variance-formula}
\end{align}
\end{subequations}
% Mathematica
% voo - moo (1 + m - moo)
% Simplify[%]
% voo - moo (1 - moo Exp[- m])
% Simplify[%]
As $\theta \to - \infty$ and $m \to 0$ the mean value parameter $\xi$ converges
(using L'Hospital's rule) to
$$
   \lim_{m \to 0} \frac{m}{1 - e^{- m}} = \lim_{m \to 0} \frac{1}{e^{m}} = 1
$$
and \eqref{eq:first-convenient-variance-formula} shows the variance converges
to zero as $m \to 0$ and $\xi \to 1$.
As $\theta \to \infty$ and $m \to \infty$ the mean value parameter $\xi$
is approximately equal to $m$ because $f(0) = e^{- m}$ is approximately zero.
Then $\xi e^{- m}$ is small compared to $\xi \approx m$, and
and \eqref{eq:second-convenient-variance-formula} shows the variance is
also approximately equal to $\xi \approx m$.

As we said above, the \emph{mean value parameter} $\xi$ is not
the usual parameter $m$.

As can be seen from that fact that \eqref{eq:zero-truncated-poisson-cumfun}
is finite for all $\theta$,
the \emph{canonical parameter space} is
the whole real line, $- \infty < \theta < \infty$.

As we saw when discussing variance formulas, $\xi \to 1$ as $m \to 0$.
Thus the lower end of the mean value parameter space is one.
And from $m$ being the mean of a Poisson distribution so $m$ has
no upper bound, and from $m \approx \xi$ when either is large, we see that
$\xi$ also has no upper bound.  Thus
the \emph{mean value parameter space} is $1 < \xi < \infty$.

As discussed at the end of the two preceding sections, LCM
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[1, \infty)$, and its
boundary consists of the single point $1$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at one.

Also as discussed at the end of the two preceding sections,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \Pr\nolimits_\theta(Y = 1)
   \\
   & =
   m + \log(1 - e^{- m})
   +
   \log \left(\frac{m e^{- m}}{1 - e^{- m}}\right)
   \\
   & =
   \log(m)
   \\
   & =
   \theta
\end{align*}
So the cumulant function of the distribution concentrated at one is the
identity function.

This just happens to agree with the $n = 1$ case for the binomial
distribution (Section~\ref{sec:binomial} above), but it need not have.
It all depends on how we defined the cumulant functions for these families
in the first place.  We could have added different arbitrary constants
to the cumulant functions of these families and they would still be
cumulant functions.

\section{Normal Location}

\begin{sloppypar}
The univariate normal distribution has \emph{probability density function}
(PDF)
\begin{equation} \label{eq:normal-pdf}
   f(y) = \frac{1}{\sqrt{2 \pi} \sigma} e^{- \frac{(y - \xi)^2}{2 \sigma^2}},
   \qquad - \infty < y < \infty.
\end{equation}
\end{sloppypar}

This is a \emph{continuous} random variable.
(Except when incorporated into an aster model, it is a mixture of discrete
and continuous.  For a normal-location arrow, when the predecessor is zero
the conditional distribution of the successor is the degenerate random variable
concentrated at zero, which is discrete, and when the predecessor is greater
than zero, the conditional distribution of the successor is continuous.)

The \emph{rationale} is the celebrated central limit theorem,
or more precisely, theorems, because there are many variants.
In non-technical terms these theorems say that a random variable that is the sum
of a large number of random variables that are not too dependent,
not too heavy tailed, and not too unequal in size will be well approximated
by a normal distribution.  (If the random variable in question is the
sum of a large number of independent random variables, then Lindeberg's
central limit theorem using Lindeberg's condition specifies what
``not too heavy tailed, and not too unequal in size'' means.
If the random variable in question is the
sum of the components of a dependent stochastic process, then various
stationary process central limit theorems, Markov chain central limit theorems,
and the martingale central limit theorem, give various notions of what
``not too dependent'' means.)
\begin{quotation}
Everybody believes in the law of errors, the experimenters because they
think it is a mathematical theorem, the mathematicians because they
think it is an experimental fact.
\\
\hspace*{\fill} --- Lippman, quoted by Poincar\'{e}, quoted by \citet{cramer}
\end{quotation}
``The law of errors'' is an old name for the normal distribution.
It has also been named after de Moivre, Laplace, and Gauss.
The term ``normal distribution'' was popularized by K. Pearson in the early
twentieth century.  Like the term ``law of errors'' it builds into the name
the idea that it is the main, principle, or only distribution for random data.
Also note the Lippman quote is sarcastic.  Justification
for this belief was always known to be shaky.
(Harald Cram\'{e}r and Henri Poincar\'{e} are, of course, famous.  It is
unclear who the Monsieur Lippman was that Poincar\'{e} attributed this to.)

Since the nonparametrics revolution \citep{hollander-wolfe-chicken},
the exploratory data analysis revolution \citep{tukey},
the bootstrap revolution \citep{efron-tibshirani,davison-hinkley},
and the robustness revolution \citep{huber-ronchetti,hampel-et-al}
no user of statistics aware of these developments wants to blindly
assume normality, especially when it can be demonstrated to be grossly
incorrect using any of these tools.  But the normal distribution may
fit data well, so it continues to be used.  It just is no longer considered
the only distribution for data, as it was before 1950 (mostly, there was
the chi-square test for contingency tables).

The other rationale for this distribution (which has nothing to do with
aster models) is that the usual assumption of homoscedastic normal errors
for linear models makes the distribution of point estimates exactly normal
and the distribution of various test statistics exactly $t$ or exactly $F$.
This rationale is often attributed to Gauss and is why the normal distribution
is sometimes called Gaussian, because Gauss independently co-invented the
method of least squares and more-or-less gave this rationale (more-or-less
because his discussion was Bayesian rather than frequentist), but of course
this was a century before the $t$ and $F$ distributions were invented.

The \emph{mean} and \emph{variance} are
\begin{align*}
   E(y) & = \xi
   \\
   \var(y) & = \sigma^2
\end{align*}
When used in R package \code{aster} every family must be a one-parameter
exponential family of distributions, so when we consider this as such a family
we must pick one parameter to be treated as unknown
and the other parameter to be treated as known.
Because the location parameter $\xi$ is the mean value parameter,
we pick this to be the unknown parameter.

With this understanding, the log likelihood is
$$
   l(\theta) = - \frac{(y - \xi)^2}{2 \sigma^2}
$$
(the term $\sqrt{2 \pi} \sigma$ can be dropped because it does not contain the
unknown parameter $\xi$.  If we expand the quadratic, we get
$$
   l(\theta)
   =
   - \frac{y^2}{2 \sigma^2}
   + \frac{y \xi}{\sigma^2}
   - \frac{\xi^2}{2 \sigma^2}
$$
and can now drop another term not containing $\xi$ obtaining
$$
   l(\theta)
   =
   \frac{y \xi}{\sigma^2} - \frac{\xi^2}{2 \sigma^2}
$$
from which we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \frac{\xi}{\sigma^2}
$$
so the \emph{mean value parameter} is
$$
   \xi = \sigma^2 \theta
$$
The \emph{cumulant function} is
$$
   c(\theta) = \frac{\xi^2}{2 \sigma^2}
   = \frac{\sigma^2 \theta^2}{2}
$$
We check that this has the correct derivatives
$$
   c'(\theta) = \sigma^2 \theta = \xi
$$
and
$$
   c''(\theta) = \sigma^2
$$

\emph{Addition rule:} the sum of $n$ independent and identically distributed
normal random variables with mean $\xi$ and variance $\sigma^2$ has the normal
distribution with mean $n \xi$ and variance $n \sigma^2$.

\emph{General Addition rule:} any sum of independent
normal random variables is again normal
(identically distributed is not required), but this has no application
in aster model theory.

There are no limit degenerate distributions.
This is because the boundary of the closed convex support,
which is the interval $(- \infty, + \infty)$ is empty.
We can never observe data on the boundary.

\section{Negative Binomial}
\label{sec:negative-binomial}

\subsection{Basics}
\label{sec:negative-binomial-basics}

According to the \code{help("NegBinomial")} in R, the negative binomial
distribution has \emph{probability mass function}
\begin{equation} \label{eq:negative-binomial-pmf}
   f(y) = \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!} p^\alpha (1-p)^y,
   \qquad y = 0, 1, 2, \ldots.
\end{equation}
where $\alpha > 0$ is the shape parameter and $0 < p \le 1$ is the
usual parameter (success probability).  The case $\alpha = 1$ is the
geometric distribution.

The \emph{first rationale} for this distribution is inverse sampling,
and for this rationale $\alpha$ must be a positive integer.
If one has an infinite
sequence of IID Bernoulli random variables with usual parameter $p$,
then the distribution of the number of observed zero outcomes before the
$\alpha$-th nonzero outcome is negative binomial with shape parameter $\alpha$
and usual parameter $p$, that is, if one observes $y$ successes in $n$ trials,
then the distribution of $y$ is binomial if $n$ was fixed and the distribution
of $n - y$ is negative binomial if $y$ was fixed.
But this rationale has nothing to do with aster models.

The \emph{second rationale} for this distribution is overdispersed Poisson.
This distribution arises as a mixture of Poisson distributions,
as is discussed below (Section~\ref{sec:mixture}).
This is the reason it is implemented in R package
\code{aster}.  For this rationale $\alpha$ can be any positive real number.

The \emph{mean} and \emph{variance} in terms of these parameters are
\begin{align*}
   E(y) & = \frac{\alpha (1 - p)}{p}
   \\
   \var(y) & = \frac{\alpha (1 - p)}{p^2}
\end{align*}

From \eqref{eq:negative-binomial-pmf} the log likelihood is
$$
   l(\theta)
   =
   \log \Gamma(\alpha + y) - \log \Gamma(\alpha) - \log(y!)
   + \alpha \log(p) + y \log(1-p).
$$
from which we can see that if $\alpha$ is considered an unknown parameter,
this is \emph{not} an exponential family, so we consider $\alpha$ known,
which means we can drop terms not containing $p$ obtaining
$$
   l(\theta)
   =
   y \log(1 - p) + \alpha \log(p) 
$$
from which we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(1 - p)
$$
and solving for $p$ gives
$$
   1 - p = e^\theta
$$
and
$$
   p = 1 - e^\theta
$$

The \emph{cumulant function} is
$$
   c(\theta)
   =
   - \alpha \log(p) 
   =
   - \alpha \log(1 - e^\theta)
$$
As $p$ goes from zero to one, $\theta$ goes from zero to $- \infty$ so
the formula above does not define the cumulant function on the whole real
line and equation (5) in \citet{geyer-gdor}, which is \eqref{eq:cumfun-expfam}
in this book, must be used
\begin{equation*}
   c(\theta) = c(\psi) +
   \log E_{\psi}\bigl( e^{y (\theta - \psi)} \bigr)
\end{equation*}
where $\psi$ is a fixed canonical parameter value, $\theta$ varies
over the whole real line, and the cumulant function has the value $\infty$
where the expectation does not exist.

Evaluating this we get, using the theorem associated with the negative
binomial distribution \citep{brand-name-distributions},
\begin{equation} \label{eq:negative-binomial-cumfun-derivation}
\begin{split}
   c(\theta)
   & =
   c(\psi) +
   \log \left( \sum_{y = 0}^\infty
   e^{y (\theta - \psi)} \cdot
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!} p^\alpha (1-p)^y
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   p^\alpha
   \sum_{y = 0}^\infty
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ (1-p) e^{\theta - \psi} \right]^y
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   p^\alpha
   \left[ 1 - (1-p) e^{\theta - \psi} \right]^{- \alpha}
   \right)
\end{split}
\end{equation}
where $p$ is the usual parameter value corresponding to canonical parameter
value $\psi$, that is, $\psi = \log(1 - p)$ and the formula is only valid
when the infinite sequence converges, which it does if and only if
$-1 < (1-p) e^{\theta - \psi} < +1$.

Now $1 - p = e^\psi$, so we can simplify $(1-p) e^{\theta - \psi} = e^\theta$.
So the convergence criterion is $e^\theta < 1$ or $\theta < 0$, and the
formula simplifies to
$$
   c(\theta)
   =
   c(\psi)
   +
   \alpha \log(p)
   -
   \alpha \log \left( 1 - e^\theta \right)
$$
but the formula only determines the cumulant function up to an arbitrary
constant (which does not matter) so we can take the cumulant function to be
\begin{equation} \label{eq:cumfun-negative-binomial}
   c(\theta)
   =
   \begin{cases}
   - \alpha \log(1 - e^\theta), & \theta < 0
   \\
   \infty, & \theta \ge 0
   \end{cases}
\end{equation}
So the full canonical parameter space is, as we guessed before,
\begin{equation} \label{eq:canonical-parameter-space-negative-binomial}
   \Theta = \set{ \theta \in \real : \theta < 0 }
\end{equation}
and \eqref{eq:cumfun-negative-binomial} agrees with what we derived just
from looking at the log likelihood wherever the function is finite.

Let's check that this cumulant function gives the correct mean and variance.
\begin{align*}
   c'(\theta)
   & =
   \frac{\alpha e^\theta}{1 - e^\theta}
   \\
   & =
   \frac{\alpha (1 - p)}{p}
   \\
   c''(\theta)
   & =
   \frac{d}{d \theta}
   \frac{\alpha e^\theta}{1 - e^\theta}
   \\
   & =
   \frac{\alpha e^\theta}{1 - e^\theta}
   -
   \frac{\alpha e^{2 \theta}}{(1 - e^\theta)^2}
   \\
   & =
   \frac{\alpha (1 - p)}{p}
   \left(
   1
   -
   \frac{1 - p}{p}
   \right)
   \\
   & =
   \frac{\alpha (1 - p)}{p^2}
\end{align*}
as we had already been told but now have derived
from exponential family theory.

The \emph{mean value parameter}
\begin{equation} \label{eq:negative-binomial-mean-value}
   \xi = \frac{\alpha (1 - p)}{p}
\end{equation}
is not the usual parameter $p$.  Solving for $p$ gives
\begin{equation} \label{eq:negative-binomial-usual}
   p = \frac{\alpha}{\alpha + \xi}
\end{equation}
% Mathematica
% Solve[ xi == alpha (1 - p) / p, p ]

The \emph{canonical parameter space}
is \eqref{eq:canonical-parameter-space-negative-binomial}
which is not the whole real line.

The \emph{mean value parameter space} is the range of the derivative of
the cumulant function $0 < \xi < \infty$.

As discussed at the end of the Sections~\ref{sec:binomial},
\ref{sec:poisson}, and~\ref{sec:zero-truncated-poisson}, LCM
are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[0, \infty)$, and its
boundary consists of the single point $0$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at zero.

Also as discussed at the end of the those sections,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \Pr\nolimits_\theta(Y = 0)
   \\
   & =
   - \alpha \log(1 - e^\theta)
   +
   \log \left(p^\alpha\right)
   \\
   & =
   0
\end{align*}
So the cumulant function of the family concentrated at zero is the zero
function, as we also found in Sections~\ref{sec:binomial} and~\ref{sec:poisson}
above.  But as mentioned at the end of Section~\ref{sec:zero-truncated-poisson}
above, this agreement just happened because of arbitrary choices of arbitrary
constants in cumulant functions.

\subsection{Negative Binomial as Mixture of Poisson}
\label{sec:mixture}

As stated above, one rationale for the negative binomial distribution
that it is a mixture of Poisson distributions.  Let the conditional
distribution of $Y$ given $\mu$ be Poisson with mean $\mu$,
and let the marginal distribution of $\mu$ be $\text{Gamma}(\alpha, \lambda)$.
Then the marginal distribution of $Y$ is given by
\begin{align*}
   f(y)
   & =
   \int f(y \mid \mu) g(\mu) \, d \mu
   \\
   & =
   \int_0^\infty \frac{\mu^y e^{- \mu}}{y!} \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)} \mu^{\alpha - 1} e^{- \lambda \mu}
   \, d \mu
   \\
   & =
   \frac{1}{y!}
   \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)}
   \int_0^\infty \mu^{y + \alpha - 1} e^{- (1 + \lambda) \mu}
   \, d \mu
   \\
   & =
   \frac{1}{y!}
   \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)}
   \frac{\Gamma(y + \alpha)}{(1 + \lambda)^{y + \alpha}}
\end{align*}
using the theorem associated with the gamma distribution
\citep{brand-name-distributions}.

For this to be equal to \eqref{eq:negative-binomial-pmf} we need
$$
   \frac{1}{y!}
   \cdot
   \frac{\lambda^\alpha}{\Gamma(\alpha)}
   \frac{\Gamma(y + \alpha)}{(1 + \lambda)^{y + \alpha}}
   =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!} p^\alpha (1-p)^y
$$
that is
$$
   \frac{\lambda^\alpha}{(1 + \lambda)^{y + \alpha}}
   =
   p^\alpha (1-p)^y
$$
or
$$
   \left(\frac{\lambda}{1 + \lambda}\right)^\alpha
   \left(\frac{1}{1 + \lambda}\right)^y
   =
   p^\alpha (1-p)^y
$$
which happens if and only if $p = \lambda / (1 + \lambda)$ and
$1 - p = 1 / (1 + \lambda)$ so $\lambda = p / (1 - p)$.

\subsection{Poisson as Limit of Negative Binomial}

Reparameterize the negative binomial distribution so the parameters
are $\alpha$ and $\xi$ so the usual parameter is
\eqref{eq:negative-binomial-usual}
and the PMF \eqref{eq:negative-binomial-pmf} becomes
\begin{align*}
   f(y)
   & =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left(\frac{\alpha}{\alpha + \xi}\right)^\alpha
   \left(\frac{\xi}{\alpha + \xi}\right)^y
   \\
   & =
   \frac{1}{y!}
   \left(\frac{\alpha}{\alpha + \xi}\right)^\alpha
   \left(\frac{\xi}{\alpha + \xi}\right)^y
   \prod_{k = 1}^y (\alpha + k - 1)
   \\
   & =
   \frac{\xi^y}{y!}
   \left(1 - \frac{\xi}{\alpha + \xi}\right)^\alpha
   \prod_{k = 1}^y \frac{\alpha + k - 1}{\alpha + \xi}
   \\
   & \to
   \frac{\xi^y}{y!} e^{- \xi}
\end{align*}
so as $\alpha \to \infty$ with $\xi$ fixed, we recover the Poisson
distribution.  But this does not happen if we let $\alpha \to \infty$
with some other parameter, such as $p$ or $\theta$, fixed.
(The limit $\left(1 - \frac{\xi}{\alpha + \xi}\right)^\alpha \to e^{- \xi}$
as $\alpha \to \infty$, which was used in the derivation above, follows
from $(1 + x / n)^n \to e^x$ as $n \to \infty$, which can be found
in any calculus book.)

The upshot of this section is that if the shape parameter $\alpha$
of a negative binomial distribution is large, then it is well approximated
by a Poisson distribution.  One only needs the negative binomial family
when the shape parameter is small.

The limit in this section is not like the limits in other sections of
this appendix.  In those other sections we took limits as the canonical
parameter of the exponential family went to plus or minus infinity.
Since the canonical parameter is considered unknown, this kind of
limit can arise in the process of maximum likelihood.
%%%%%%%%%% NEED BACKWARD REFERENCE to limiting conditional models %%%%%%%%%%
In this section we took a limit as the shape parameter $\alpha$ went
to infinity.
Since this parameter is considered known, this kind of
limit cannot arise in the process of maximum likelihood.

\section{Zero-Truncated Negative Binomial}

This section is just like Section~\ref{sec:zero-truncated-poisson} above
\emph{mutatis mutandis}.

The \emph{zero-truncated negative binomial} distribution is
the negative binomial distribution conditioned on being nonzero.

The \emph{rationale} is that it can be used to incorporate zero-inflated
negative binomial random variables into aster models.

This is a \emph{discrete} distribution.

\begin{sloppypar}
If $f$ is the PMF of the negative binomial distribution, then the PMF of
the zero-truncated negative binomial distribution is
\begin{equation}
\label{eq:zero-truncated-negative-binomial-pmf-in-terms-of-negative-binomial}
   g(y) = \frac{f(y)}{1 - f(0)},
   \qquad y = 1, 2, \ldots,
\end{equation}
that is, if $\alpha$ is the shape parameter and $p$ is the usual parameter
of the untruncated negative binomial distribution, then
the PDF of the zero-truncated negative binomial distribution is
\begin{equation} \label{eq:zero-truncated-negative-binomial-pmf}
   g(y) =
   \frac{\Gamma(\alpha + y) p^\alpha (1-p)^y}
   {\Gamma(\alpha) \, y! \, (1 - p^\alpha)},
   \qquad y = 1, 2, \ldots.
\end{equation}
\end{sloppypar}

Since this is not a ``brand name distribution'' the mean and variance
cannot just be looked up.  The \emph{mean} is again given by
\eqref{eq:mean-of-zero-truncated-in-terms-of-untruncated} except
that now $m$ is the mean of the untruncated negative binomial distribution,
that is,
$$
   E(y) = \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
$$
and we still have
\eqref{eq:second-ordinary-moment-of-zero-truncated-in-terms-of-untruncated}
except that $m + m^2$ in the last line, which is variance plus mean squared
of the untruncated Poisson distribution needs to be replaced by
variance plus mean squared of the untruncated negative binomial distribution,
which is
$$
   \frac{\alpha (1 - p)}{p^2} - \left( \frac{\alpha (1 - p)}{p} \right)^2
   =
   \frac{\alpha (1 - p) [1 - \alpha (1 - p)]}{p^2}
$$
so
$$
   E(y^2)
   =
   \frac{1}{1 - p^\alpha}
   \left[ \frac{\alpha (1 - p)}{p^2} + \left( \frac{\alpha (1 - p)}{p}
   \right)^2 \right]
$$
Hence the \emph{variance} is
$$
   \var(y)
   =
   \frac{1}{1 - p^\alpha}
   \left[ \frac{\alpha (1 - p)}{p^2} + \left( \frac{\alpha (1 - p)}{p}
   \right)^2 \right]
   -
   \left( \frac{\alpha (1 - p)}{p (1 - p^\alpha)} \right)^2
$$
% Mathematica
% dist = NegativeBinomialDistribution[alpha,p]
% f[y_] = PDF[dist, y]
% g[y_] = f[y] / (1 - f[0])
% Sum[ g[y], {y, 1, Infinity} ]
% moo = Sum[ y g[y], {y, 1, Infinity} ]
% moo - alpha (1 - p) / (p (1 - p^alpha))
% Simplify[%]
% voo = Sum[ (y - moo)^2 g[y], {y, 1, Infinity} ]
% voo - (1 / (1 - p^alpha) (alpha (1 - p) / p^2 + (alpha (1 - p) / p)^2) -
%     (alpha (1 - p) / (p (1 - p^alpha)))^2)
% Simplify[%]
%
% Oops!
% mooUntrunc = Sum[ y f[y], {y, 0, Infinity} ]
% vooUntrunc = Sum[ (y - mooUntrunc)^2 f[y], {y, 0, Infinity} ]

With the assumptions of Section~\ref{sec:negative-binomial} above
($\alpha$ known and $p$ unknown) this is an exponential family.
From \eqref{eq:zero-truncated-negative-binomial-pmf} the log likelihood is
$$
   l(\theta)
   =
   \log \Gamma(\alpha + y) + \alpha \log(p) + y \log(1-p)
   - \log \Gamma(\alpha) - \log(y!) - \log(1 - p^\alpha)
$$
and we may drop terms that do not contain the unknown parameter $p$ obtaining
$$
   l(\theta)
   =
   y \log(1-p) + \alpha \log(p) - \log(1 - p^\alpha)
$$
from which we see that we have an \emph{exponential family} with
\emph{canonical statistic} $y$ and \emph{canonical parameter}
$$
   \theta = \log(1 - p)
$$
(the same as for the untruncated negative binomial distribution)
and solving for $p$ gives
$$
   p = 1 - e^\theta
$$
(the same as for the untruncated negative binomial distribution).

The \emph{cumulant function} is
$$
   c(\theta)
   =
   - \alpha \log(p) + \log(1 - p^\alpha)
   =
   - \alpha \log(1 - e^\theta) + \log(1 - (1 - e^\theta)^\alpha)
$$
As in Section~\ref{sec:negative-binomial} this does not define the
cumulant function on the whole real line so we use
\begin{align*}
   c(\theta)
   & =
   c(\psi) +
   \log \left( \sum_{y = 1}^\infty
   e^{y (\theta - \psi)} \cdot
   \frac{\Gamma(\alpha + y) p^\alpha (1-p)^y}
   {\Gamma(\alpha) \, y! \, (1 - p^\alpha)}
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   \frac{p^\alpha}{1 - p^\alpha}
   \sum_{y = 1}^\infty
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ (1-p) e^{\theta - \psi} \right]^y
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   \frac{p^\alpha}{1 - p^\alpha}
   \left\{
   - 1 +
   \sum_{y = 0}^\infty
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ (1-p) e^{\theta - \psi} \right]^y
   \right\}
   \right)
   \\
   & =
   c(\psi) +
   \log \left(
   \frac{p^\alpha}{1 - p^\alpha}
   \left\{
   - 1 +
   \bigl[ 1 - (1-p) e^{\theta - \psi} \bigr]^{- \alpha}
   \right\}
   \right)
\end{align*}
where the last equality is the theorem associated with the negative
binomial distribution \citep{brand-name-distributions}
and where, as in \eqref{eq:negative-binomial-cumfun-derivation}, $p$ is
the usual parameter that goes with $\psi$ not $\theta$ so $p$ is a known
constant and $(1-p) e^{\theta - \psi} = e^\theta$ and the infinite series
converges if and only if $\theta < 0$.
Thus our formula simplifies to
$$
   c(\theta) = c(\psi) + \log\left( \frac{p^\alpha}{1 - p^\alpha} \right)
   +
   \log\left(- 1 + (1 - e^\theta)^{- \alpha} \right)
$$
and we may drop the terms that do not contain $\theta$ obtaining
$$
   c(\theta)
   =
   \begin{cases}
   \log\left((1 - e^\theta)^{- \alpha} - 1\right), & \theta < 0 \\
   \infty, & \theta \ge 0
   \end{cases}
$$
With some rearrangement, this agrees with what we deduced from looking
at the log likelihood.
% Mathematica
% ( - alpha Log[1 - Exp[theta]] + Log[1 - (1 - Exp[theta])^alpha] ) -
%     Log[(1 - Exp[theta])^(- alpha) - 1]
% Simplify[%]
% FullSimplify[%]
% PowerExpand[%]

Let's check that this cumulant function gives the correct mean and variance.
\begin{align*}
   c'(\theta)
   & =
   \frac{- \alpha (1 - e^\theta)^{- \alpha - 1} (- e^\theta)}
   {(1 - e^\theta)^{- \alpha} - 1}
   \\
   & =
   \frac{\alpha (1 - e^\theta)^{- \alpha - 1} e^\theta}
   {(1 - e^\theta)^{- \alpha} - 1}
   \\
   & =
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
   \\
   & =
   \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   \\
   c''(\theta)
   & =
   \frac{d}{d \theta}
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
   \\
   & =
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
   +
   \frac{\alpha e^\theta e^\theta}{(1 - e^\theta)^2 [1 - (1 - e^\theta)^\alpha]}
   \\
   & \qquad
   - \frac{\alpha^2 e^\theta e^\theta (1 - e^\theta)^{\alpha - 1}
   }{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]^2}
   \\
   & =
   \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   +
   \frac{\alpha (1 - p)^2}{p^2 (1 - p^\alpha)}
   -
   \frac{\alpha^2 (1 - p)^2 p^{\alpha - 2}}{(1 - p^\alpha)^2}
\end{align*}
% Mathematica
% (1 / 1 - p^alpha (alpha (1 - p) / p^2 + (alpha (1 - p) / p)^2) -
%     (alpha (1 - p) / (p (1 - p^alpha)))^2) -
%     (alpha (1 - p) / (p (1 - p^alpha)) +
%     alpha (1 - p)^2 / (p^2 (1 - p^alpha)) -
%     alpha^2 p^alpha (1 - p)^2 / (p^2 (1 - p^alpha)^2))
% Simplify[%]
% FullSimplify[%]
% PowerExpand[%]
%
% Oops!
%
% c[theta_] = Log[(1 - Exp[theta])^(- alpha) - 1]
% cp[theta_] = D[c[theta], theta]
% cp[theta_] = Simplify[cp[theta]]
% cpp[theta_] = D[cp[theta], theta]
% cpp[Log[1 - p]]
%
% alpha (1 - p) / (p (1 - p^alpha)) + alpha (1 - p)^2 / (p^2 (1 - p^alpha)) -
%     alpha^2 (1 - p)^2 p^(alpha - 2) / (1 - p^alpha)^2 - cpp[Log[1 - p]]
% Simplify[%]
% alpha (1 - p) / (p (1 - p^alpha)) + alpha (1 - p)^2 / (p^2 (1 - p^alpha)) -
%     alpha^2 (1 - p)^2 p^(alpha - 2) / (1 - p^alpha)^2 - voo
% Simplify[%]
And after some rearrangement $c''(\theta)$ agrees with the variance
calculated above.

The \emph{mean value parameter}
\begin{equation} \label{eq:zero-truncated-negative-binomial-mean-value}
   \xi
   =
   \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   =
   \frac{\alpha e^\theta}{(1 - e^\theta) [1 - (1 - e^\theta)^\alpha]}
\end{equation}
is not the usual parameter $p$.  As with zero-truncated Poisson,
we find that the inverse mapping $\xi \to \theta$ has no closed-form
expression.
% Mathematica
% Solve[ xi == cpp[theta], theta ]
As stated in Section~\ref{sec:zero-truncated-poisson} above
we know from general exponential family theory
that this mapping $\xi \to \theta$ is
%%%%%%%%%% NEED BACKWARD REFERENCE to mean value parameterization %%%%%%%%%%
strictly increasing, invertible, and infinitely differentiable.
But in this case the inverse mapping $\xi \to \theta$ seems to have
no closed-form expression.
Also as stated in Section~\ref{sec:zero-truncated-poisson} above,
this means that this family does not have what GLM theory calls a link function
in any useful form.

The full canonical parameter space is
\eqref{eq:canonical-parameter-space-negative-binomial}
as it was for the negative binomial.

Taking the limit in
\eqref{eq:zero-truncated-negative-binomial-mean-value}
as $\theta \to - \infty$ and $p \to 1$ we see that
$$
   \lim_{p \uparrow 1} \frac{\alpha (1 - p)}{p (1 - p^\alpha)}
   =
   \lim_{p \uparrow 1} \frac{- \alpha}{1 - p^\alpha - p \alpha p^{\alpha - 1}}
   =
   1
$$
(using L'Hospital's rule).
Taking the limit in
\eqref{eq:zero-truncated-negative-binomial-mean-value}
as $\theta \to \infty$ and $p \to 0$ we see that $\xi \to \infty$ in
this case.  And since we know from general exponential family theory
that $c'(\theta)$ is a continuous increasing function, it follows
that the \emph{mean value parameter space} is $1 < \xi < \infty$.

As discussed at the end of the Sections~\ref{sec:binomial},
\ref{sec:poisson}, \ref{sec:zero-truncated-poisson},
and~\ref{sec:negative-binomial-basics},
LCM are conditioned on the boundary of the closed convex support.
The closed convex support is the closed interval $[1, \infty)$, and its
boundary consists of the single point $1$.

Thus there is one limiting conditional model, which contains only
the distribution concentrated at one.

\REVISED

Also as discussed at the end of the those sections,
it is important that we use \eqref{eq:cumfun-lcm} to determine the cumulant
function for the LCM.  So
\begin{align*}
   c_{- 1}(\theta)
   & =
   c(\theta) + \Pr\nolimits_\theta(Y = 1)
   \\
   & =
   \log\left((1 - e^\theta)^{- \alpha} - 1\right)
   +
   \log\left(\frac{\alpha p^\alpha (1 - p)}{1 - p^\alpha}\right)
   \\
   & =
   \log\left(p^{- \alpha} - 1\right)
   +
   \log\left(\frac{\alpha p^\alpha (1 - p)}{1 - p^\alpha}\right)
   \\
   & =
   \log\left(\frac{p^\alpha}{1 - p^\alpha}\right)
   +
   \log\left(\frac{\alpha p^\alpha (1 - p)}{1 - p^\alpha}\right)
\end{align*}
So the cumulant function of the family concentrated at zero is the zero
function, as we also found in Sections~\ref{sec:binomial} and~\ref{sec:poisson}
above.  But as mentioned at the end of Section~\ref{sec:zero-truncated-poisson}
above, this agreement just happened because of arbitrary choices of arbitrary
constants in cumulant functions.

\REVISED

Taking the limit in
\eqref{eq:zero-truncated-negative-binomial-pmf}
as $\theta \to - \infty$ and $p \to 1$ we see that
\begin{align*}
   \lim_{p \uparrow 1} g(y)
   & =
   \lim_{p \uparrow 1}
   \frac{\Gamma(\alpha + y) p^\alpha (1-p)^y}
   {\Gamma(\alpha) \, y! \, (1 - p^\alpha)}
   \\
   & =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \lim_{p \uparrow 1}
   \frac{p^\alpha (1-p)^y}{1 - p^\alpha}
   \\
   & =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \left[ \lim_{p \uparrow 1} p^\alpha \right]
   \left[ \lim_{p \uparrow 1} \frac{(1-p)^y}{1 - p^\alpha} \right]
   \\
   & =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \lim_{p \uparrow 1}
   \frac{(1-p)^y}{1 - p^\alpha}
   \\
   & =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \lim_{p \uparrow 1}
   \frac{- y (1-p)^{y - 1}}{- \alpha p^{\alpha - 1}}
\end{align*}
where the third equality is the limit of a product is the product of the limits
(if the limits exist, which further calculations show), the fourth equality
is $p^\alpha \to 1$ as $p \to 1$, and the fifth equality is L'Hospital's rule.
Now we have two cases
$$
   \lim_{p \uparrow 1} g(1)
   =
   \alpha \lim_{p \uparrow 1}
   \frac{- 1}{- \alpha p^{\alpha - 1}}
   =
   1
$$
and for $y > 1$
$$
   \lim_{p \uparrow 1} g(y)
   =
   \frac{\Gamma(\alpha + y)}{\Gamma(\alpha) \, y!}
   \lim_{p \uparrow 1}
   \frac{- y (1-p)^{y - 1}}{- \alpha p^{\alpha - 1}}
   =
   0
$$
so as $\theta \to - \infty$ and $m \to 0$ and $\xi \to 1$
the zero-truncated negative binomial distribution converges to
the degenerate distribution concentrated at one.
This LCM is discussed in Section~\ref{app:sec:degenerate} above.
There are no other limit degenerate distributions.

\section{Multinomial}
\label{app:sec:multinomial}

Every family preceding this one is one-dimensional and is implemented
or could be implemented in R package \code{aster}.
This family is multi-dimensional and hence cannot be implemented in R package
\code{aster}.
It is implemented in R package \code{aster2}.
When incorporated in an aster model, this family is a \emph{dependence group}.

When IID individuals (a simple random sample) are classified into mutually
exclusive and exhaustive categories (every individual fall in exactly one
category), the vector of category counts has
the \emph{multinomial distribution}.
This is one \emph{rationale} for this distribution.

The other \emph{rationale} is that when the sample size (predecessor)
is equal to one, this family serves as a $k$-way switch if there are $k$
categories.
%%%%%%%%%% NEED BACKWARD REFERENCE to k-way switch %%%%%%%%%%

This is the distribution of a random vector, not a random variable.

The Bernoulli and binomial distributions are related to the multinomial
distribution, but the multinomial distribution with two categories is
still a two-dimensional random vector, so it is not Bernoulli or binomial,
which are one-dimensional distributions (of random variables).

This is a \emph{discrete} random vector.

The \emph{probability mass function} is
\begin{equation} \label{eq:multinomial-pmf}
   f(y) = \frac{n !}{\prod_{i \in I} y_i!} \prod_{i \in I} p_i^{y_i},
   \qquad y \in S,
\end{equation}
where $p$ is the \emph{usual parameter vector}, which is a probability
vector satisfying
\begin{gather*}
   p_i \ge 0
   \\
   \sum_{i \in I} p_i = 1
\end{gather*}
where $y$ is the the vector of counts (nonnegative integers) satisfying
\begin{subequations}
\begin{gather}
   y_i \ge 0
   \label{eq:multinomial-canonical-statistic-constraint-positivity}
   \\
   \sum_{i \in I} y_i = n
   \label{eq:multinomial-canonical-statistic-constraint-sum-to-n}
\end{gather}
\end{subequations}
where $n$ is the sample size (the number of IID individuals classified),
where $S$ is the sample space for $y$, the set
$$
   S = \bigset{ y \in \nats^I : \sum_{i \in I} y_i = n},
$$
where $\nats$ denotes the \emph{natural numbers} $\{0, 1, 2, 3, \ldots\}$,
and where we have chosen the index set of $y$ and $\theta$ to be an
arbitrary finite set $I$ rather than $\{1, \ldots, k\}$ for some $k$
to fit in with the conventions of aster models (in an aster model $I$
would be a subset of nodes of the aster graph comprising a multinomial
dependence group).
%%%%%%%%%% NEED BACKWARD REFERENCE to subvectors %%%%%%%%%%

The \emph{mean vector} and \emph{variance matrix} have components
\begin{subequations}
\begin{alignat}{2}
   E(y_i) & = n p_i
   \label{eq:mean-vector-components}
   \\
   \var(y_i) & = n p_i (1 - p_i)
   \label{eq:variance-matrix-components-diagonal}
   \\
   \cov(y_i, y_j) & = - n p_i p_j, & \qquad & i \neq j
   \label{eq:variance-matrix-components-off-diagonal}
\end{alignat}
\end{subequations}
The mean of a random vector $y$ is the vector whose components are the
means of the components of $y$.  Here $E(y) = \xi$ and $\xi_i = n p_i$.
The variance of a random vector $y$ is the matrix whose components are the
covariances of the components of $y$.  Here $\var(y) = M$ has components
$m_{i j}$ which are given by \eqref{eq:variance-matrix-components-diagonal}
when $i = j$ and by \eqref{eq:variance-matrix-components-off-diagonal} when
$i \neq j$.

Other names for \emph{variance matrix} are \emph{covariance matrix} (because
the elements are actually covariances, including the diagonal elements because
$\cov(y_i, y_i) = \var(y_i)$, \emph{variance-covariance} matrix (because
the diagonal elements are variances and the off-diagonal elements are
covariances), and \emph{dispersion matrix}.  We think ``covariance matrix''
is a particularly bad name because if the term is used up in this context,
then what does one call the covariance matrix of two random vectors, which
is an important concept in multivariate statistics?

This is an \emph{exponential family}.  From \eqref{eq:multinomial-pmf}
the log likelihood is
\begin{equation} \label{eq:multinomial-logl-try-one}
   l(\theta) = \sum_{i \in I} y_i \log(p_i)
\end{equation}
from which we see that we have an exponential family with
\emph{canonical statistic vector} $y$ and \emph{canonical parameter vector}
$\theta$ having components
\begin{equation} \label{eq:multinomial-canonical-try-one}
   \theta_i = \log(p_i), \qquad i \in I.
\end{equation}

Trying to read the cumulant function off of \eqref{eq:multinomial-logl-try-one}
seems to say $c(\theta)$ is the constant function everywhere equal to zero,
and this is correct because the $p_i$ must sum to one, and as we shall see
the cumulant function does have the value zero
when
\begin{equation} \label{eq:multinomial-canonical-constraint-try-one}
   \sum_{i \in I} e^{\theta_i} = 1.
\end{equation}

But we want the cumulant function defined on the whole vector space where
$\theta$ lives, so we must use equation (5) in \citet{geyer-gdor},
which is \eqref{eq:cumfun-expfam} in this book,
\begin{align*}
   c(\theta)
   & =
   c(\psi)
   +
   \log E_\psi\left\{ e^{\sum_{i \in I} y_i (\theta_i - \psi_i)} \right\}
   \\
   & =
   c(\psi) + \log \sum_{y \in S}
   e^{\sum_{i \in I} y_i (\theta_i - \psi_i)}
   \cdot
   \frac{n !}{\prod_{i \in I} y_i!} \prod_{i \in I} p_i^{y_i}
   \\
   & =
   c(\psi) + \log \sum_{y \in S}
   \frac{n !}{\prod_{i \in I} y_i!}
   \prod_{i \in I} (p_i e^{\theta_i - \psi_i})^{y_i}
   \\
   & =
   c(\psi) + \log \left( \sum_{i \in I} p_i e^{\theta_i - \psi_i} \right)^n
   \\
   & =
   c(\psi) + n \log \left( \sum_{i \in I} p_i e^{\theta_i - \psi_i} \right)
\end{align*}
where the last equality is the multinomial theorem also called the theorem
associated with the multinomial distribution \citep{brand-name-distributions}.
Here $\psi$ is a possible value of the canonical parameter vector (held fixed)
and $p$ is the usual parameter vector corresponding to it so $p_i = e^{\psi_i}$
and $p_i e^{\theta_i - \psi_i} = e^{\theta_i}$.  So dropping $c(\psi)$,
which is an arbitrary constant, we obtain
\begin{equation} \label{eq:multinomial-cumfun}
   c(\theta) = n \log \left( \sum_{i \in I} e^{\theta_i} \right)
\end{equation}
and this gives a log likelihood valid on the whole vector space where
$\theta$ lives
\begin{equation} \label{eq:multinomial-logl}
\begin{split}
   l(\theta) & = \inner{y, \theta} - c(\theta)
   \\
   & = \left( \sum_{i \in I} y_i \theta_i \right)
   - \log \left( \sum_{i \in I} e^{\theta_i} \right)^n
\end{split}
\end{equation}
We check that this has the correct derivatives
\begin{subequations}
\begin{align}
   \frac{\partial c(\theta)}{\partial \theta_i}
   & = 
   \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
   \label{eq:multinomial-cumfun-first-derivatives}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_i^2}
   & = 
   \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
   -
   \frac{n e^{\theta_i} e^{\theta_i}}
   {\left( \sum_{j \in I} e^{\theta_j} \right)^2}
   \label{eq:multinomial-cumfun-second-derivatives-diagonal}
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_i \theta_j}
   & = 
   -
   \frac{n e^{\theta_i} e^{\theta_j}}
   {\left( \sum_{k \in I} e^{\theta_k} \right)^2}
   \label{eq:multinomial-cumfun-second-derivatives-off-diagonal}
\end{align}
\end{subequations}
But here we have a problem that this does not even make sense with our
previous notion of canonical parameters, which, recall, was defined
by \eqref{eq:multinomial-canonical-try-one} but only subject to the
constraint \eqref{eq:multinomial-canonical-constraint-try-one}.
So we do not have a notion of what the map $\theta \to p$ should be
for values of $\theta$ that do not satisfy the
constraint \eqref{eq:multinomial-canonical-constraint-try-one}.

We solve the problem by taking \eqref{eq:multinomial-cumfun-first-derivatives}
to give the correct mean values
$$
   \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}} = n p_i
$$
so
\begin{equation} \label{eq:multinomial-canonical-to-usual}
   p_i = \frac{e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}, \qquad i \in I.
\end{equation}
Note that this function is not invertible.  If one adds the same constant
to all of the $\theta_i$, then the value of $p_i$ does not change
$$
   \frac{e^{\theta_i + c}}{\sum_{j \in I} e^{\theta_j + c}}
   =
   \frac{e^c e^{\theta_i}}{e^c \sum_{j \in I} e^{\theta_j}}
   =
   \frac{e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
$$

This illustrates another thing wrong with the concept of the link function:
it forces the canonical parameterization to be identifiable even when this
is inadvisable.  Using our choice of parameterization here there can be
no link function because the map $\theta \to \xi$ is not one-to-one, so
its inverse mapping does not exist (an that inverse mapping is supposed to
be the ``link'' function).

We also clear up a mystery left hanging and check
that \eqref{eq:multinomial-cumfun} does indeed evaluate to zero when the
constraint \eqref{eq:multinomial-canonical-constraint-try-one} holds.

Having made the identification \eqref{eq:multinomial-canonical-to-usual}
we see that \eqref{eq:multinomial-cumfun-second-derivatives-diagonal}
and \eqref{eq:multinomial-cumfun-second-derivatives-off-diagonal}
do give the correct variances and covariances
\begin{align*}
   \frac{\partial^2 c(\theta)}{\partial \theta_i^2}
   & =
   \var(y_i) = n p_i (1 - p_i)
   \\
   \frac{\partial^2 c(\theta)}{\partial \theta_i \partial \theta_j}
   & =
   \cov(y_i, y_j) = - n p_i p_j
\end{align*}

The canonical parameter space is the whole vector space where $\theta$ lives.

The mean value parameter vector, as stated above, is the vector $\xi$ having
components $\xi_i = n p_i$.  In case $n = 1$, the mean value parameter vector
is the usual parameter vector.  Otherwise, not.

The mean value parameter space is the relative interior of $n$ times the
convex hull of $S$
$$
   \Xi
   =
   \bigset{ \xi \in \real^n : \xi_i > 0, i \in I \opand
   \sum_{j \in I} \xi_j = n }
$$
To see this we note that $\theta_i = \log(\xi_i)$ always defines a point
in the canonical sample space so long as the $\xi_i$ are strictly positive.
and that point maps via \eqref{eq:multinomial-canonical-to-usual} to
$$
   p_i = \frac{e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}
   =
   \frac{\xi_i}{\sum_{j \in I} \xi_j} = \frac{\xi_i}{n}
$$
which makes the $p_i$ sum to one.
So every point in $\Xi$ is a mean value parameter vector value,
and, conversely, every point $\theta \in \real^I$ maps to a value $\xi$
that satisfies $\xi_i > 0$ for all $i$ and $\sum_{i \in I} \xi_i = n$.

Taking limits in \eqref{eq:multinomial-pmf} as $p_i \to 0$ for
$i \in A$ where $A \subsetneq I$
$$
   f_A(y) = \begin{cases} 1, & y_i = 0, i \in A \\ 0, & \text{otherwise}
   \end{cases}
$$
which is the PMF of the distribution which is partially or wholly degenerate
(next section).  The distribution of the random vector $y_A$
%%%%%%%%%% NEED BACKWARD REFERENCE to subvectors %%%%%%%%%%
is wholly degenerate concentrated at the point 0 (the vector having all
components zero),
and the distribution of the random vector $y_{A^c}$ has the multinomial
distribution with canonical parameter vector $\theta_{A^c}$ and mean
value parameter $\xi_{A^c}$,
and the random vectors $y_A$ and $y_{A^c}$ are stochastically independent
because a constant random vector is stochastically independent of any
random vector (even itself).  Thus the preceding sentence completely describes
the distribution of the random vector $y$.

\emph{Addition rule:} the sum of independent multinomial random vectors
having the same index set $i$ and the same usual parameter vector $p$ and
sample sizes $n_1$, $\ldots,$ $n_k$ has the multinomial
distribution with sample size $n_1 + \cdots + n_k$ and
usual parameter vector $p$.

\section{Multivariate Degenerate} \label{app:sec:multivariate-degenerate}

This section is just like Section~\ref{app:sec:degenerate} above
\emph{mutatis mutandis}.

A random vector is \emph{degenerate} if it is concentrated in a hyperplane
of the vector space where it lives.  If it is concentrated at one point
we say it is \emph{wholly degenerate}, otherwise \emph{partially degenerate}.

One \emph{rationale} for such random variables is that they arise as limits
(as we saw at the end of the preceding section).

Another \emph{rationale} for such random variables is that they are allowed
in probability theory.  The official definition of ``random vector'' in
probability theory is: a random vector is a vector-valued function
on the sample space.  Hence any vector-valued function of a random vector
is another random vector.  (This is just like any scalar-valued function
of a random variable is another random variable.)
But this gives us degenerate random vectors.
For example the function $x \mapsto (x, x)$ takes a non-degenerate random
variable $X$ to the degenerate random vector $(X, X)$ which is concentrated
on the hyperplane
$$
   \set{ (x, y) \in \real^2 : x = y }
$$
This may not seem to be a very interesting example, but it is not far removed
from interesting examples.  In probability theory we say that any linear
function of a multivariate normal random vector is another multivariate
normal random vector.  But this is only true if degenerate multivariate
normal random vectors are allowed \citep[Slides~118 ff.]{geyer-5101-deck5}.

A random vector is degenerate in the sense defined here if and only if
it is concentrated on a hyperplane in the vector space where it lives
\citep[Slide~119 ff.]{geyer-5101-deck5}.  For an exponential family
the canonical parameterization is not identifiable if and only if the
distribution of the canonical statistic is degenerate
\citep[Theorem~1]{geyer-gdor}.  Every multinomial distribution is degenerate
because of the constraint
\eqref{eq:multinomial-canonical-statistic-constraint-sum-to-n}.
This is why the canonical parameterization is not identifiable.

But a limiting conditional model of a multinomial distribution is even
more degenerate than the original (non-limit) multinomial model.
It satisfies additional constraints $y_i = 0$, $i \in A$ for some proper
subset of the index set $I$.

Aster models do not implement any degenerate families except multinomial,
so we restrict our discussion to that case.  We assert that the
cumulant function for the multinomial LCM just described is
\begin{equation} \label{eq:multinomial-cumfun-degenerate}
   c_A(\theta) = n \log \left( \sum_{i \in A^c} e^{\theta_i} \right)
\end{equation}
We prove that this is correct by evaluating limit formula in Theorem~6
of \citet{geyer-gdor}
\begin{align*}
   c_A(\theta)
   & =
   c(\theta) + \log \Pr( \set{y : y_i = 0, i \in A} )
   \\
   & =
   n \log \left( \sum_{i \in I} e^{\theta_i} \right)
   +
   n \log \left( \sum_{i \in A^c} p_i \right)
   \\
   & =
   n \log \left( \sum_{i \in I} e^{\theta_i} \right)
   +
   n \log \left( \sum_{i \in A^c} \frac{e^{\theta_i}}
   {\sum_{j \in I} e^{\theta_j}} \right)
\end{align*}
and after some cancellation, we see this equals
\eqref{eq:multinomial-cumfun-degenerate}.

We check that this gives the correct means
\begin{alignat*}{2}
   \frac{\partial c_A(\theta)}{\partial \theta_i} & = 0, & \qquad & i \in A
   \\
   \frac{\partial c_A(\theta)}{\partial \theta_i}
   & = \frac{n e^{\theta_i}}{\sum_{j \in I} e^{\theta_j}}, & \qquad & i \notin A
\end{alignat*}
and for once we will not bother with the details in checking the variances.

If $A$ is the complement of a singleton, that is, $A = \{i\}^c$, then
the LCM multinomial distribution described in this section is wholly degenerate
because then we know that $y_j = 0$ with probability one for $j \neq i$,
and we also know from
\eqref{eq:multinomial-canonical-statistic-constraint-sum-to-n}
that we must have $y_i = n$ with probability one.

This is why we required $A$ to be a proper subset of $I$.
If we had $y_i = 0$ for all $i$, then we could not satisfy
\eqref{eq:multinomial-canonical-statistic-constraint-sum-to-n}.

\section{Multivariate Bernoulli}

The multinomial distribution for sample size one is sometimes called
\emph{multivariate Bernoulli}.  This is the family that is actually
implemented in R package \code{aster2} (there is no sample size
hyperparameter).

If there were a need for multinomial families with sample size greater than
one, a sample size hyperparameter could be added to R function
\code{fam.multinomial} in that package with default one, that is,
the signature would be
\begin{verbatim}
fam.multinomial(dimension, sample.size = 1)
\end{verbatim}
But, as with the digits model discribed in Section~\ref{app:sec:degenerate}
we do not know of any need for such models in life history analysis.

Not even the authors know why this appendix was written the way it is,
with the long section titled ``Bernoulli'' describing the family
actually implemented and the short section ``Binomial'' describing the
family possibly but not actually implemented but, in contrast,
the long section titled ``Multinomial''
describing the family possibly but not actually implemented and
this short section describing the family actually implemented.

\section{Normal Location-Scale}

The univariate normal distribution is curious in that it remains an exponential
family even if we consider both parameters unknown, but then
the dimensions of the canonical statistic vector and the canonical parameter
vector must match.  So if the canonical parameter vector is going to be
two-dimensional so must be the canonical statistic vector.

Let's see how that happens.  We already have the probability density
function \eqref{eq:normal-pdf} of the normal distribution.
Now if we write down the log likelihood not dropping any terms that contain
either parameter we get
$$
   l(\theta)
   =
   - \log(\sigma) - \frac{(x - \nu)^2}{2 \sigma^2}
   =
   - \log(\sigma) - \frac{x^2}{2 \sigma^2}
   + \frac{x \nu}{\sigma^2}
   - \frac{\nu^2}{2 \sigma^2}
$$
where for reasons to be discussed presently we have changed the notation
for the random variable from $y$ to $x$ and the notation
for the mean from $\xi$ to $\nu$.

As was discussed in Section~\ref{sec:define-expfam} in the main text,
there is some freedom in choosing the canonical statistic vector and
the canonical parameter vector we must have the terms containing both
data and parameters in exponential family form, that is,
$$
   - \frac{x^2}{2 \sigma^2} + \frac{x \nu}{\sigma^2}
   =
   y_1 \theta_1 + y_2 \theta_2
$$
but that still allows lots of choices.
We could, for example, choose $y_1 = x$ or $y_1 = x^2$ or $y_1 = - x^2 / 2$.
And each such choice forces a different choice of the corresponding
canonical parameter.

The choice made in the implementation in R package \code{aster2} is
\begin{align*}
   y_1 & = x,
   \\
   y_2 & = x^2,
   \\
   \theta_1 & = \frac{\nu}{\sigma^2}
   \\
   \theta_2 & = - \frac{1}{2 \sigma^2}
\end{align*}
We have had many examples where the usual parameters are not the canonical
parameters.  Here is our first example where the usual statistics are not
the canonical statistics (the usual statistic is one canonical statistic
but not the other).

In terms of the canonical parameters, the usual parameters are
\begin{align*}
   \sigma^2 & = - \frac{1}{2 \theta_2}
   \\
   \nu & = \theta_1 \sigma^2 = - \frac{\theta_1}{2 \theta_2}
\end{align*}

The first canonical parameter is unrestricted $- \infty < \theta_1 < \infty$
but the second canonical parameter is restricted $- \infty < \theta_2 < 0$.
Thus our guess at the cumulant function from looking at the log likelihood
\begin{align*}
   c(\theta)
   & =
   \log(\sigma) + \frac{\nu^2}{2 \sigma^2}
   \\
   & =
   \frac{1}{2} \log\left(- \frac{1}{2 \theta_2}\right)
   +
   \left( - \frac{\theta_1}{2 \theta_2} \right)^2 \cdot \frac{1}{2}
   \cdot (- 2 \theta_2)
   \\
   & =
   \frac{1}{2} \log\left(\frac{1}{2}\right)
   -
   \frac{1}{2} \log(- \theta_2)
   -
   \frac{\theta_1^2}{4 \theta_2} 
\end{align*}
Because our guess at the cumulant function is not defined on a whole vector
space we use equation (5) in \citet{geyer-gdor}, which is \eqref{eq:cumfun-expfam}
in this book, as we have done several times before in this appendix
\begin{align*}
   c(\theta)
   & =
   c(\psi) + \log E_\psi
   \bigl( e^{y_1 (\theta_1 - \psi_1) + y_2 (\theta_2 - \psi_2)} \bigr)
   \\
   & =
   c(\psi) + \log E_\psi
   \bigl( e^{x (\theta_1 - \psi_1) + x^2 (\theta_2 - \psi_2)} \bigr)
   \\
   & =
   c(\psi) + \frac{2 \nu_\psi (\theta_1 - \psi_1 + \nu_\psi (\theta_2 - \psi_2))
   + (\theta_1 - \psi_1)^2 \sigma^2_\psi}
   {2 - 4 (\theta_2 - \psi_2) \sigma^2_\psi}
   \\
   & \qquad
   - \frac{1}{2} \log\left( - 2 (\theta_2 - \psi_2) + \frac{1}{\sigma^2_\psi}
   \right)
   - \frac{1}{2} \log\left( \sigma^2_\psi \right)
\end{align*}
% Mathematica
%
% dist = NormalDistribution[moo, Sqrt[voo]]
% f[x_] = PDF[dist, x]
% Integrate[f[x], {x, -Infinity, Infinity}, Assumptions -> voo > 0]
% Integrate[x f[x], {x, -Infinity, Infinity}, Assumptions -> voo > 0]
% Integrate[(x - moo)^2 f[x], {x, -Infinity, Infinity}, Assumptions -> voo > 0]
%
% cumfun[theta1_, theta2_] = Integrate[Exp[x theta1 + x^2 theta2] f[x],
%     {x, -Infinity, Infinity}, Assumptions -> voo > 0]

\section{Other Families Not Yet Implemented}

\section{K-Truncated Families}

