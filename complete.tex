
\chapter{Completion} \label{chap:completion}

In this chapter we deal with what to do when maximum likelihood estimates
do not exist in the exponential family or aster model we are initially given.
There may, and usually do, exist maximum likelihood estimates in the
\emph{completion} of the family.  It is a bit unclear what we should call
the statistical models studied in this chapter.
\begin{itemize}
\item \citet[Sections~9.3 and~9.4]{barndorff-nielsen} calls this concept
    \emph{completion}.
\item \citet[Chapter~6]{brown} calls this concept
    an \emph{aggregate exponential family} for reasons that will be explained
    presently.
\item \citet[Chapters~2 and~4]{geyer-thesis} calls this concept
    \emph{closure}.
\item \citet{geyer-gdor} calls this concept
    \emph{Barndorff-Nielsen completion}.
\end{itemize}
\citeauthor[personal communication]{brown} pointed out that the eponym chosen
in \citet{geyer-gdor} was not quite correct, since \citet{barndorff-nielsen}
works under more restrictive regularity conditions than \citet{brown}, and
\citet{brown} works under more restrictive regularity conditions than
\citet{geyer-thesis}.  The choice in \citet{geyer-gdor} follows
Stigler's law of eponomy.  At least in this case \citeauthor{barndorff-nielsen}
had the concept first if not in the most generality.
The reason why \citet{geyer-thesis} chose ``closure'' rather than ``completion''
is that when one works under the weakest regularity conditions, the topological
space that is the statistical model being ``completed'' is not metrizable,
hence ``complete'' (every Cauchy sequence converges) doesn't make any sense
(the definition of Cauchy sequence requires a metric).  Thus we have only
the more general topological concept of closure.
We won't fuss about any of this and will continue use Barndorff-Nielsen
completion or just completion.

\section{Binomial Example}

For this simplest example of the phenomenon of interest, we consider the
binomial distribution.  We know from the discussion
in Section~\ref{sec:direction-of-recession} above that the MLE does not exist
when the observed value of the canonical statistic, which for the binomial
distribution is the number of successes, is an extreme value, either as small
as it can be or as large as it can be, in this case either 0 or $n$, where
$n$ is the sample size.

Usually, we think the MLE for the usual parameter $p$, the success probability,
does exist for all data and is $\hat{p} = x / n$.  But when $x = 0$ or $x = n$,
so $\hat{p}$ is zero or one, the MLE for the canonical parameter
$\theta = \logit(p)$
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
does not exist because the domain of the logit function is the open interval
$(0, 1)$ and does not include the endpoints.  Since
\begin{align*}
   \lim_{p \downarrow 0} \logit(p) & = - \infty
   \\
   \lim_{p \uparrow 0} \logit(p) & = \infty
\end{align*}
we could try to identify these endpoints with infinite values of the canonical
parameter, but that is not the way exponential family theory works,
and, as we shall see, it does not generalize to multiparameter problems.

So instead of trying to complete the parameter space, we try to complete the
family of distributions.  These distributions have PMF
$$
   f_p(x) = \binom{n}{x} p^x (1 - p)^{n - x}
$$
and we have
\begin{align*}
   \lim_{p \downarrow 0} f_p(x) & = \begin{cases} 1, & x = 0 \\ 0, & x > 0
   \end{cases}
   \\
   \lim_{p \uparrow 1} f_p(x) & = \begin{cases} 0, & x < n \\ 1, & x = n
   \end{cases}
\end{align*}
so the completion contains the original exponential family we were given
plus two new distributions, the degenerate distribution concentrated at zero
and the degenerate distribution concentrated at $n$.  And these new
distributions are what are usually thought of as the binomial distributions
for $p = 0$ and $p = 1$ (when $p = 0$ no successes are possible so $x = 0$
almost surely; when $p = 1$ no failures are possible so $x = n$ almost surely).

\section{General Exponential Families}

\subsection{Support and Support Function}

Let $C$ denote the \emph{closed convex support} of the exponential family
under discussion.  This is the smallest closed convex set that contains
the canonical statistic vector with probability one.  Hence it is a closed
convex subset of the vector space where the canonical statistic takes values.

The closed convex support always exists because the intersection of closed sets
is closed and the intersection of convex sets is convex and because
finite-dimensional vector spaces are second countable.  Define $C$ to be
the intersection of all closed convex sets that contain the canonical
statistic vector $Y$ with probability one under some distribution in the
family, and hence for all distributions in the family (because all have
the same support).  Then event $Y \notin C$ is the
union of a countable family of open sets having probability zero,
hence $Y \in C$ almost surely.

Let $\sigma_C$ denote the \emph{support function} of $C$, defined by
\begin{equation} \label{eq:support-function}
   \sigma_C(\delta) = \sup_{y \in C} \inner{y, \delta}
\end{equation}
\citep[Section~8.E]{rockafellar-wets}.  The term ``support'' here is
unfortunate in that it is unrelated to the term ``support'' in $C$ being
a support of the canonical statistic vector of the exponential family.
But both terms are well established (``closed convex support'' in exponential
family theory and ``support function'' in convex analysis).

\subsection{Probability Mass-Density Functions}

If \eqref{eq:logl-expfam} is the log likelihood of an exponential family,
the the PMDF of that family must be the exponential of the log likelihood.
In order that we do not get extra terms that do not appear in the log
likelihood and in order to get the right support of the family, we take
the measure with respect to which we calculate densities to be a measure
in the family, say the measure corresponding to canonical parameter vector
$\psi$.  Then the PMDF are
\begin{equation} \label{eq:pmdf-expfam}
   f_\theta(\omega) = e^{\inner{Y(\omega), \theta - \psi} - c(\theta) + c(\psi)}
\end{equation}
where $\omega$ is the complete data (remember that $Y$ is a statistic,
not necessarily the complete data) \citep[Equation~(4)]{geyer-gdor}.

\subsection{Straight Line Limits}

\begin{theorem} \label{th:completion-fundamental}
For a full exponential family having log likelihood \eqref{eq:logl-expfam},
densities \eqref{eq:pmdf-expfam}, canonical statistic vector $Y$,
full canonical parameter space $\Theta$, and closed convex support $C$,
suppose $\delta$ is a direction in the vector space where the canonical
parameter takes values,
\begin{equation} \label{eq:complete-fundamental-hyperplane}
   H_\delta = \set{ y \in \real^J : \inner{y, \delta} = \sigma_C(\delta) },
\end{equation}
then for all $\theta \in \Theta$
\begin{equation} \label{eq:complete-fundamental-limit}
   \lim_{\theta + s \delta} f_{\theta + s \delta}(\omega)
   =
   \begin{cases}
   0, & \inner{Y(\omega), \delta} < \sigma_C(\delta)
   \\
   f_\theta(\omega) / \Pr_\theta(Y \in H_\delta),
   & \inner{Y(\omega), \delta} = \sigma_C(\delta)
   \\
   \infty, & \inner{Y(\omega), \delta} > \sigma_C(\delta)
   \end{cases}
\end{equation}
where the middle term is defined to be $\infty$ in case of divide by zero.
If $\delta$ is not a direction of constancy
and $\Pr_\theta(Y \in H_\delta) > 0$, then the function
$s \mapsto \Pr_{\theta + s \delta}(Y \in H_\delta)$ is continuous,
strictly increasing, and converges to one
as $s \to \infty$.
\end{theorem}
In the two cases ruled out by the precondition of the last sentence
the function $s \mapsto \Pr_{\theta + s \delta}(Y \in H_\delta)$ is
a constant function.  If $\delta$ is a direction of constancy,
then $\Pr_\theta(Y \in H_\delta) = 1$ for all $\theta$.
If $\Pr_\theta(Y \in H_\delta) = 0$ for some $\theta$,
then $\Pr_\theta(Y \in H_\delta) = 0$ for all $\theta$.
\begin{proof}
This is a complication of Theorem~{6} in \citet{geyer-gdor}
that is essentially Theorem~{2.3} in \citet{geyer-thesis}.
However the proof of that Theorem~{2.3} contains some errors,
so a corrected proof is given in the appendix of \citet{geyer-gdor}.
Then the case $\Pr_\theta(Y \in H_\delta) > 0$ is Theorem~{2.6}
in \citet{geyer-thesis},
and the case $\Pr_\theta(Y \in H_\delta) = 0$ follows from Theorem~{2.2}
in \citet{geyer-thesis}.
The last sentence of the theorem statement is in Corollary~{5} and Theorem~{6}
in \citet{geyer-gdor}.
\end{proof}
In case $\Pr_\theta(Y \in H_\delta) > 0$,
we note three things about the limit in \eqref{eq:complete-fundamental-limit}.
\begin{itemize}
\item It is a probability distribution because the set where it is infinite
    has measure zero under the dominating measure $\Pr_\psi$
\item It is a conditional distribution of the original family,
    the conditional distribution of $Y$ given the event $Y \in H_\delta$
    for the parameter vector $\theta$.
\item It is a limit distribution of the original family,
    the limit of the distributions for parameter vectors $\theta + s \delta$
    as $s \to \infty$.  By Scheff\'{e}'s lemma, convergence of PMDF implies
    convergence in total variation of the corresponding probability measures.
\end{itemize}
Note that the distribution under discussion is both a limit distribution
and a conditional distribution.  Thinking of it as just one or the other
is missing something.  It is both.

In case $\Pr_\theta(Y \in H_\delta) = 0$,
we note one thing about the limit in \eqref{eq:complete-fundamental-limit}.
\begin{itemize}
\item It is the zero measure because it is zero on the support of
the dominating measure $\Pr_\psi$.
\end{itemize}

\subsection{Limiting Conditional Models}

We note one thing about the set of all limits
in \eqref{eq:complete-fundamental-limit}
in the case when $\Pr_\theta(Y \in H_\delta) > 0$.
\begin{itemize}
\item They form an exponential family of distributions.  The log likelihood is
\begin{equation} \label{eq:logl-expfam-lcm}
   l_\delta(\theta) =
   \inner{y, \theta} - c(\theta) - \log \Pr\nolimits_\theta(Y \in H_\delta)
\end{equation}
and this is clearly an exponential family with
\begin{itemize}
\item canonical statistic vector $y$,
\item canonical parameter vector $\theta$, and
\item cumulant function given by
\begin{equation} \label{eq:cumfun-lcm}
   c_\delta(\theta) = c(\theta) + \log \Pr\nolimits_\theta(Y \in H_\delta)
\end{equation}
\end{itemize}
\end{itemize}
\citet{geyer-gdor} calls this family the \emph{limiting conditional model}
(LCM).  Of course, there are many LCM, one in each direction, but as we
shall presently see, there is usually only one LCM of interest
in any particular data analysis.

\begin{theorem} \label{th:cumfun-lcm}
Equation \eqref{eq:cumfun-lcm} gives the correct limit of the cumulant function
to make \eqref{eq:logl-expfam-lcm} equal to the limit of \eqref{eq:logl-expfam}
when limits are taken as in Theorem~\ref{th:completion-fundamental}
in the case $\inner{y, \theta} = \sigma_C(\delta)$.
\end{theorem}
\begin{proof}
In symbols, the assertion of the theorem is
$$
   \lim_{s \to \infty} l(\theta + s \delta)
   =
   l_\delta(\theta)
$$
And
\begin{align*}
   \lim_{s \to \infty} l(\theta + s \delta)
   & =
   \lim_{s \to \infty}
   \bigl[ \inner{y, \theta + s \delta} - c(\theta + s \delta) \bigr]
   \\
   & =
   \inner{y, \theta} + \lim_{s \to \infty}
   \bigl[ s \inner{y, \delta} - c(\theta + s \delta) \bigr]
   \\
   & =
   \inner{y, \theta} + \lim_{s \to \infty}
   \bigl[ s \sigma_C(\delta) - c(\theta + s \delta) \bigr]
   \\
   & =
   \inner{y, \theta} - c(\theta) - \log \Pr\nolimits_\theta(Y \in H_\delta)
   \\
   & =
   \inner{y, \theta} - c_\delta(\theta)
\end{align*}
where the fourth equality is Theorem~{2.2} in \citet{geyer-thesis}.
\end{proof}

We know, of course, that cumulant functions can be redefined by adding an
arbitrary constant (the $c(\psi)$ in \eqref{eq:cumfun-expfam}).
As mentioned in Section~\ref{sec:define-expfam} above, we could even redefine
the cumulant function by adding an arbitrary affine function if we were to
accept a different choice of canonical statistic.  But things would get
very confusing if we made different arbitrary choices for the original
exponential family and its limiting conditional models.  Hence, however
the cumulant function of the original exponential family was chosen, we
will always use \eqref{eq:cumfun-lcm} to define cumulant functions
for limiting conditional models.

\subsection{Aggregate Exponential Family}

Denote the LCM in the direction $\delta$ by $\mathcal{P}_\delta$.
When $\Pr_\theta(Y \in H_\delta) = 0$ we say $\mathcal{P}_\delta$ is empty
(there are no limit probability distributions, and we do not want to include
the zero measure in our completion, at least not yet).
Taking limits when $\delta = 0$ does nothing
(because \eqref{eq:support-function} says $\sigma_C(0) = 0$ for any $C$
and this gives $H_\delta = \real^J$
in \eqref{eq:complete-fundamental-hyperplane}).
So $\mathcal{P}_0$ is the exponential family we started with, which we
call the original model (OM) for short.

Then
$$
   \mathcal{P} = \bigcup_{\delta \in \real^J} \mathcal{P}_\delta
$$
is a union of exponential families that contains all straight-line limits.

Under certain regularity conditions used by \citet{barndorff-nielsen},
\citet{brown}, and \citet{geyer-gdor} this union is the completion.
We do not get anything more by taking further straight-line limits
in $\mathcal{P}_\delta$ for the various $\delta$.

But in general \citep[Chapters~2 and~4]{geyer-thesis} we may need to take
further straight-line limits or general (not straight line) limits
to arrive at the completion.

Anyway, one can see why \citet{brown} gave this idea the name aggregate
exponential family.  It is a union (or aggregate) of exponential families.

\subsection{Support and Directions of Recession and Constancy}

\begin{theorem} \label{th:recession-constancy}
A vector $\delta$ is a direction of recession of the log likelihood of
a full exponential family
with closed convex support $C$ and observed value of the canonical statistic
vector $y$ if and only if $\inner{y, \delta} \ge \sigma_C(\delta)$.
If $\delta$ and $- \delta$ are both directions of recession, then $\delta$
is a direction of constancy.
Conversely, if $\delta$ is a direction of constancy and $y \in C$,
then $\delta$ and $- \delta$ are both directions of recession.
\end{theorem}
The condition $y \in C$ is measure-theoretic nonsense.
We have to say $y \in C$ to be measure-theoretically correct, but
if your data fail to satisfy $y \in C$, then something is wrong with your data.
\begin{proof}
The first sentence is Corollary~{2.4.1} in \citet{geyer-thesis}.
If $\delta$ and $- \delta$ are both directions of recession then
$$
   \inner{y, \delta} \ge \sigma_C(\delta) = \sup_{x \in C} \inner{x, \delta}
$$
and
\begin{align*}
   - \inner{y, \delta}
   & =
   \inner{y, - \delta}
   \\
   & \ge
   \sigma_C(- \delta)
   \\
   & =
   \sup_{x \in C} \inner{x, - \delta}
   \\
   & =
   - \inf_{x \in C} \inner{x, \delta}
\end{align*}
or
$$
   \inner{y, \delta} \le \inf_{x \in C} \inner{x, \delta}
$$
so
\begin{equation} \label{eq:recession-constancy-inequalities}
   \inner{y, \delta} \le \inf_{x \in C} \inner{x, \delta}
   \le
   \sup_{x \in C} \inner{x, \delta} \le \inner{y, \delta} 
\end{equation}
hence
\begin{equation} \label{eq:recession-constancy}
   \inner{x, \delta} = \inner{y, \delta}, \qquad x \in C
\end{equation}
hence $\inner{Y, \delta} = \inner{y, \delta}$ almost surely,
and $\delta$ is a direction of constancy.

Conversely, if $\delta$ is a direction of constancy, then $\inner{Y, \delta}$
is constant almost surely.  And if $y \in C$, that constant must
be $\inner{y, \delta}$.  Hence \eqref{eq:recession-constancy} holds.
Hence \eqref{eq:recession-constancy-inequalities} holds.
And we have already seen that \eqref{eq:recession-constancy-inequalities}
is equivalent to both $\delta$ and $- \delta$ being directions of recession.
\end{proof}

\subsection{Curved Line Limits}

Chapter~4 of \citet{geyer-thesis} covers completely general
limits of sequences of distributions in an exponential family of distributions,
these limits being in the sense of convergence of probability mass-density
functions.  Although the section title says ``curved line limits,'' these
limits are just limits of sequences.  We only get a line by connecting
the dots, and that line does not have to be a smooth curve.

Theorems~4.1 through~{4.5} in \citet{geyer-thesis} show that taking general
limits gives no more limits that correspond to probability distributions
than taking iterated straight line limits.  General limits can produce
limits that are subprobability distributions
\citep[Examples~4.2 through~4.4]{geyer-thesis}, but these can never be
maximum likelihood estimates for a full family, because iterated straight
line limits produce the corresponding probability distribution, which must
have higher likelihood.
This shows that we do not need to consider curved line limits, so long
as we limit our attention to full families.
%%%%% Need to worry about subsampling, which are curved exponential family
%%%%% Do not need to worry about conditional aster models, which are also
%%%%%     curved exponential family, because of associated independence
%%%%%     models, which are full exponential family

\section{Unconditional Aster Models}

Unconditional aster models are regular full exponential families.
Thus the theory of the preceding section applies to them.

\begin{theorem} \label{th:dor-arrow}
Suppose $\{j\}$ is a univariate dependence group in an aster graph, and
the one-parameter exponential family of distributions for the arrow
$y_{p(j)} \longrightarrow y_j$ has closed convex support that is an
interval with endpoints $a_j$ and $b_j$ (either of which may be infinite and
which satisfy $a_j \le b_j$ with equality possible, in which case this
distribution is concentrated at one point).  Let $J$
be the set of non-initial nodes of the aster graph,
and let $Y$ denote the response vector and $y$ its observed value.

If $y_j = a_j y_{p(j)}$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-lower-bound}
   \eta_i = \begin{cases} -1, & i = j \\ a_j & i = p(j) \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

Taking the limit in the direction of recession \eqref{eq:dor-lower-bound}
gives the LCM that is the same as the OM except the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $a_j$.
This LCM is the OM conditioned on the event $Y_j = a_j Y_{p(j)}$.

If $y_j = b_j y_{p(j)}$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-upper-bound}
   \eta_i = \begin{cases} 1, & i = j \\ - b_j & i = p(j) \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

Taking the limit in the direction of recession \eqref{eq:dor-upper-bound}
gives the LCM that is the same as the OM except the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $b_j$.
This LCM is the OM conditioned on the event $Y_j = b_j Y_{p(j)}$.

In case $a_j = b_j$ the vectors \eqref{eq:dor-lower-bound}
and \eqref{eq:dor-upper-bound} are both directions of recession,
and are negatives of each other, hence are both directions of constancy.
But in this case we only need one direction of constancy since one
is a scalar multiple of the other.

In case $y_{p(j)} = 0$  and $-\infty < a_j < b_j < \infty$
the vectors \eqref{eq:dor-lower-bound}
and \eqref{eq:dor-upper-bound} are still both directions of recession,
but are not directions of constancy unless $Y_{p(j)} = 0$ almost surely.
\end{theorem}
\begin{proof}
We have a direction of recession if and only if $\inner{Y - y, \eta} \le 0$
almost surely.
From the definition
of $a_j$ and $b_j$ we know $a_j Y_{p(j)} \le Y_j \le b_j Y_{p(j)}$
almost surely.

In case $y_j = a_j y_{p(j)}$ and $\eta$ is
given by \eqref{eq:dor-lower-bound} we have two cases.
If $p(j)$ is noninitial, then
$$
   \inner{Y - y, \eta} = - (Y_j - y_j) + a_j (Y_{p(j)} - y_{p(j)})
   =
   - Y_j + a_j Y_{p(j)}
$$
and this is indeed less than or equal to zero almost surely by definition
of $a_j$.
If $p(j)$ is initial, then
$$
   \inner{Y - y, \eta} = - (Y_j - y_j)
   =
   - Y_j + a_j y_{p(j)}
   =
   - Y_j + a_j Y_{p(j)}
$$
the last equality being that $Y_{p(j)}$ is a constant random variable
so $Y_{p(j)} = y_{p(j)}$ almost surely,
and this is indeed less than or equal to zero almost surely by definition
of $a_j$.  Thus in either case we have \eqref{eq:dor-lower-bound} is
a direction of recession and
\begin{equation} \label{eq:lower-foo}
   \inner{Y - y, \eta} = - Y_j + a_j Y_{p(j)}
\end{equation}
Taking limits in the direction \eqref{eq:dor-lower-bound} arrives at
the limiting conditional model that conditions
on \eqref{eq:lower-foo} being equal to zero, that is, on the event
$Y_j = a_j Y_{p(j)}$.  By the predecessor-is-sample-size principle
this is the same thing as saying the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $a_j$.

The proofs of the assertions about \eqref{eq:dor-upper-bound} are similar.

That the conditions for \eqref{eq:dor-lower-bound}
and \eqref{eq:dor-upper-bound} both hold when $a_j = b_j$ and that
one is then the negative of the other is obvious.
That $\eta$ and $- \eta$ both being
directions of recession implies either is a direction of constancy is
\citet{geyer-gdor} Theorem~3 part (e) and Theorem~1 part (g).

In case $y_{p(j)} = 0$  and $-\infty < a_j < b_j < \infty$
we also have $y_j = 0$ so $y_j = a_j y_{p(j)} = b_j y_{p(j)}$ holds trivially.
Thus we have already proved that both are directions of recession.
In order for \eqref{eq:dor-lower-bound} to be a direction of constancy
we need $Y_j = a_j Y_{p(j)}$ to hold almost surely, but this is false
unless $Y_{p(j)} = 0$ almost surely.
Similarly for \eqref{eq:dor-upper-bound}.
\end{proof}

As we see in the proof, there are two cases.  When $p(j)$ is noninitial
the arrow $y_{p(j)} \longrightarrow y_j$ represents a conditional
distribution and $\eta$ has two nonzero components unless $a_j = 0$ and
$\eta$ is given by \eqref{eq:dor-lower-bound} or $b_j = 0$ and
$\eta$ is given by \eqref{eq:dor-upper-bound}.
When $p(j)$ is initial the arrow $y_{p(j)} \longrightarrow j$ represents,
in effect, a marginal distribution and $\eta$ has one nonzero component.
But the formulas \eqref{eq:dor-lower-bound} and \eqref{eq:dor-upper-bound}
work in either case because the middle case does not occur when
$p(j) \notin J$.

The case $a_j = b_j$ cannot occur in aster models allowed
by R package \code{aster}.  But once we start
taking limits, then they can.  So they are allowed by R package \code{aster2}.

Degenerate distributions concentrated at $a_j$ or $b_j$ are further
discussed in the appropriate section of Appendix~\ref{app:families}
(the details depend on the family the degenerate distribution is derived
from).
R package \code{aster2} implements them.

In the case considered last in the theorem where $y_{p(j)} = 0$ and
\eqref{eq:dor-lower-bound} and \eqref{eq:dor-upper-bound} are both
directions of recession and point in different directions, any nonnegative
combination (linear combination with nonnegative coefficients) of these
two vectors is another direction of recession (any nonnegative combimation
of directions of recession is another direction of recession,
\citealp{geyer-gdor}, Theorem~3).  For example, the vector $\eta$ whose
only nonzero component is $\eta_{p(j)} = a_j - b_j$ is a direction
of recession.

\begin{theorem} \label{th:dor-multinomial}
Suppose $G$ is a multinomial dependence group in an aster graph.
Let $J$ be the set of non-initial nodes of the aster graph,
and let $Y$ denote the response vector and $y$ its observed value.

If $j \in G$ and $y_j = 0$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-multinomial}
   \eta_i = \begin{cases} -1, & i = j \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

This vector is not a direction of constancy
of the saturated aster model unless $Y_{q(G)} = 0$ almost surely.

The vector $\eta$ having index set $J$ and coordinates
\begin{equation} \label{eq:doc-multinomial}
   \eta_i = \begin{cases} -1, & i \in G \\
   +1, & i = q(G) \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of constancy of the saturated aster model.

Taking the limit in the direction of recession \eqref{eq:dor-multinomial}
gives the LCM that is the same as the OM except the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at zero, and the conditional distribution for dependence group
$G$ becomes a partially degenerate multinomial distribution that has
$Y_j = 0$ almost surely.
\end{theorem}
\begin{proof}
For \eqref{eq:dor-multinomial} we need to show
that $\inner{Y - y, \eta} \le 0$ almost surely.
This is obvious.
\begin{equation} \label{eq:dor-multinomial-foo}
   \inner{Y - y, \eta} = - (Y_j - y_j)
   =
   - Y_j
\end{equation}
and this is indeed less than or equal to zero almost surely
by definition of the multinomial distribution.

In order for \eqref{eq:dor-multinomial} to be a direction of constancy
we need \eqref{eq:dor-multinomial-foo} to be zero almost surely.
But this is false unless $Y_{q(G)} = 0$ almost surely.

For $\eta$ given by \eqref{eq:doc-multinomial} to be a direction of
constancy we need to show
that $\inner{Y - y, \eta} = 0$ almost surely,
where $Y$ and $y$ are as above.
This too is obvious.
$$
   \inner{Y - y, \eta} = (Y_{q(G)} - y_{q(G)}) - \sum_{j \in G} (Y_j - y_j)
$$
and this is indeed equal to zero almost surely,
by definition of the multinomial distribution.

By Theorem~\ref{th:completion-fundamental}, taking limits in the direction
\eqref{eq:dor-multinomial} results in an LCM that conditions the OM on
the event $Y_j = 0$ almost surely, and this corresponds to
the arrow $y_{p(j)} \longrightarrow y_j$ having the degenerate family
concentrated at zero almost surely.  And this makes the multinomial
distribution of $Y_G$ given $Y_{q(G)}$ partially degenerate.
\end{proof}

As mentioned after the preceeding theorem, any nonnegative combination
of directions of recession is another direction of recession.  This includes
the direction of constancy (any direction of constancy is a direction
of recession, and so is the negative of any direction of constancy).
Hence a vector $\eta$ is a direction of recession described by this theorem
if and only if
\begin{gather*}
    \eta_j < \max_{i \in G} \eta_i \quad \text{implies} \quad y_j = 0
    \\
    \text{$q(G)$ noninitial} \quad \text{implies} \quad 
    \max_{i \in G} \eta_i = - \eta_{q(G)}
\end{gather*}

In case all but one of the components of a multinomial $Y_G$ are zero,
we can apply the theorem repeatedly to get a completely degenerate
multinomial family.  If $j \in G$ and $y_k = 0$ for $k \in G \setminus \{j\}$,
then repeated limits give us the degenerate multinomial family that
conditions on $Y_k = 0$ for $k \in G \setminus \{j\}$, but then we must
also have $Y_j = Y_{q(G)}$ almost surely by definition of the multinomial
distribution.

These partially degenerate multinomial distributions are further
discussed in Section~\ref{app:sec:multinomial} in the appendix.

In case $y_{q(G)} = 0$ but $Y_{q(G)} = 0$ does not hold almost surely,
the theorem says that every $\eta$ whose only nonzero component is
$\eta_j = -1$ is a direction of recession.  Hence any nonnegative
combination of these is a direction of recession.  Hence considering
also \eqref{eq:doc-multinomial} gives a vector $\eta$ having nonzero
components $\eta_G$ in case $q(G)$ is initial and $\eta_{G \cup \{q(G)\}}$
in case $q(G)$ is noninitial, is a direction of recession if and only if
$$
    \text{$q(G)$ noninitial} \quad \text{implies} \quad 
    \max_{i \in G} \eta_i = - \eta_{q(G)}
$$
and this direction of recession is a direction of constancy if and only if
$$
    \eta_i = \eta_j, \qquad i, j \in G
$$

\begin{theorem} \label{th:dor-normal}
Suppose $G = \{j, k\}$ is a normal-location-scale dependence group
in an aster graph.
Let $J$ be the set of non-initial nodes of the aster graph,
and let $Y$ denote the response vector and $y$ its observed value.

If $y_{q(G)} = 1$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-normal}
   \eta_i = \begin{cases} 2 y_j, & i = j \\
   -1, & i = k \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

But this direction of recession does not produce a limiting conditional model
because it corresponds to the case $\Pr_\theta(Y \in H_\delta) = 0$ in
in Theorem~\ref{th:completion-fundamental}.

If $y_{q(G)} \ge 2$, then almost surely there are no directions of recession
for this family.

If $y_{q(G)} = 0$, then every vector of the form \eqref{eq:dor-normal}
is a direction of recession (for all real numbers $y_j$).
These directions of recession are not directions of constancy unless
$Y_{q(G)} = 0$ almost surely.
\end{theorem}
\begin{proof}
The closed curve consisting of points $y_G$ such that $y_k = y_j^2$
supports the conditional distribution of $y_G$ given $y_{q(G)} = 1$.
The function $f : y_j \mapsto y_j^2$ is a convex function.
By the gradient inequality \citep[Theorem~2.13 (b)]{rockafellar-wets}
$$
   f(Y_j) - f(y_j) \ge f'(y_j) (Y_j - y_j)
$$
but this can also be written
$$
   Y_k - y_k \ge 2 y_j (Y_j - y_j)
$$
because $f(y_j) = y_k$ and $f(Y_j) = Y_k$ and $f'(y_j) = 2 y_j$.
And it can also be written
$$
   - \eta_k (Y_k - y_k) \ge \eta_j (Y_j - y_j)
$$
because of the definition of $\eta$ in the theorem statement.
And this says $\inner{Y - y, \eta} \le 0$ so $\eta$ is a direction of
recession.

Since $f$ is strictly convex
\citep[Theorem~2.13 ($\text{a}'$)]{rockafellar-wets},
we have strict inequality in the gradient inequality
\citep[Theorem~2.13 ($\text{b}'$)]{rockafellar-wets} when $Y_j \neq y_j$.
Hence we have \hbox{$\inner{Y - y, \eta} < 0$} almost surely.
But the latter is equivalent to $\Pr_\theta(Y \in H_\delta) = 0$ in
in Theorem~\ref{th:completion-fundamental}.

The (closed) convex support of the family for sample size one is
the set
$$
   C = \set{ y_G : y_k \ge y_j^2 }
$$
The (closed) convex support of the family for sample size $n$ is
is the $n$-fold Minkowski sum of this set, which is $n C$
\citep[Proposition~2.23]{rockafellar-wets}.  Because the distributions
in this family are continuous, the interior of $n C$ actually supports
the family for $n \ge 2$.  Hence (almost surely) it is not possible
to observe data on the boundary of the convex support for $n \ge 2$.

In case $y_{q(g)} = 0$, the convex support of the conditional distribution
of $Y_G$ given $Y_{q(G)}$ is $0 \cdot C = \{ 0 \}$.
Now we have to consider the three-dimensional set of all possible vectors
$Y_{\{j, k, l\}}$ with $l = q(G)$.  Since limits of sequences of normal vectors
are again normal vectors
\citep[Proposition~6.6 and Theorem~6.9]{rockafellar-wets},
normal vectors at the point $0 = (0, 0, 0)$ are those of the form
\eqref{eq:dor-normal} and any nonnegative combinations of such.

In order for such a vector $\eta$ to be a direction of constancy,
we must have $\eta_j Y_j + \eta_k Y_k = 0$ almost surely.
But this is false unless $Y_{q(G)} = 0$ almost surely.
\end{proof}

If $y_{q(G)} \ge 1$, then the theorem gives at most one direction of recession,
and it is not a direction of constancy.
If $y_{q(G)} = 0$, then the theorem gives an infinite number of directions
of recession pointing in different directions.
Our intention with these theorems is to use them to discover generic
directions of recession of (not saturated) unconditional aster models
using repeated linear programming.  But we cannot put an infinite number
of vectors into a linear program.

Moreover, our use of this theorem has to be fundamentally different from
our use of Theorems~\ref{th:dor-arrow} and~\ref{th:dor-multinomial}.
From the latter we discover DOR that lead to LCM in which we find MLE.
From the former we discover DOR that do not lead to LCM, and the MLE
does not exist, and we have to rudely inform users that their models
are no good: you cannot estimate the variance of a normal distribution
from one observation.

Users can avoid these kind of error messages by assuming homoscedastic
errors (just like in linear models).  The way this is done in aster models
is to have the variance node of each normal-location-scale family
have the same parameter.  And the way to do that is to have \code{+ foo}
in the formula, where \code{foo} is the indicator vector of the variance
nodes of all normal-location-scale dependence groups (the $k$ in the theorem)
and no other appearance of \code{foo} in the formula.

Users can have more complicated formulas in which variance differs among
normal-location-scale dependence groups, but then it is the job of the
computer to catch situations in which this leads to directions of recession
described by the theorem.

Fortunately, we can separate these two kinds of problems.
All normal dependence groups must be terminal (so not really
``fortunately'' because this follows from the predecessor-is-sample-size
principle). 
Thus we can trim all of the normal dependence groups off of the aster graph
and still have a possible aster model.
Then we can apply our algorithm (still to be developed in what follows)
to determine whether a GDOR exists and, if so, what the LCM is
(for the model with normal dependence groups trimmed off).

Then we can put the normal dependence groups back, and look for more
directions of recession.  In this, the following theorem helps.

\begin{theorem} \label{th:dor-predecessor-zero}
Suppose $G$ is a dependence group in an aster graph, and $y_{q(G)} = 0$
where $Y$ is the response vector and $y$ its observed value, then the
vector $\eta$ whose only nonzero component is $\eta_{q(G)} = -1$ is a
direction of recession of the saturated aster model.

Taking the limit in this direction gives the LCM that is the same as the
OM except $Y_{q(G)} = 0$ almost surely, and this means the distributions
of all successors, successors of successors, successors of successors of
successors, etc.\ are not identifiable in this LCM.

This direction of recession is not a direction of constancy
unless $Y_{q(G)} = 0$ almost surely.
\end{theorem}
\begin{proof}
We know $q(G)$ is not a terminal node.  It is not an initial node either,
because aster models are required to have nonzero data at initial nodes.
We know from
the predecessor-is-sample-size property that $Y_{q(G)} \ge 0$ is required.
Since $y_{q(G)}$ is at the lower endpoint of the support of $Y_{q(G)}$,
the vector described in the theorem statement is a direction of recession.

If we take limits in this direction, we get the OM conditioned on the event
$Y_{q(G)} = 0$ almost surely, and this implies $Y_j = 0$ almost surely for
all $j \prec q(G)$, where $\prec$ is the transitive closure of the successor
relation.  The cumulant function for this LCM does not depend on any
of the variables $\varphi_j$ for $j \preceq q(G)$.
This can be seen by applying \eqref{eq:cumfun-expfam} to this model.
\begin{align*}
   c(\varphi)
   & =
   c(\varphi^*) + \log\left\{ E_{\varphi^*} \left(
   e^{\inner{Y, \varphi - \varphi^*}} \right) \right\}
   \\
   & =
   c(\varphi^*) + \log\left\{ E_{\varphi^*} \left(
   \prod_{j \in J} e^{Y_j (\varphi_j - \varphi^*_j)}
   \right) \right\}
   \\
   & =
   c(\varphi^*) + \log\left\{ E_{\varphi^*} \left(
   \prod_{\substack{j \in J \\ j \not\preceq q(G)}}
   e^{Y_j (\varphi_j - \varphi^*_j)}
   \right) \right\}
\end{align*}
where $\varphi$ varies and $\varphi^*$ is fixed.
And then \eqref{eq:logl-aster-phi} shows that the log likelihood for the
LCM does not depend on any of these variables either.

This vector is a direction of constancy if and only if $Y_{q(G)} = 0$ almost
surely.
\end{proof}

The vector this theorem finds to be a direction of recession is also found
by theorems preceeding it, but the point of this theorem is that it
applies to any aster model whatsoever, even those having families that
have not been implemented yet.  And the theorem also provides more information
about LCM.

In particular, it tells us that for LCM we found by applying our
(yet to be developed) GDOR algorithm to the model with normal dependence
groups trimmed off, any normal dependence groups having $Y_{q(G)} = 0$
in the LCM can still be ignored: their parameters will be non-identifiable
in the LCM.

\begin{theorem} \label{th:dor-aster}
The set of all directions of recession is a closed convex cone.
The set of all directions of constancy is a vector subspace.
Every direction of constancy is also a direction of recession.
A vector $\delta$ is a direction of constancy if and only if
both $\delta$ and $- \delta$ are directions of recession.

The set of all directions of recession of saturated aster models
having families described by Theorems~\ref{th:dor-arrow},
\ref{th:dor-multinomial}, \ref{th:dor-normal},
and~\ref{th:dor-predecessor-zero}
is the smallest closed convex cone containing all of the directions
of recession described by those theorems.

The set of all directions of constancy of such aster models
is the smallest vector subspace containing all of the directions
of constancy described by those theorems.
\end{theorem}
\begin{proof}
The assertions of the first paragraph are all in Theorems 1 and 3 of
\citet{geyer-gdor} and the discussion surrounding them.

Sections~4.1 and~{4.2} in \citet{geyer-thesis} characterize all possible
limit distributions in an exponential family of distributions.
Theorem~{2.7} in \citet{geyer-thesis} says that all limit distributions
can be obtained by taking iterated straight line limits.
The limit of a product being the product of the limits, when we take a
limit we get a limit in each term of the fundamental factorization of
aster models \eqref{eq:factorize}.
Thus when we have all possible limiting conditional models for each of
the families for each of the dependence groups, we have also gotten
all of the limits for the whole aster model.

Conversely, since the theorems mentioned describe all possible limits
of distributions for dependence groups in the families described by
those theorems, which includes all families currently implemented in
R packages \code{aster} and \code{aster2}, we have discovered all
possible limits.  There are no other directions of recession.
\end{proof}

As the theorem statement only implicitly refers to but the the proof
explicitly says, the theorem does not apply to aster models having
multivariate dependence groups whose families have not been invented yet.
We would need to add theorems about their directions of recession if we
add them to aster models.

Now we need to figure out how to use these theorems when applied to
general unconditional aster models (canonical affine submodels of
the saturated aster model).  The principle is simple.  If $M$ is the
model matrix of a canonical affine submodel, then $\delta$ is a direction
of recession (resp.\ constancy) of that submodel if and only
if $\eta = M \delta$ is a direction of recession (resp.\ constancy)
of the saturated model.

So we revisit the theorems.

In Theorem~\ref{th:dor-arrow} we have either
\eqref{eq:dor-lower-bound} or \eqref{eq:dor-upper-bound} or both or neither
is a direction of recession.  If $a_j = b_j$, then we can take either to
be a direction of constancy and ignore the other.
If the predecessor is zero almost surely (this cannot happen unless the
model we are considering is already an LCM) then any directions of recession
are directions of constancy, but not otherwise.

In Theorem~\ref{th:dor-multinomial} when $y_{q(G)} > 0$ we have one
direction of recession for each $j \in G$ such that $y_j = 0$ and
we also have one direction of constancy for the whole dependence group.
If the predecessor is zero almost surely (this cannot happen unless the
model we are considering is already an LCM) then all directions of recession
are directions of constancy, but not otherwise.

For now we ignore Theorem~\ref{th:dor-normal}.

So that completes our list of directions of recession and constancy.

\begin{theorem} \label{th:dor-aster-explicit}
A vector $\delta$ is a direction of recession
of an unconditional canonical affine submodel
with normal dependence groups trimmed off and
no arrows having degenerate families
if and only if $\eta = M \delta$ has the form
\begin{equation} \label{eq:dor-aster-explicit}
   \eta = \sum_{j \in J_r} e_j \eta_j
\end{equation}
where $J_r$ is the index set for directions of recession
of the saturated model discussed above,
$J_c$ is the subset of $J_r$ indexing directions of constancy, and
the $e_j$ are real numbers satisfying $e_j \ge 0$
for $j \in J_r \setminus J_c$.
\end{theorem}
\begin{proof}
This just makes explicit what Theorem~\ref{th:dor-aster} already says.
\end{proof}

Consider the following linear program having variables $e_j$
for $j \in J_r$ and $\delta_k$ for $k \in K$.
\begin{alignat}{2}
  \text{maximize}   & \ \sum_{j \in J_r \setminus J_c} e_j
  \nonumber
  \\
  \text{subject to} & \ 0 \le e_j \le 1, & \qquad & j \in J_r \setminus J_c
  \label{prog:foo}
  \\
                    & \ M \delta =
  \sum_{j \in J_r} e_j \eta_j
  \nonumber
\end{alignat}
\begin{theorem} \label{th:lin-prog-one}
Linear program \eqref{prog:foo} always has a solution.
Linear program \eqref{prog:foo} has optimal value zero if and only if
there does
not exist a direction of recession that is not a direction of constancy
and the MLE exists in the originally given unconditional aster model.
Otherwise, the optimal value is greater than or equal to one and the
$\delta$ part of the solution is a direction of recession that is not
a direction of constancy.
\end{theorem}
\begin{proof}
The feasible region is nonempty because it always contains the zero vector.
Then solutions exist because the objective function is obviously bounded
on the feasible region.

The optimal value is zero if and only if at the solution $e_j = 0$ for
$j \in J_r \setminus J_c$, in which case the $\delta$ part of the solution
is a direction of constancy of the submodel and $\eta = M \delta$ is a
direction of constancy of the saturated model.  The assertion about
existence of MLE is then Theorem~{4} in \citet{geyer-gdor}.

If there exists any feasible point such that some $e_j$
for $j \in J_r \setminus J_c$ is nonzero, then we can multiply all components
of $\delta$ and all $e_j$ by a strictly positive constant to make
the largest $e_j$ for $j \in J_r \setminus J_c$ equal to one, in which case
the objective function is greater than or equal to one.
Optimizing then only increases the objective function.
The solution then is clearly a direction of recession that is not a
direction of constancy by Theorem~\ref{th:dor-aster-explicit}.
\end{proof}

We are not done yet because we haven't yet in the terminology of
\citet{geyer-gdor} found a \emph{generic} direction of recession (GDOR).
That will be one that has the maximal number of nonzero $e_j$ for
for $j \in J_r \setminus J_c$.  Since any nonnegative combination of
directions of recession is another direction of recession, we can
seek GDOR by modifying our linear program to find DOR with $e_j > 0$
that we haven't found so far.

Let $J^{*}$ be any nonempty subset of $J_r \setminus J_c$,
and consider the following linear program.
\begin{alignat}{2}
  \text{maximize}   & \ \sum_{j \in J^{*}} e_j
  \nonumber
  \\
  \text{subject to} & \ 0 \le e_j \le 1, & \qquad & j \in J^{*}
  \label{prog:foobar}
  \\
                    & \ 0 \le e_j, & \qquad &
                      j \in (J_r \setminus J_c) \setminus J^{*}
  \nonumber
  \\
                    & \ M \delta = \sum_{j \in J_r} e_j \eta_j
  \nonumber
\end{alignat}

Then we iterate
(Algorithm~\ref{alg:unconditional}\ifthenelse{\equal{\arabic{page}}{\pageref{alg:unconditional}}}{).}
{, page~\pageref{alg:unconditional}).}
\begin{algorithm}
\caption{Find GDOR for Unconditional Aster Model}
\label{alg:unconditional}
\begin{tabbing}
Set $J^{*} = J_r \setminus J_c$\\
Set $J^{{*}{*}} = \emptyset$\\
Set $\gamma = 0$\\
\textbf{repeat} \{\\
\qquad \= Solve the linear program \eqref{prog:foobar}\\
\> \textbf{if} (linear program has no solution) \textbf{error}\\
\> \textbf{if} (optimal value is zero) \textbf{break}\\
\> Set $\delta$ to be the $\delta$ part of the solution of the linear program\\
\> Set $\gamma = \gamma + \delta$\\
\> Set $e$ to be the $e$ part of the solution of the linear program\\
\> Set $J^{**} = J^{**} \cup \set{j \in J^{*} : e_j > 0}$\\
\> Set $J^{*} = J^{*} \setminus J^{{*}{*}}$\\
\> \textbf{if} ($J^{*} = \emptyset$) \textbf{break}\\
\}
\end{tabbing}
\end{algorithm}
\begin{theorem} \label{th:lin-prog-two}
Algorithm~\ref{alg:unconditional} always terminates, and $\gamma$ is
a generic direction of recession unless $\gamma = 0$, in which case
the MLE exists in the originally given unconditional aster model.
\end{theorem}
\begin{proof}
The algorithm must terminate because $J^{*}$ decreases in each iteration.
So we terminate when $J^{*} = \emptyset$ if not before.

Since each $\delta$ found is a direction of recession that is not
a direction of constancy, so is $\gamma$.

The termination condition of optimal value zero or $J^{{*}{*}} = \emptyset$,
proves that $\gamma$ is such that $\eta = M \gamma$ has the most possible
nonzero components $\eta_j$ for $j \in J_r \setminus J_c$.  Hence it
is generic unless $\gamma = 0$ and the algorithm proves the MLE exists
in the OM.
\end{proof}

From now on we only use the LCM corresponding to the GDOR found.
This means every $\eta_j$ for $j \in J^{{*}{*}}$ found by the algorithm
is a direction of constancy of this LCM.  The other DOR remain the same.

Now we add back in all of the normal dependence groups.
Normal-location arrows are covered by Theorem~\ref{th:dor-arrow}
with $a_j = - \infty$ and $b_j = + \infty$.  They can never have
directions of recession.  So that leaves normal-location-scale dependence
groups.

From Theorem~\ref{th:dor-normal} we know that those with $y_{q(G)} \ge 2$
have no directions of recession to add to our problem.
From Theorem~\ref{th:dor-predecessor-zero} we know that
those with $y_{q(G)} = 0$ have non-identifiable parameters in the LCM.
If $G = \{ j, k \}$ then we add a vector whose only nonzero component
is $\eta_j = 1$ to the list of directions of constancy and also a vector
whose only nonzero component
is $\eta_k = 1$ to the list of directions of constancy.
Finally, from Theorem~\ref{th:dor-normal} we know that those
with $y_{q(G)} = 1$ have exactly one direction of recession that is not
a direction of constancy given by \eqref{eq:dor-normal}.
There are no further directions of recession to add to our problem.

So we throw all of this back into linear program \eqref{prog:foo}.
It had better have optimal value zero.
Otherwise users get rude error messages.

\section{Conditional Aster Models}

Saturated aster models are regular full exponential families.
Unconditional canonical affine submodels of aster models are
regular full exponential families.
So the theory in this chapter up to now applies to them.

Conditional canonical affine submodels of aster models are not
regular full exponential families.
As smooth submodels of saturated aster models, they are what the
jargon calls curved exponential families.
But that does not tell us much about existence or non-existence of MLE.
We know that all possible limits have to be limits of distributions
in the saturated model (because it is a submodel of the saturated model).
But when those limits are MLE is something for which there is no general
theory for curved exponential families.

\subsection{Associated Independence Models}

A cheap trick, however, does crack the problem of conditional aster models.
This is the notion of associated independence models
(Section~\ref{sec:conditional-aster-model-mle} above).
For reference, we repeat \eqref{eq:logl-aster-theta-tricky} above
\begin{equation} \label{eq:logl-aster-theta-tricky-duplicate}
   l(\theta)
   =
   \sum_{G \in \mathcal{G}}
   \left[ \inner{y_G, \theta_G} - n_{q(G)} c_G(\theta_G) \right]
\end{equation}
(so this equation now has two equation numbers, one here and one there).

The actual conditional model has (saturated model) log likelihood
that is \eqref{eq:logl-aster-theta-tricky-duplicate}
with $n_{q(G)}$ replaced by $y_{q(G)}$.
The associated independence model (AIM) has (saturated model) log likelihood
that is \eqref{eq:logl-aster-theta-tricky-duplicate}
with $n_{q(G)}$ constant and $y_j$ random.

As Section~\ref{sec:conditional-aster-model-mle} above says,
the AIM makes no sense when considered statistically, probabilistically,
because it pretends that variables that are actually the same
($n_{q(G)}$ and $y_{q(G)}$) are different, and one is constant
and the other random.  But,
as Section~\ref{sec:conditional-aster-model-mle} above also says,
the AIM makes perfect sense when considered numerically, algebraically
when we are considering maximum likelihood estimation.
Then $n_{q(G)}$ and $y_{q(G)}$ are just numbers, fixed at their observed
values, and if we use different notation for the same number in different
parts of the expression, that is OK.

In Section~\ref{sec:conditional-aster-model-mle}
we used the AIM to reach the conclusion that the log likelihood of
a conditional aster model is concave, something that is not generally
true of curved exponential family models.

In this section,
we will use the AIM to completely characterize existence and uniqueness
of MLE for conditional aster models and directions of recession and constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate} and for its canonical
affine submodels (conditional aster models).

What puts the I (for independence) in AIM is that the AIM makes
the $Y_G$ for $G \in \mathcal{G}$ independent random vectors.
This makes the AIM much easier to reason about than the actual conditional
aster model.

So now we repeat the preceding section, \emph{mutatis mutandis} reasoning
about AIM rather than unconditional aster models.

\begin{theorem} \label{th:dor-aim-arrow}
Suppose $\{j\}$ is a univariate dependence group in an AIM, and
the one-parameter exponential family of distributions for the arrow
$n_{p(j)} \longrightarrow y_j$ has closed convex support that is an
interval with endpoints $a_j$ and $b_j$ (either of which may be infinite and
which satisfy $a_j \le b_j$ with equality possible, in which case this
distribution is concentrated at one point).  Let $J$
be the set of non-initial nodes of the aster graph,
let $Y$ denote the response vector and $y$ its observed value,
and let $n$ denote the vector of sample sizes whose components are $n_j$.

If $n_{p(j)} = 0$ or $a_j = b_j$,
then the vector $\eta$ whose only nonzero component
is $\eta_j = 1$ is a direction of constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate}.

If $n_{p(j)} > 0$ and $a_j < b_j$ and $y_j = a_j n_{p(j)}$,
then the vector $\eta$ whose only nonzero component
is $\eta_j = -1$ is a direction of recession
that is not a direction of constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate}.

Taking the limit in this direction of recession
gives the LCM that is the same as the OM except the arrow
$n_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $a_j$.
This LCM is the OM (of the AIM) conditioned on the event $Y_j = a_j n_{p(j)}$.

If $n_{p(j)} > 0$ and $a_j < b_j$ and $y_j = b_j n_{p(j)}$,
then the vector $\eta$ whose only nonzero component
is $\eta_j = 1$ is a direction of recession
that is not a direction of constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate}.

Taking the limit in this direction of recession
gives the LCM that is the same as the OM except the arrow
$n_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $b_j$.
This LCM is the OM (of the AIM) conditioned on the event $Y_j = b_j n_{p(j)}$.
\end{theorem}

\begin{theorem} \label{th:dor-aim-multinomial}
Suppose $G$ is a multinomial dependence group in an AIM.
Let $J$ be the set of non-initial nodes of the aster graph,
let $Y$ denote the response vector and $y$ its observed value,
and let $n$ denote the vector of sample sizes whose components are $n_j$.

If $n_{p(j)} = 0$ then the vector $\eta$ whose only nonzero component
is $\eta_j = 1$ is a direction of constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate},
and this is true for each $j \in G$.

If $n_{p(j)} > 0$ and $y_j = 0$,
then the vector $\eta$ whose only nonzero component
is $\eta_j = -1$ is a direction of recession
that is not a direction of constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate},
and this is true for each $j \in G$.

Taking the limit in any of these directions of recession,
say the one having $\eta_j$ nonzero,
gives the LCM that is the same as the OM (of the AIM) except the arrow
$n_{p(j)} \longrightarrow y_j$ has
the degenerate family of distributions
concentrated at zero.

The vector $\eta$ having index set $J$ and coordinates
\begin{equation} \label{eq:doc-aim-multinomial}
   \eta_i = \begin{cases} 1, & i \in G \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of constancy of
of \eqref{eq:logl-aster-theta-tricky-duplicate}.
\end{theorem}

As in the discussion following Theorem~\ref{th:dor-multinomial}
(which this theorem duplicates \emph{mutatis mutandis}),
we note that any nonnegative combination of directions of recession is
another direction of recession.  Hence the directions of recession
described by this theorem are vectors $\eta$ whose only nonzero components
are in the subvector $\eta_G$ and such a vector is a direction of recession
of \eqref{eq:logl-aster-theta-tricky-duplicate} if
$$
   \eta_j < \max_{i \in G} \eta_i \quad \text{implies} \quad y_j = 0
$$
and such a vector is a direction of constancy
of \eqref{eq:logl-aster-theta-tricky-duplicate} if $n_{q(G)} = 0$ or if
its nonzero components are all the same.

\begin{theorem} \label{th:dor-aim-normal}
Suppose $G = \{j, k\}$ is a normal-location-scale dependence group
in an AIM.
Let $J$ be the set of non-initial nodes of the aster graph,
let $Y$ denote the response vector and $y$ its observed value,
and let $n$ denote the vector of sample sizes whose components are $n_j$.

If $n_{q(G)} = 0$, then any vector $\eta$ whose only nonzero components
are $\eta_j$ and $\eta_k$ is a direction of constancy of
of \eqref{eq:logl-aster-theta-tricky-duplicate}.

If $n_{q(G)} = 1$, then \eqref{eq:dor-normal} is a direction of recession
of \eqref{eq:logl-aster-theta-tricky-duplicate}
that is not a direction of constancy.

But this direction of recession does not produce a limiting conditional model
because it corresponds to the case $\Pr_\theta(Y \in H_\delta) = 0$ in
in Theorem~\ref{th:completion-fundamental}.

If $n_{q(G)} \ge 2$, then, almost surely,
there are no directions of recession for this family.
\end{theorem}

\begin{theorem} \label{th:dor-aim}
In addition to the general properties of directions of recession
and constancy found in Theorem~\ref{th:dor-aster},
the set of all directions of recession of AIM
having families described by Theorems~\ref{th:dor-aim-arrow},
\ref{th:dor-aim-multinomial}, and~\ref{th:dor-aim-normal}
is the smallest closed convex cone containing all of the directions
of recession described by those theorems.

The set of all directions of constancy of such aster models
is the smallest vector subspace containing all of the directions
of constancy described by those theorems.
\end{theorem}

\begin{theorem} \label{th:dor-aim-explicit}
For an AIM with normal dependence groups trimmed off define
\begin{align*}
   J_\text{\normalfont up}
   & =
   \set{ j \in J : y_j = a_j n_{p(j)} }
   \\
   J_\text{\normalfont dn}
   & =
   \set{ j \in J : y_j = b_j n_{p(j)} }
\end{align*}
where these include multinomial dependence groups with the convention
$a_j = 0$ and $b_j = \infty$ for them.
Then a vector $\eta$ is a direction of recession of this model if
\begin{alignat*}{2}
   \eta_j & \le 0, & \qquad &
   j \in J_\text{\normalfont dn} \setminus J_\text{\normalfont up}
   \\
   \eta_j & \ge 0, & \qquad &
   j \in J_\text{\normalfont up} \setminus J_\text{\normalfont dn}
\end{alignat*}
Conversely, any direction of recession of this model satisfies these
conditions if we modify it by subtracting $\max(\eta_G)$ from the elements
of $\eta_G$ for each multinomial dependence group $G$.
\end{theorem}
