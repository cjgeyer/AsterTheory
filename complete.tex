
\chapter{Completion}

In this chapter we deal with what to do when maximum likelihood estimates
do not exist in the exponential family or aster model we are initially given.
There may, and usually do, exist maximum likelihood estimates in the
\emph{completion} of the family.  It is a bit unclear what we should call
the statistical models studied in this chapter.
\begin{itemize}
\item \citet[Sections~9.3 and~9.4]{barndorff-nielsen} calls this concept
    \emph{completion}.
\item \citet[Chapter~6]{brown} calls this concept
    an \emph{aggregate exponential family} for reasons that will be explained
    presently.
\item \citet[Chapters~2 and~4]{geyer-thesis} calls this concept
    \emph{closure}.
\item \citet{geyer-gdor} calls this concept
    \emph{Barndorff-Nielsen completion}.
\end{itemize}
\citeauthor[personal communication]{brown} pointed out that the eponym chosen
in \citet{geyer-gdor} was not quite correct, since \citet{barndorff-nielsen}
works under more restrictive regularity conditions than \citet{brown}, and
\citet{brown} works under more restrictive regularity conditions than
\citet{geyer-thesis}.  The choice in \citet{geyer-gdor} follows
Stigler's law of eponomy.  At least in this case \citeauthor{barndorff-nielsen}
had the concept first if not in the most generality.
The reason why \citet{geyer-thesis} chose ``closure'' rather than ``completion''
is that when one works under the weakest regularity conditions, the topological
space that is the statistical model being ``completed'' is not metrizable,
hence ``complete'' (every Cauchy sequence converges) doesn't make any sense
(the definition of Cauchy sequence requires a metric).  Thus we have only
the more general topological concept of closure.
We won't fuss about any of this and will continue use Barndorff-Nielsen
completion or just completion.

\section{Binomial Example}

For this simplest example of the phenomenon of interest, we consider the
binomial distribution.  We know from the discussion
in Section~\ref{sec:direction-of-recession} above that the MLE does not exist
when the observed value of the canonical statistic, which for the binomial
distribution is the number of successes, is an extreme value, either as small
as it can be or as large as it can be, in this case either 0 or $n$, where
$n$ is the sample size.

Usually, we think the MLE for the usual parameter $p$, the success probability,
does exist for all data and is $\hat{p} = x / n$.  But when $x = 0$ or $x = n$,
so $\hat{p}$ is zero or one, the MLE for the canonical parameter
$\theta = \logit(p)$
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
does not exist because the domain of the logit function is the open interval
$(0, 1)$ and does not include the endpoints.  Since
\begin{align*}
   \lim_{p \downarrow 0} \logit(p) & = - \infty
   \\
   \lim_{p \uparrow 0} \logit(p) & = \infty
\end{align*}
we could try to identify these endpoints with infinite values of the canonical
parameter, but that is not the way exponential family theory works,
and, as we shall see, it does not generalize to multiparameter problems.

So instead of trying to complete the parameter space, we try to complete the
family of distributions.  These distributions have PMF
$$
   f_p(x) = \binom{n}{x} p^x (1 - p)^{n - x}
$$
and we have
\begin{align*}
   \lim_{p \downarrow 0} f_p(x) & = \begin{cases} 1, & x = 0 \\ 0, & x > 0
   \end{cases}
   \\
   \lim_{p \uparrow 1} f_p(x) & = \begin{cases} 0, & x < n \\ 1, & x = n
   \end{cases}
\end{align*}
so the completion contains the original exponential family we were given
plus two new distributions, the degenerate distribution concentrated at zero
and the degenerate distribution concentrated at $n$.  And these new
distributions are what are usually thought of as the binomial distributions
for $p = 0$ and $p = 1$ (when $p = 0$ no successes are possible so $x = 0$
almost surely; when $p = 1$ no failures are possible so $x = n$ almost surely).

\section{General Exponential Families}

\subsection{Support and Support Function}

Let $C$ denote the \emph{closed convex support} of the exponential family
under discussion.  This is the smallest closed convex set that contains
the canonical statistic vector with probability one.  Hence it is a closed
convex subset of the vector space where the canonical statistic takes values.

The closed convex set always exists because the intersection of closed sets
is closed and the intersection of convex sets is convex and because
finite-dimensional are second countable, so the complement of $C$ is the
union of a countable family of open sets having probability zero under
all distributions in the family (because all have the same support).

Let $\sigma_C$ denote the \emph{support function} of $C$, defined by
$$
   \sigma_C(\delta) = \sup_{y \in C} \inner{y, \delta}
$$
\citep[Section~8.E]{rockafellar-wets}.  The term ``support'' here is
unfortunate in that it is unrelated to the term ``support'' in $C$ being
a support of the canonical statistic vector of the exponential family.
But both terms are well established (``closed convex support'' in exponential
family theory and ``support function'' in convex analysis).

\subsection{Probability Mass-Density Functions}

If \eqref{eq:logl-expfam} is the log likelihood of an exponential family,
the the PMDF of that family must be the exponential of the log likelihood.
In order that we do not get extra terms that do not appear in the log
likelihood and in order to get the right support of the family, we take
the measure with respect to which we calculate densities to be a measure
in the family, say the measure corresponding to canonical parameter vector
$\psi$.  Then the PMDF are
\begin{equation} \label{eq:pmdf-expfam}
   f_\theta(\omega) = e^{\inner{Y(\omega), \theta - \psi} - c(\theta) + c(\psi)}
\end{equation}
where $\omega$ is the complete data (remember that $Y$ is a statistic,
not necessarily the complete data) \citep[Equation~(4)]{geyer-gdor}.

\subsection{Straight Line Limits}

\begin{theorem} \label{th:completion-fundamental}
For a full exponential family having log likelihood \eqref{eq:logl-expfam},
densities \eqref{eq:pmdf-expfam}, canonical statistic vector $Y$,
full canonical parameter space $\Theta$, and closed convex support $C$,
suppose $\delta$ is a direction in the vector space where the canonical
parameter takes values,
\begin{equation} \label{eq:complete-fundamental-hyperplane}
   H_\delta = \set{ y \in \real^J : \inner{y, \delta} = \sigma_C(\delta) },
\end{equation}
then for all $\theta \in \Theta$
\begin{equation} \label{eq:complete-fundamental-limit}
   \lim_{\theta + s \delta} f_{\theta + s \delta}(\omega)
   =
   \begin{cases}
   0, & \inner{Y(\omega), \delta} < \sigma_C(\delta)
   \\
   f_\theta(\omega) / \Pr_\theta(Y \in H_\delta),
   & \inner{Y(\omega), \delta} = \sigma_C(\delta)
   \\
   \infty, & \inner{Y(\omega), \delta} > \sigma_C(\delta)
   \end{cases}
\end{equation}
where the middle term is defined to be $\infty$ in case of divide by zero.
If $\delta$ is not a direction of constancy
and $\Pr_\theta(Y \in H_\delta) > 0$, then the function
$s \mapsto \Pr_{\theta + s \delta}(Y \in H_\delta)$ is continuous,
strictly increasing, converges to one
as $s \to \infty$.
\end{theorem}
In the two cases ruled out by the precondition of the last sentence
the function $s \mapsto \Pr_{\theta + s \delta}(Y \in H_\delta)$ is
a constant function.  If $\delta$ is a direction of constancy,
then $\Pr_\theta(Y \in H_\delta) = 1$ for all $\theta$.
If $\Pr_\theta(Y \in H_\delta) = 0$ for some $\theta$,
then $\Pr_\theta(Y \in H_\delta) = 0$ for all $\theta$.
\begin{proof}
This is a complication of Theorem~{6} in \citet{geyer-gdor}
that is essentially Theorem~{2.3} in \citet{geyer-thesis}.
However the proof of that Theorem~{2.3} contains some errors,
so a corrected proof is given in the appendix of \citet{geyer-gdor}.
Then the case $\Pr_\theta(Y \in H_\delta) > 0$ is Theorem~{2.6}
in \citet{geyer-thesis},
and the case $\Pr_\theta(Y \in H_\delta) = 0$ follows from Theorem~{2.2}
in \citet{geyer-thesis}.
The last sentence of the theorem statement is in Theorem~{6}
in \citet{geyer-gdor}.
\end{proof}
In case $\Pr_\theta(Y \in H_\delta) > 0$,
we note three things about the limit in \eqref{eq:complete-fundamental-limit}.
\begin{itemize}
\item It is a probability distribution because the set where it is infinite
    has measure zero under the dominating measure $\Pr_\psi$
\item It is a conditional distribution of the original family,
    the conditional distribution of $Y$ given the event $Y \in H_\delta$
    for the parameter vector $\theta$.
\item It is a limit distribution of the original family,
    the limit of the distributions for parameter vectors $\theta + s \delta$
    as $s \to \infty$.  By Scheff\'{e}'s lemma, convergence of PMDF implies
    convergence in total variation of the corresponding probability measures.
\end{itemize}
Note that the distribution under discussion is both a limit distribution
and a conditional distribution.  Thinking of it as just one or the other
is missing something.  It is both.

In case $\Pr_\theta(Y \in H_\delta) = 0$,
we note one thing about the limit in \eqref{eq:complete-fundamental-limit}.
\begin{itemize}
\item It is the zero measure because it is zero on the support of
the dominating measure $\Pr_\psi$.
\end{itemize}

\subsection{The Limiting Conditional Model}

We note one thing about the set of all limits
in \eqref{eq:complete-fundamental-limit}
in the case when $\Pr_\theta(Y \in H_\delta) > 0$.
\begin{itemize}
\item They form an exponential family of distributions.  The log likelihood is
$$
   \inner{y, \theta} - c(\theta) - \Pr\nolimits_\theta(Y \in H_\delta)
$$
and this is clearly an exponential family with
\begin{itemize}
\item canonical statistic vector $y$,
\item canonical parameter vector $\theta$, and
\item cumulant function given by
\begin{equation} \label{eq:cumfun-lcm}
   c_\delta(\theta) = c(\theta) + \Pr\nolimits_\theta(Y \in H_\delta)
\end{equation}
\end{itemize}
\end{itemize}
\citet{geyer-gdor} calls this family the \emph{limiting conditional model}
(LCM).  Of course, there are many LCM, one in each direction, but as we
shall presently see, there is only one LCM of interest in any particular
data analysis.

\subsection{Aggregate Exponential Family}

Denote the LCM in the direction $\delta$ by $\mathcal{P}_\delta$.
When $\Pr_\theta(Y \in H_\delta) = 0$ we say $\mathcal{P}_\delta$ is empty
(there are no limit probability distributions, and we do not want to include
the zero measure in our completion, at least not yet).

Then
$$
   \mathcal{P} = \bigcup_{\delta \in \real^J} \mathcal{P}_\delta
$$
is a union of exponential families that contains all straight-line limits.

Under certain regularity conditions used by \citet{barndorff-nielsen},
\citet{brown}, and \citet{geyer-gdor} this union is the completion.
We do not get anything more by taking further limits in $\mathcal{P}$.

But in general \citet[Chapters~2 and~4]{geyer-thesis} we may need to take
further limits to arrive at the completion.

Anyway, one can see why \citet{brown} gave this idea the name aggregate
exponential family.  It is a union (or aggregate) of exponential families.

\subsection{Support and Directions of Recession and Constancy}

\begin{theorem}
A vector $\delta$ is a direction of recession of the log likelihood of
a full exponential family
with closed convex support $C$ and observed value of the canonical statistic
vector $y$ if and only if $\inner{y, \delta} \ge \sigma_C(\delta)$.
If $y \in C$ (which happens with probability one), then a vector $\delta$
is a direction of constancy of the family if and only if
$\delta$ and $- \delta$ are both directions of recession.
\end{theorem}
The condition $y \in C$ is measure-theoretic nonsense.
We have to say $y \in C$ to be measure-theoretically correct, but
if your data fail to satisfy $y \in C$, then something is wrong with your data.
\begin{proof}
The first sentence is Corollary~{2.4.1} in \citet{geyer-thesis}.
For the rest, $\delta$ is a direction of constancy if and only if
$\inner{Y, \delta} = a$ almost surely for some constant $a$.
And under the assumption $y \in C$, this implies $\inner{y, \delta} = a$
too, so $\inner{Y - y, \delta} = 0 = \inner{Y - y, - \delta}$ almost surely
and both $\delta$ and $- \delta$ are directions of recession.
Conversely, both $\delta$ and $-\delta$ being directions of recession implies
too, so $\inner{Y - y, \delta} \le 0$ almost surely and
$\inner{Y - y, - \delta} \le 0$ almost surely, which imply
$\inner{Y - y, \delta} = 0$ almost surely so $\inner{Y, \delta} = a$ almost
surely, and $\delta$ is a direction of constancy.
\end{proof}

