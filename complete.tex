
\chapter{Completion} \label{chap:completion}

In this chapter we deal with what to do when maximum likelihood estimates
do not exist in the exponential family or aster model we are initially given.
There may, and usually do, exist maximum likelihood estimates in the
\emph{completion} of the family.  It is a bit unclear what we should call
the statistical models studied in this chapter.
\begin{itemize}
\item \citet[Sections~9.3 and~9.4]{barndorff-nielsen} calls this concept
    \emph{completion}.
\item \citet[Chapter~6]{brown} calls this concept
    an \emph{aggregate exponential family} for reasons that will be explained
    presently.
\item \citet[Chapters~2 and~4]{geyer-thesis} calls this concept
    \emph{closure}.
\item \citet{geyer-gdor} calls this concept
    \emph{Barndorff-Nielsen completion}.
\end{itemize}
\citeauthor[personal communication]{brown} pointed out that the eponym chosen
in \citet{geyer-gdor} was not quite correct, since \citet{barndorff-nielsen}
works under more restrictive regularity conditions than \citet{brown}, and
\citet{brown} works under more restrictive regularity conditions than
\citet{geyer-thesis}.  The choice in \citet{geyer-gdor} follows
Stigler's law of eponomy.  At least in this case \citeauthor{barndorff-nielsen}
had the concept first if not in the most generality.
The reason why \citet{geyer-thesis} chose ``closure'' rather than ``completion''
is that when one works under the weakest regularity conditions, the topological
space that is the statistical model being ``completed'' is not metrizable,
hence ``complete'' (every Cauchy sequence converges) doesn't make any sense
(the definition of Cauchy sequence requires a metric).  Thus we have only
the more general topological concept of closure.
We won't fuss about any of this and will continue use Barndorff-Nielsen
completion or just completion.

\section{Binomial Example}

For this simplest example of the phenomenon of interest, we consider the
binomial distribution.  We know from the discussion
in Section~\ref{sec:direction-of-recession} above that the MLE does not exist
when the observed value of the canonical statistic, which for the binomial
distribution is the number of successes, is an extreme value, either as small
as it can be or as large as it can be, in this case either 0 or $n$, where
$n$ is the sample size.

Usually, we think the MLE for the usual parameter $p$, the success probability,
does exist for all data and is $\hat{p} = x / n$.  But when $x = 0$ or $x = n$,
so $\hat{p}$ is zero or one, the MLE for the canonical parameter
$\theta = \logit(p)$
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
does not exist because the domain of the logit function is the open interval
$(0, 1)$ and does not include the endpoints.  Since
\begin{align*}
   \lim_{p \downarrow 0} \logit(p) & = - \infty
   \\
   \lim_{p \uparrow 0} \logit(p) & = \infty
\end{align*}
we could try to identify these endpoints with infinite values of the canonical
parameter, but that is not the way exponential family theory works,
and, as we shall see, it does not generalize to multiparameter problems.

So instead of trying to complete the parameter space, we try to complete the
family of distributions.  These distributions have PMF
$$
   f_p(x) = \binom{n}{x} p^x (1 - p)^{n - x}
$$
and we have
\begin{align*}
   \lim_{p \downarrow 0} f_p(x) & = \begin{cases} 1, & x = 0 \\ 0, & x > 0
   \end{cases}
   \\
   \lim_{p \uparrow 1} f_p(x) & = \begin{cases} 0, & x < n \\ 1, & x = n
   \end{cases}
\end{align*}
so the completion contains the original exponential family we were given
plus two new distributions, the degenerate distribution concentrated at zero
and the degenerate distribution concentrated at $n$.  And these new
distributions are what are usually thought of as the binomial distributions
for $p = 0$ and $p = 1$ (when $p = 0$ no successes are possible so $x = 0$
almost surely; when $p = 1$ no failures are possible so $x = n$ almost surely).

\section{General Exponential Families}

\subsection{Support and Support Function}

Let $C$ denote the \emph{closed convex support} of the exponential family
under discussion.  This is the smallest closed convex set that contains
the canonical statistic vector with probability one.  Hence it is a closed
convex subset of the vector space where the canonical statistic takes values.

The closed convex support always exists because the intersection of closed sets
is closed and the intersection of convex sets is convex and because
finite-dimensional vector spaces are second countable.  Define $C$ to be
the intersection of all closed convex sets that contain the canonical
statistic vector $Y$ with probability one under some distribution in the
family, and hence for all distributions in the family (because all have
the same support).  Then event $Y \notin C$ is the
union of a countable family of open sets having probability zero,
hence $Y \in C$ almost surely.

Let $\sigma_C$ denote the \emph{support function} of $C$, defined by
\begin{equation} \label{eq:support-function}
   \sigma_C(\delta) = \sup_{y \in C} \inner{y, \delta}
\end{equation}
\citep[Section~8.E]{rockafellar-wets}.  The term ``support'' here is
unfortunate in that it is unrelated to the term ``support'' in $C$ being
a support of the canonical statistic vector of the exponential family.
But both terms are well established (``closed convex support'' in exponential
family theory and ``support function'' in convex analysis).

\subsection{Probability Mass-Density Functions}

If \eqref{eq:logl-expfam} is the log likelihood of an exponential family,
the the PMDF of that family must be the exponential of the log likelihood.
In order that we do not get extra terms that do not appear in the log
likelihood and in order to get the right support of the family, we take
the measure with respect to which we calculate densities to be a measure
in the family, say the measure corresponding to canonical parameter vector
$\psi$.  Then the PMDF are
\begin{equation} \label{eq:pmdf-expfam}
   f_\theta(\omega) = e^{\inner{Y(\omega), \theta - \psi} - c(\theta) + c(\psi)}
\end{equation}
where $\omega$ is the complete data (remember that $Y$ is a statistic,
not necessarily the complete data) \citep[Equation~(4)]{geyer-gdor}.

\subsection{Straight Line Limits}

\begin{theorem} \label{th:completion-fundamental}
For a full exponential family having log likelihood \eqref{eq:logl-expfam},
densities \eqref{eq:pmdf-expfam}, canonical statistic vector $Y$,
full canonical parameter space $\Theta$, and closed convex support $C$,
suppose $\delta$ is a direction in the vector space where the canonical
parameter takes values,
\begin{equation} \label{eq:complete-fundamental-hyperplane}
   H_\delta = \set{ y \in \real^J : \inner{y, \delta} = \sigma_C(\delta) },
\end{equation}
then for all $\theta \in \Theta$
\begin{equation} \label{eq:complete-fundamental-limit}
   \lim_{\theta + s \delta} f_{\theta + s \delta}(\omega)
   =
   \begin{cases}
   0, & \inner{Y(\omega), \delta} < \sigma_C(\delta)
   \\
   f_\theta(\omega) / \Pr_\theta(Y \in H_\delta),
   & \inner{Y(\omega), \delta} = \sigma_C(\delta)
   \\
   \infty, & \inner{Y(\omega), \delta} > \sigma_C(\delta)
   \end{cases}
\end{equation}
where the middle term is defined to be $\infty$ in case of divide by zero.
If $\delta$ is not a direction of constancy
and $\Pr_\theta(Y \in H_\delta) > 0$, then the function
$s \mapsto \Pr_{\theta + s \delta}(Y \in H_\delta)$ is continuous,
strictly increasing, and converges to one
as $s \to \infty$.
\end{theorem}
In the two cases ruled out by the precondition of the last sentence
the function $s \mapsto \Pr_{\theta + s \delta}(Y \in H_\delta)$ is
a constant function.  If $\delta$ is a direction of constancy,
then $\Pr_\theta(Y \in H_\delta) = 1$ for all $\theta$.
If $\Pr_\theta(Y \in H_\delta) = 0$ for some $\theta$,
then $\Pr_\theta(Y \in H_\delta) = 0$ for all $\theta$.
\begin{proof}
This is a complication of Theorem~{6} in \citet{geyer-gdor}
that is essentially Theorem~{2.3} in \citet{geyer-thesis}.
However the proof of that Theorem~{2.3} contains some errors,
so a corrected proof is given in the appendix of \citet{geyer-gdor}.
Then the case $\Pr_\theta(Y \in H_\delta) > 0$ is Theorem~{2.6}
in \citet{geyer-thesis},
and the case $\Pr_\theta(Y \in H_\delta) = 0$ follows from Theorem~{2.2}
in \citet{geyer-thesis}.
The last sentence of the theorem statement is in Corollary~{5} and Theorem~{6}
in \citet{geyer-gdor}.
\end{proof}
In case $\Pr_\theta(Y \in H_\delta) > 0$,
we note three things about the limit in \eqref{eq:complete-fundamental-limit}.
\begin{itemize}
\item It is a probability distribution because the set where it is infinite
    has measure zero under the dominating measure $\Pr_\psi$
\item It is a conditional distribution of the original family,
    the conditional distribution of $Y$ given the event $Y \in H_\delta$
    for the parameter vector $\theta$.
\item It is a limit distribution of the original family,
    the limit of the distributions for parameter vectors $\theta + s \delta$
    as $s \to \infty$.  By Scheff\'{e}'s lemma, convergence of PMDF implies
    convergence in total variation of the corresponding probability measures.
\end{itemize}
Note that the distribution under discussion is both a limit distribution
and a conditional distribution.  Thinking of it as just one or the other
is missing something.  It is both.

In case $\Pr_\theta(Y \in H_\delta) = 0$,
we note one thing about the limit in \eqref{eq:complete-fundamental-limit}.
\begin{itemize}
\item It is the zero measure because it is zero on the support of
the dominating measure $\Pr_\psi$.
\end{itemize}

\subsection{Limiting Conditional Models}

We note one thing about the set of all limits
in \eqref{eq:complete-fundamental-limit}
in the case when $\Pr_\theta(Y \in H_\delta) > 0$.
\begin{itemize}
\item They form an exponential family of distributions.  The log likelihood is
\begin{equation} \label{eq:logl-expfam-lcm}
   l_\delta(\theta) =
   \inner{y, \theta} - c(\theta) - \log \Pr\nolimits_\theta(Y \in H_\delta)
\end{equation}
and this is clearly an exponential family with
\begin{itemize}
\item canonical statistic vector $y$,
\item canonical parameter vector $\theta$, and
\item cumulant function given by
\begin{equation} \label{eq:cumfun-lcm}
   c_\delta(\theta) = c(\theta) + \log \Pr\nolimits_\theta(Y \in H_\delta)
\end{equation}
\end{itemize}
\end{itemize}
\citet{geyer-gdor} calls this family the \emph{limiting conditional model}
(LCM).  Of course, there are many LCM, one in each direction, but as we
shall presently see, there is usually only one LCM of interest
in any particular data analysis.

\begin{theorem} \label{th:cumfun-lcm}
Equation \eqref{eq:cumfun-lcm} gives the correct limit of the cumulant function
to make \eqref{eq:logl-expfam-lcm} equal to the limit of \eqref{eq:logl-expfam}
when limits are taken as in Theorem~\ref{th:completion-fundamental}
in the case $\inner{y, \theta} = \sigma_C(\delta)$.
\end{theorem}
\begin{proof}
In symbols, the assertion of the theorem is
$$
   \lim_{s \to \infty} l(\theta + s \delta)
   =
   l_\delta(\theta)
$$
And
\begin{align*}
   \lim_{s \to \infty} l(\theta + s \delta)
   & =
   \lim_{s \to \infty}
   \bigl[ \inner{y, \theta + s \delta} - c(\theta + s \delta) \bigr]
   \\
   & =
   \inner{y, \theta} + \lim_{s \to \infty}
   \bigl[ s \inner{y, \delta} - c(\theta + s \delta) \bigr]
   \\
   & =
   \inner{y, \theta} + \lim_{s \to \infty}
   \bigl[ s \sigma_C(\delta) - c(\theta + s \delta) \bigr]
   \\
   & =
   \inner{y, \theta} - c(\theta) - \log \Pr\nolimits_\theta(Y \in H_\delta)
   \\
   & =
   \inner{y, \theta} - c_\delta(\theta)
\end{align*}
where the fourth equality is Theorem~{2.2} in \citet{geyer-thesis}.
\end{proof}

We know, of course, that cumulant functions can be redefined by adding an
arbitrary constant (the $c(\psi)$ in \eqref{eq:cumfun-expfam}).
As mentioned in Section~\ref{sec:define-expfam} above, we could even redefine
the cumulant function by adding an arbitrary affine function if we were to
accept a different choice of canonical statistic.  But things would get
very confusing if we made different arbitrary choices for the original
exponential family and its limiting conditional models.  Hence, however
the cumulant function of the original exponential family was chosen, we
will always use \eqref{eq:cumfun-lcm} to define cumulant functions
for limiting conditional models.

\subsection{Aggregate Exponential Family}

Denote the LCM in the direction $\delta$ by $\mathcal{P}_\delta$.
When $\Pr_\theta(Y \in H_\delta) = 0$ we say $\mathcal{P}_\delta$ is empty
(there are no limit probability distributions, and we do not want to include
the zero measure in our completion, at least not yet).
Taking limits when $\delta = 0$ does nothing
(because \eqref{eq:support-function} says $\sigma_C(0) = 0$ for any $C$
and this gives $H_\delta = \real^J$
in \eqref{eq:complete-fundamental-hyperplane}).
So $\mathcal{P}_0$ is the exponential family we started with, which we
call the original model (OM) for short.

Then
$$
   \mathcal{P} = \bigcup_{\delta \in \real^J} \mathcal{P}_\delta
$$
is a union of exponential families that contains all straight-line limits.

Under certain regularity conditions used by \citet{barndorff-nielsen},
\citet{brown}, and \citet{geyer-gdor} this union is the completion.
We do not get anything more by taking further straight-line limits
in $\mathcal{P}_\delta$ for the various $\delta$.

But in general \citep[Chapters~2 and~4]{geyer-thesis} we may need to take
further straight-line limits or general (not straight line) limits
to arrive at the completion.

Anyway, one can see why \citet{brown} gave this idea the name aggregate
exponential family.  It is a union (or aggregate) of exponential families.

\subsection{Support and Directions of Recession and Constancy}

\begin{theorem} \label{th:recession-constancy}
A vector $\delta$ is a direction of recession of the log likelihood of
a full exponential family
with closed convex support $C$ and observed value of the canonical statistic
vector $y$ if and only if $\inner{y, \delta} \ge \sigma_C(\delta)$.
If $\delta$ and $- \delta$ are both directions of recession, then $\delta$
is a direction of constancy.
Conversely, if $\delta$ is a direction of constancy and $y \in C$,
then $\delta$ and $- \delta$ are both directions of recession.
\end{theorem}
The condition $y \in C$ is measure-theoretic nonsense.
We have to say $y \in C$ to be measure-theoretically correct, but
if your data fail to satisfy $y \in C$, then something is wrong with your data.
\begin{proof}
The first sentence is Corollary~{2.4.1} in \citet{geyer-thesis}.
If $\delta$ and $- \delta$ are both directions of recession then
$$
   \inner{y, \delta} \ge \sigma_C(\delta) = \sup_{x \in C} \inner{x, \delta}
$$
and
\begin{align*}
   - \inner{y, \delta}
   & =
   \inner{y, - \delta}
   \\
   & \ge
   \sigma_C(- \delta)
   \\
   & =
   \sup_{x \in C} \inner{x, - \delta}
   \\
   & =
   - \inf_{x \in C} \inner{x, \delta}
\end{align*}
or
$$
   \inner{y, \delta} \le \inf_{x \in C} \inner{x, \delta}
$$
so
\begin{equation} \label{eq:recession-constancy-inequalities}
   \inner{y, \delta} \le \inf_{x \in C} \inner{x, \delta}
   \le
   \sup_{x \in C} \inner{x, \delta} \le \inner{y, \delta} 
\end{equation}
hence
\begin{equation} \label{eq:recession-constancy}
   \inner{x, \delta} = \inner{y, \delta}, \qquad x \in C
\end{equation}
hence $\inner{Y, \delta} = \inner{y, \delta}$ almost surely,
and $\delta$ is a direction of constancy.

Conversely, if $\delta$ is a direction of constancy, then $\inner{Y, \delta}$
is constant almost surely.  And if $y \in C$, that constant must
be $\inner{y, \delta}$.  Hence \eqref{eq:recession-constancy} holds.
Hence \eqref{eq:recession-constancy-inequalities} holds.
And we have already seen that \eqref{eq:recession-constancy-inequalities}
is equivalent to both $\delta$ and $- \delta$ being directions of recession.
\end{proof}

\subsection{Curved Line Limits}

Chapter~4 of \citet{geyer-thesis} covers completely general
limits of sequences of distributions in an exponential family of distributions,
these limits being in the sense of convergence of probability mass-density
functions.  Although the section title says ``curved line limits,'' these
limits are just limits of sequences.  We only get a line by connecting
the dots, and that line does not have to be a smooth curve.

Theorems~4.1 through~{4.5} in \citet{geyer-thesis} show that taking general
limits gives no more limits that correspond to probability distributions
than taking iterated straight line limits.  General limits can produce
limits that are subprobability distributions
\citep[Examples~4.2 through~4.4]{geyer-thesis}, but these can never be
maximum likelihood estimates for a full family, because iterated straight
line limits produce the corresponding probability distribution, which must
have higher likelihood.
This shows that we do not need to consider curved line limits, so long
as we limit our attention to full families.
%%%%% Need to worry about subsampling, which are curved exponential family
%%%%% Do not need to worry about conditional aster models, which are also
%%%%%     curved exponential family, because of associated independence
%%%%%     models, which are full exponential family

\section{Unconditional Aster Models}

Unconditional aster models are regular full exponential families.
Thus the theory of the preceding section applies to them.

\begin{theorem} \label{th:dor-predecessor-zero}
Suppose $G$ is a dependence group in an aster graph, and $y_{q(G)} = 0$
where $y$ is the observed value of the response vector,  then the
vector $\eta$ whose only nonzero component is $\eta_{q(G)} = -1$ is a
direction of recession of the saturated aster model.

Taking the limit in this direction gives the LCM that is the same as the
OM except $Y_{q(G)} = 0$ almost surely, and this means the distributions
of all successors, successors of successors, successors of successors of
successors, etc.\ are not identifiable in this LCM.
\end{theorem}
\begin{proof}
We know $q(G)$ is not a terminal node.  It is not an initial node either,
because aster models are required to have nonzero data at initial nodes.
If $Y$ is the response vector, then we know from
the predecessor-is-sample-size property that $Y_{q(G)} \ge 0$ is required.
Since $y_{q(G)}$ is at the lower endpoint of the support of $Y_{q(G)}$,
the vector described in the theorem statement is a direction of recession.

If we take limits in this direction, we get the OM conditioned on the event
$Y_{q(G)} = 0$ almost surely, and this implies $Y_j = 0$ almost surely for
all $j \prec q(G)$, where $\prec$ is the transitive closure of the successor
relation.  The cumulant function for this LCM does not depend on any
of the variables $\varphi_j$ for $j \preceq q(G)$.
This can be seen by applying \eqref{eq:cumfun-expfam} to this model.
\begin{align*}
   c(\varphi)
   & =
   c(\varphi^*) + \log\left\{ E_{\varphi^*} \left(
   e^{\inner{Y, \varphi - \varphi^*}} \right) \right\}
   \\
   & =
   c(\varphi^*) + \log\left\{ E_{\varphi^*} \left(
   \prod_{j \in J} e^{Y_j (\varphi_j - \varphi^*_j)}
   \right) \right\}
   \\
   & =
   c(\varphi^*) + \log\left\{ E_{\varphi^*} \left(
   \prod_{\substack{j \in J \\ j \not\preceq q(G)}}
   e^{Y_j (\varphi_j - \varphi^*_j)}
   \right) \right\}
\end{align*}
where $\varphi$ varies and $\varphi^*$ is fixed.
And then \eqref{eq:logl-aster-phi} shows that the log likelihood for the
LCM does not depend on any of these variables either.
\end{proof}
\begin{theorem} \label{th:dor-arrow}
Suppose $\{j\}$ is a univariate dependence group in an aster graph, and
the one-parameter exponential family of distributions for the arrow
$y_{p(j)} \longrightarrow y_j$ has closed convex support that is an
interval with endpoints $a_j$ and $b_j$ (either of which may be infinite and
which satisfy $a_j \le b_j$ with equality possible, in which case this
distribution is concentrated at one point).  Let $J$
be the set of non-initial nodes of the aster graph,
and let $Y$ denote the response vector and $y$ its observed value.

If $y_j = a_j y_{p(j)}$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-lower-bound}
   \eta_i = \begin{cases} -1, & i = j \\ a_j & i = p(j) \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

Taking the limit in the direction of recession \eqref{eq:dor-lower-bound}
gives the LCM that is the same as the OM except the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $a_j$.
This LCM is the OM conditioned on the event $Y_j = a_j Y_{p(j)}$.

If $y_j = b_j y_{p(j)}$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-upper-bound}
   \eta_i = \begin{cases} 1, & i = j \\ - b_j & i = p(j) \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

Taking the limit in the direction of recession \eqref{eq:dor-upper-bound}
gives the LCM that is the same as the OM except the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $b_j$.
This LCM is the OM conditioned on the event $Y_j = b_j Y_{p(j)}$.

In case $a_j = b_j$ the vectors \eqref{eq:dor-lower-bound}
and \eqref{eq:dor-upper-bound} are both directions of recession,
and are negatives of each other, hence both directions of constancy.
But in this case we only need one direction of constancy since one
is a scalar multiple of the other.
\end{theorem}
\begin{proof}
We have a direction of recession if and only if $\inner{Y - y, \eta} \le 0$
almost surely.
From the definition
of $a_j$ and $b_j$ we know $a_j Y_{p(j)} \le Y_j \le b_j Y_{p(j)}$
almost surely.

In case $y_j = a_j y_{p(j)}$ and $\eta$ is
given by \eqref{eq:dor-lower-bound} we have two cases.
If $p(j)$ is noninitial, then
$$
   \inner{Y - y, \eta} = - (Y_j - y_j) + a_j (Y_{p(j)} - y_{p(j)})
   =
   - Y_j + a_j Y_{p(j)}
$$
and this is indeed less than or equal to zero almost surely by definition
of $a_j$.
If $p(j)$ is initial, then
$$
   \inner{Y - y, \eta} = - (Y_j - y_j)
   =
   - Y_j + a_j y_{p(j)}
   =
   - Y_j + a_j Y_{p(j)}
$$
the last equality being that $Y_{p(j)}$ is a constant random variable,
and this is indeed less than or equal to zero almost surely by definition
of $a_j$.  Thus in either case we have \eqref{eq:dor-lower-bound} is
a direction of recession and
\begin{equation} \label{eq:lower-foo}
   \inner{Y - y, \eta} = - Y_j + a_j Y_{p(j)}
\end{equation}
Taking limits in the direction \eqref{eq:dor-lower-bound} arrives at
the limiting conditional model that conditions
on \eqref{eq:lower-foo} being equal to zero, that is, on the event
$Y_j = a_j Y_{p(j)}$.  By the predecessor-is-sample-size principle
this is the same thing as saying the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at $a_j$.

The proofs of the assertions about \eqref{eq:dor-upper-bound} are similar.

That the conditions for \eqref{eq:dor-lower-bound}
and \eqref{eq:dor-upper-bound} both hold when $a_j = b_j$ and that
one is then the negative of the other is obvious.
That $\eta$ and $- \eta$ both being
directions of recession implies either is a direction of constancy is
\citet{geyer-gdor} Theorem~3 part (e) and Theorem~1 part (g).
\end{proof}
As we see in the proof, there are two cases.  When $p(j)$ is noninitial
the arrow $y_{p(j)} \longrightarrow y_j$ represents a conditional
distribution and $\eta$ has two nonzero components unless $a_j = 0$ and
$\eta$ is given by \eqref{eq:dor-lower-bound} or $b_j = 0$ and
$\eta$ is given by \eqref{eq:dor-upper-bound}.
When $p(j)$ is initial the arrow $y_{p(j)} \longrightarrow j$ represents,
in effect, a marginal distribution and $\eta$ has one nonzero component.
But the formulas \eqref{eq:dor-lower-bound} and \eqref{eq:dor-upper-bound}
work in either case because the middle case does not occur when
$p(j) \notin J$.

When $a_j = 0$ and $y_j = 0$ the vector \eqref{eq:dor-lower-bound} agrees
with the direction of recession in Theorem~\ref{th:dor-predecessor-zero}
when $j$ in this theorem is $q(G)$ in that theorem.

The case $a_j = b_j$ cannot occur in aster models allowed
by R package \code{aster}.  But once we start
taking limits, then they can.  So they are allowed by R package \code{aster2}.

Degenerate distributions concentrated at $a_j$ or $b_j$ are further
discussed in the appropriate section of Appendix~\ref{app:families}
(the details depend on the family the degenerate distribution is derived
from).

\begin{theorem} \label{th:dor-multinomial}
Suppose $G$ is a multinomial dependence group in an aster graph.
Let $J$ be the set of non-initial nodes of the aster graph,
and let $Y$ denote the response vector and $y$ its observed value.

If $j \in G$ and $y_j = 0$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-multinomial}
   \eta_i = \begin{cases} -1, & i = j \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

The vector $\eta$ having index set $J$ and coordinates
\begin{equation} \label{eq:doc-multinomial}
   \eta_i = \begin{cases} -1, & i \in G \\
   +1, & i = q(G) \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of constancy of the saturated aster model.

Taking the limit in the direction of recession \eqref{eq:dor-multinomial}
gives the LCM that is the same as the OM except the arrow
$y_{p(j)} \longrightarrow y_j$ has the degenerate family of distributions
concentrated at zero, and the conditional distribution for dependence group
$G$ becomes a partially degenerate multinomial distribution that has
$Y_j = 0$ almost surely.
\end{theorem}
\begin{proof}
For \eqref{eq:dor-multinomial} we need to show
that $\inner{Y - y, \eta} \le 0$ almost surely.
This is obvious.
$$
   \inner{Y - y, \eta} = - (Y_j - y_j)
   =
   - Y_j
$$
and this is indeed less than or equal to zero almost surely
by definition of the multinomial distribution.

For \eqref{eq:doc-multinomial} we need to show
that $\inner{Y - y, \eta} = 0$ almost surely,
where $Y$ and $y$ are as above.
This too is obvious.
$$
   \inner{Y - y, \eta} = (Y_{q(G)} - y_{q(G)}) - \sum_{j \in G} (Y_j - y_j)
$$
and this is indeed equal to zero almost surely,
by definition of the multinomial distribution.

By Theorem~\ref{th:completion-fundamental} taking limits in the direction
\eqref{eq:dor-multinomial} results in an LCM that conditions the OM on
the event $Y_j = 0$ almost surely, and this corresponds to
the arrow $y_{p(j)} \longrightarrow y_j$ having the degenerate family
concentrated at zero almost surely.  And this makes the multinomial
distribution of $Y_G$ given $Y_{q(G)}$ partially degenerate.
\end{proof}

The vector \eqref{eq:dor-multinomial} agrees
with the direction of recession in Theorem~\ref{th:dor-predecessor-zero}
when $j$ in this theorem is $q(G)$ in that theorem.

In case all but one of the components of a multinomial $Y_G$ are zero,
we can apply the theorem repeatedly to get a completely degenerate
multinomial family.  If $j \in G$ and $y_k = 0$ for $k \in G \setminus \{j\}$,
then repeated limits give us the degenerate multinomial family that
conditions on $Y_k = 0$ for $k \in G \setminus \{j\}$, but then we must
also have $Y_j = Y_{q(G)}$ almost surely by definition of the multinomial
distribution.

These partially degenerate multinomial distributions are further
discussed in Section~\ref{app:sec:multinomial} in the appendix.

\begin{theorem} \label{th:dor-normal}
Suppose $G = \{j, k\}$ is a normal-location-scale dependence group
in an aster graph.
Let $J$ be the set of non-initial nodes of the aster graph,
and let $Y$ denote the response vector and $y$ its observed value.

If $y_{q(G)} = 1$, then the vector $\eta$ having index set $J$
and coordinates
\begin{equation} \label{eq:dor-normal}
   \eta_i = \begin{cases} 2 y_j, & i = j \\
   -1, & i = k \\
   0, & \text{otherwise} \end{cases}
\end{equation}
is a direction of recession of the saturated aster model.

But this direction of recession does not produce a limiting conditional model
because it corresponds to the case $\Pr_\theta(Y \in H_\delta) = 0$ in
in Theorem~\ref{th:completion-fundamental}.

If $y_{q(G)} \ge 2$, then almost surely there are no directions of recession
for this family.
\end{theorem}
\begin{proof}
The closed curve consisting of points $y_G$ such that $y_k = y_j^2$
supports the conditional distribution of $y_G$ given $y_{q(G)} = 1$.
The function $f : y_j \mapsto y_j^2$ is a convex function.
By the gradient inequality \citep[Theorem~2.13 (b)]{rockafellar-wets}
$$
   f(Y_j) - f(y_j) \ge f'(y_j) (Y_j - y_j)
$$
but this can also be written
$$
   Y_k - y_k \ge 2 y_j (Y_j - y_j)
$$
because $f(y_j) = y_k$ and $f(Y_j) = Y_k$ and $f'(y_j) = 2 y_j$.
And it can also be written
$$
   - \eta_k (Y_k - y_k) \ge \eta_j (Y_j - y_j)
$$
because of the definition of $\eta$ in the theorem statement.
And this says $\inner{Y - y, \eta} \le 0$ so $\eta$ is a direction of
recession.

Since $f$ is strictly convex
\citep[Theorem~2.13 ($\text{a}'$)]{rockafellar-wets},
we have strict inequality in the gradient inequality
\citep[Theorem~2.13 ($\text{b}'$)]{rockafellar-wets} when $Y_j \neq y_j$.
Hence we have \hbox{$\inner{Y - y, \eta} < 0$} almost surely.
But the latter is equivalent to $\Pr_\theta(Y \in H_\delta) = 0$ in
in Theorem~\ref{th:completion-fundamental}.

The (closed) convex support of the family for sample size one is
the set
$$
   C = \set{ y_G : y_k \ge y_j^2 }
$$
The (closed) convex support of the family for sample size $n$ is
is the $n$-fold Minkowski sum of this set, which is $n C$
\citep[Proposition~2.23]{rockafellar-wets}.  Because the distributions
in this family are continuous, the interior of $n C$ actually supports
the family for $n \ge 2$.  Hence (almost surely) it is not possible
to observe data on the boundary of the convex support for $n \ge 2$.
\end{proof}

Since any two-parameter-normal dependence group must be terminal,
we can ignore all such dependence groups in figuring out GDOR for the
rest of the nodes of the graph.  Then we can take the limit in the direction
of the GDOR found and get rid of all variables that are zero almost surely
in that LCM.  Then we can look for DOR again using this theorem
with the case $y_q(G) = 0$ now excluded.

What this theorem says is that we had better not have any of the directions
of recession it describes because otherwise we don't have an LCM and MLE
do not exist in any sense.

\begin{theorem} \label{th:dor-aster}
The set of all directions of recession is a closed convex cone.
The set of all directions of constancy is a vector subspace.
Every direction of constancy is also a direction of recession.
A vector $\delta$ is a direction of constancy if and only if
both $\delta$ and $- \delta$ are directions of recession.

The set of all directions of recession of saturated aster models
having families described by Theorems~\ref{th:dor-predecessor-zero},
\ref{th:dor-arrow}, and~\ref{th:dor-multinomial}
is the smallest closed convex cone containing all of the directions
of recession described by those theorems.

The set of all directions of recession for normal-location-scale
dependence groups $G$ with $y_{q(G)} > 0$
is the smallest closed convex cone containing all of the directions
of recession described by Theorem~\ref{th:dor-normal}.

The set of all directions of constancy of such aster models
is the smallest vector subspace containing all of the directions
of constancy described by those theorems.
\end{theorem}
\begin{proof}
The assertions of the first paragraph are all in Theorems 1 and 3 of
\citet{geyer-gdor} and the discussion surrounding them.

Sections~4.1 and~{4.2} in \citet{geyer-thesis} characterize all possible
limit distributions in an exponential family of distributions.
Theorem~{2.7} in \citet{geyer-thesis} says that all limit distributions
can be obtained by taking iterated straight line limits.
The limit of a product being the product of the limits, when we take a
limit we get a limit in each term of the fundamental factorization of
aster models \eqref{eq:factorize}.
Thus when we have all possible limiting conditional models for each of
the families for each of the dependence groups, we have also gotten
all of the limits for the whole aster model.
\end{proof}

Now we need to figure out how to use these theorems when applied to
general unconditional aster models (canonical affine submodels of
the saturated aster model).  The principle is simple.  If $M$ is the
model matrix of a canonical affine submodel, then $\delta$ is a direction
of recession (resp.\ constancy) of that submodel if and only
if $\eta = M \delta$ is a direction of recession (resp.\ constancy)
of the saturated model.

So we revisit the theorems.

In Theorem~\ref{th:dor-predecessor-zero} we have one direction of recession
whose only nonzero coordinate is $\eta_j = - 1$ when $j$ is a predecessor
node such that $y_j = 0$.  Then we may ignore all nodes that are successors,
successors of successors, etc.\ of such nodes because we may figure out GDOR
for graphs that do not have these nodes.  So we only include such DOR when
$y_j = 0$ but $y_{p(j)} > 0$.

In Theorem~\ref{th:dor-arrow} we have either
\eqref{eq:dor-lower-bound} or \eqref{eq:dor-upper-bound} or both or neither
is a direction of recession.  If $a_j = b_j$, then we can take either to
be a direction of constancy and ignore the other.
If $y_{p(j)} = 0$, then we can ignore the directions of recession as discussed
above.  Thus we get only one direction of recession or direction of constancy.

In Theorem~\ref{th:dor-multinomial} when $y_{q(G)} > 0$ we have one
direction of recession for each $j \in G$ such that $y_j = 0$ and
we also have one direction of constancy for the whole dependence group.

In Theorem~\ref{th:dor-normal} we have one direction of recession for
each dependence group with $y_{q(G)} = 1$.

So that completes our list of directions of recession and constancy.
\begin{theorem} \label{th:dor-aster-explicit}
A vector $\delta$ is a direction of recession
of an unconditional canonical affine submodel
if and only if $\eta = M \delta$ has the form
\begin{equation} \label{eq:dor-aster-explicit}
   \eta = \sum_{j \in J_r} e_j \eta_j
\end{equation}
where $J_r$ is the index set for directions of recession,
$J_c$ is the index set for directions of constancy, and
the $e_j$ are real numbers satisfying $e_j \ge 0$
for $j \in J_r \setminus J_c$.
\end{theorem}
\begin{proof}
This just makes explicit what Theorem~\ref{th:dor-aster} already says.
\end{proof}

Consider the following linear program having variables $e_j$
for $j \in J_r \cup J_c$ and $\delta_k$ for $k \in K$.
\begin{alignat}{2}
  \text{maximize}   & \ \sum_{j \in J_r \setminus J_c} e_j
  \nonumber
  \\
  \text{subject to} & \ 0 \le e_j \le 1, & \qquad & j \in J_r \setminus J_c
  \label{prog:foo}
  \\
                    & \ M \delta =
  \sum_{j \in J_r} e_j \eta_j
  \nonumber
\end{alignat}
\begin{theorem} \label{th:lin-prog-one}
Linear program \eqref{prog:foo} always has a solution.
Linear program \eqref{prog:foo} has optimal value zero if and only if
there does
not exist a direction of recession that is not a direction of constancy
and the MLE exists in the originally given unconditional aster model.
Otherwise, the optimal value is greater than or equal to one and the
$\delta$ part of the solution is a direction of recession that is not
a direction of constancy.
\end{theorem}
\begin{proof}
The feasible region is nonempty because it always contains the zero vector.
Then solutions exist because the objective function is obviously bounded
on the feasible region.

The optimal value is zero if and only if at the solution $e_j = 0$ for
$j \in J_r \setminus J_c$, in which case the $\delta$ part of the solution
is a direction of constancy of the submodel and $\eta = M \delta$ is a
direction of constancy of the saturated model.  The assertion about
existence of MLE is then Theorem~{4} in \citet{geyer-gdor}.

If there exists any feasible point such that some $e_j$
for $j \in J_r \setminus J_c$ is nonzero, then we can multiply all components
of $\delta$ and all $e_j$ by a strictly positive constant to make
the largest $e_j$ for $j \in J_r \setminus J_c$ equal to one, in which case
the objective function is greater than or equal to one.
Optimizing then only increases the objective function.
The solution then is clearly a direction of recession that is not a
direction of constancy by Theorem~\ref{th:dor-aster-explicit}.
\end{proof}

We are not done yet because we haven't yet in the terminology of
\citet{geyer-gdor} found a \emph{generic} direction of recession (GDOR).
That will be one that has the maximal number of nonzero $e_j$ for
for $j \in J_r \setminus J_c$.  Since any nonnegative combination of
directions of recession is another direction of recession, we can
seek GDOR by modifying our linear program to find DOR with $e_j > 0$
that we haven't found so far.

Let $J^{*}$ be any nonempty subset of $J_r \setminus J_c$,
and consider the following linear program.
\begin{alignat}{2}
  \text{maximize}   & \ \sum_{j \in J^{*}} e_j
  \nonumber
  \\
  \text{subject to} & \ 0 \le e_j \le 1, & \qquad & j \in J^{*}
  \label{prog:foobar}
  \\
                    & \ 0 \le e_j, & \qquad &
                      j \in (J_r \setminus J_c) \setminus J^{*}
  \nonumber
  \\
                    & \ M \delta = \sum_{j \in J_r} e_j \eta_j
  \nonumber
\end{alignat}

Then we iterate
(Algorithm~\ref{alg:unconditional}\ifthenelse{\equal{\arabic{page}}{\pageref{alg:unconditional}}}{).}
{, page~\pageref{alg:unconditional}).}
\begin{algorithm}
\caption{Find GDOR for Unconditional Aster Model}
\label{alg:unconditional}
\begin{tabbing}
Set $J^{*} = J_r \setminus J_c$\\
Set $J^{{*}{*}} = \emptyset$\\
Set $\gamma = 0$\\
\textbf{repeat} \{\\
\qquad \= Solve the linear program \eqref{prog:foobar}\\
\> \textbf{if} (linear program has no solution) \textbf{error}\\
\> \textbf{if} (optimal value is zero) \textbf{break}\\
\> Set $\delta$ to be the $\delta$ part of the solution of the linear program\\
\> Set $\gamma = \gamma + \delta$\\
\> Set $e$ to be the $e$ part of the solution of the linear program\\
\> Set $J^{**} = J^{**} \cup \set{j \in J^{*} : e_j > 0}$\\
\> Set $J^{*} = J^{*} \setminus J^{{*}{*}}$\\
\> \textbf{if} ($J^{*} = \emptyset$) \textbf{break}\\
\}
\end{tabbing}
\end{algorithm}
\begin{theorem} \label{th:lin-prog-two}
Algorithm~\ref{alg:unconditional} always terminates, and $\gamma$ is
a generic direction of recession unless $\gamma = 0$, in which case
the MLE exists in the originally given unconditional aster model.
\end{theorem}
\begin{proof}
The algorithm must terminate because $J^{*}$ decreases in each iteration.
So we terminate when $J^{*} = \emptyset$ if not before.

Since each $\delta$ found is a direction of recession that is not
a direction of constancy, so is $\gamma$.

The termination condition of optimal value zero or $J^{{*}{*}} = \emptyset$,
proves that $\gamma$ is such that $\eta = M \gamma$ has the most possible
nonzero components $\eta_j$ for $j \in J_r \setminus J_c$.  Hence it
is generic.
\end{proof}
