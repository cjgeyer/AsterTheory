
\chapter{Introduction}
\label{ch:introduction}

\section{Background}
\label{sec:background}

Aster models \citep*{aster1,aster2,reaster} \index{aster model}
are parametric statistical models
specifically designed for life history analysis.  They are exponential family
\index{life history analysis|(}
models that generalize generalized linear models (GLM) that are also
exponential family models (for example, logistic regression and
Poisson regression with log link) in two ways
\begin{itemize}
\item in GLM components of the response vector are
    necessarily conditionally independent given covariate data
    but in aster models they need not be, and
\item in GLM the conditional distributions of components of the
    response vector given covariate data all come from the same family
    but in aster models they need not.
\end{itemize}
As generalizations of GLM, aster models are also regression models.
They model the conditional distribution of the response vector given
covariate data.  The marginal distribution of covariate data is not
modeled.

In life history analysis,
the data are about survival and reproduction
of biological organisms.  Thus aster models also generalize discrete time
survival analysis (aster models model not only survival but also
what happens conditional on survival).
Aster models unify many disparate kinds of life history analysis that have
appeared in the biological literature: comparison of Darwinian fitness between
various groups \citep{aster1,aster2}, estimation of fitness landscapes
(\citealp{lande-arnold}; \citealp{aster2,aster3}, \citealp*{aster-hornworm}),
Leslie matrix analysis
\citep{caswell}, life table analysis in demography \citep{goodman},
and estimation of population growth rates
\citep{fisher,lenski-service,aster2,aster-hornworm}.
Aster models also generalize zero-inflated Poisson regression \citep{lambert},
negative binomial regression (overdispersed Poisson regression),
and zero-inflated negative binomial regression.

Aster models are a special case of graphical models \citep{lauritzen}.
In particular, they are statistical models for which the joint distribution
of the response vector factorizes completely as a product of marginal and
conditional distributions (equation~\eqref{eq:factorize} below).
This makes aster models a special case of chain graph models
\citep[Sections~2.1.1 and~3.2.3]{lauritzen}.
Aster models also have the predecessor-is-sample-size property
(Section~\ref{sec:piss} below)
that makes the joint distribution of the response vector an exponential
family.  This property can be seen to generalize unnamed properties
of survival analysis, life-table analysis, Leslie matrix analysis,
and population growth rate analysis (Section~\ref{sec:mu-and-xi} below).
\index{life history analysis|)}

\section{Software}
\label{sec:software}

Currently, all software for aster models is written in the R statistical
computing language \citep{r-core}.  There are two CRAN
(\url{cran.r-project.org}) packages, \code{aster} \citep{aster-package} and
\code{aster2} \citep{aster2-package}.
\index{R package!aster@\code{aster}}
\index{R package!aster2@\code{aster2}}
Both R and these packages can be installed in minutes on any computer,
so any user can get started with aster models in almost no time.

R package \code{aster} is the most complete.
It does everything except dependence groups
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
and limiting conditional models.
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%

R package \code{aster2} is the very incomplete.
It does do dependence groups and limiting conditional models, but everything
else is either missing or much harder to use than in R package \code{aster}.

So any aster model that can be done with R package \code{aster} should
be done with that package.

\section{Summary}

Aster models combine three ideas
\begin{itemize}
\item factorization of a joint distribution into a product of marginals
    and conditionals (Section~\ref{sec:factorization} below),
\item the predecessor is sample size property (Section~\ref{sec:piss} below),
    which says the conditioning variables in the conditional distributions
    in the factorization act like sample sizes, and
\item the exponential family property (Section~\ref{sec:aster-expfam} below),
    which says the conditional distributions in the factorization are
    exponential families of distributions,
\end{itemize}
into one big idea.  Together they make the joint distribution of the
response vector also an exponential family.  And this makes aster models
as well-behaved as generalized linear models or log-linear models for
categorical data analysis, even though they are far more more complicated.

\section{Vectors and Subvectors}
\label{sec:subvector}

We adopt a notation from \citet{lauritzen} for subvectors, but fuss about it
more.

As in set theory \citep[Section~8]{halmos-set-theory}, if $A$ and $B$
are sets, then $A^B$ denotes the set of all functions $B \to A$.
In particular, if $J$ is a finite set, then we let $\real^J$ denote
the set of all functions $J \to \real$.  This set can also be considered
a finite-dimensional vector space.  That functions $J \to V$ where $J$ is
any set and $V$ is a field or a vector space can be considered
vectors is the reason the study of infinite-dimensional topological vector
spaces is called functional analysis.

Another way of looking at this distinction is that the usual view of
finite-dimensional vector spaces is that they are $\real^d$ for some
natural number $d$, which is tantamount to insisting that the index
set for vectors in this space must be the set $\{1, \ldots, d\}$.
Here we are saying the index set can be any finite set $J$.

Even though we consider vectors to be functions, we write evaluation
of these functions $y_j$ so it looks like usual notation.  We even say
that $y_j$ is a component of the vector $y$ rather than the value of
the function $y$ at the point $j$.  But behind the scenes our vectors
are also functions, and we could write $y(j)$ instead of $y_j$.

We need a notation for subvectors of a vector.  If $y$ is an element
of $\real^J$ and $A \subset J$, then we let $y_A$ denote the restriction
of $y$ to the set $A$.
As such, it is an element of the vector space $\real^A$.
Like all functions, it knows its domain and codomain.
It knows it is a function $A \to \real$.
So it knows its components are $y_j$, $j \in A$.
And these are also the components of $y$ for $j \in A$.
Since the components of $y_A$ are a subset of the components of $y$,
we say $y_A$ is a \emph{subvector} of $y$.
\index{subvector}

If we were to insist that all vectors, including subvectors, have
index sets $\{1, \ldots, k\}$ for some natural number $k$.  Then we could
not distinguish different subvectors of the same length, or at least could
not without ugly and cumbersome extra decoration of the notation.
(It is hard to explain how elegant this notation is with simple examples,
but a perusal of Appendices~\ref{app:markov} and~\ref{app:regular}
will show this notation is
extremely powerful, and those appendices would be much longer and more confusing
if we had to use conventional notation with indices going from 1 to $k$.)

Our notation does have the drawback that we have only the convention
that lower case letters denote elements of sets and upper case letters
denote sets to indicate that $y_j$ is a component of a vector or subvector
(the value of a function at the index $j$) and $y_A$ is a subvector
(the restriction of a function to the set $A$, so still a function,
not the value of a function).
We also consider any subscript notation that clearly denotes a set
as indicating a subvector, for example, $y_{\{1, 3, 5\}}$ or $y_{\{j\}}$ or
$y_{\set{ j \in J : j \prec i }}$.

\section{Regression Notation}

Strictly speaking, in regression theory, every probability and expectation
is conditional on covariate data, at least on the part of the covariate data
that is considered random rather than fixed by the design of the experiment.
Thus to be hyperpedantic, we should always write
\begin{gather*}
   E(Y_A \mid \text{the part of covariate data that is random})
   \\
   \Pr(Y_A \in B \mid \text{the part of covariate data that is random})
\end{gather*}
rather than $E(Y_A)$ or $\Pr(Y_A \in B)$.  But, like most regression books,
we will not do this.  The dependence of probabilities and expectations on
covariates is usually not made explicit in the notation.

This is especially important in aster models when components of the response
vector depend on the values of other components, so we frequently write
\begin{gather*}
   E(Y_A \mid y_j)
   \\
   \pr(y_A \mid y_j)
\end{gather*}
and the like.  And we do not want this dependence confused with dependence
on covariate data.

When necessary for clarity, as in the discussion of fitness landscapes,
which are regression functions,
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
we can explicitly denote the dependence on covariate data in conditional
probabilities and expectations.

\section{Factorization}
\label{sec:factorization}

If $J$ is the index set of the response vector $y$ of an aster model,
then there is a partition $\mathcal{G}$ of $J$
and a function $q : \mathcal{G} \to N$, where $N \supset J$, such that
\index{aster model!property!factorization}
\index{factorization|seeunder{aster model}}
the joint distribution of $y$ factorizes as
\begin{equation} \label{eq:factorize}
   \pr(y) = \prod_{G \in \mathcal{G}} \pr(y_G \mid y_{q(G)})
\end{equation}
We emphasize that $q : \mathcal{G} \to N$ maps elements of $\mathcal{G}$,
which are sets of indices, to elements of $N$, which are single indices,
so each $y_G$ in \eqref{eq:factorize} is a subvector of $y$,
and each $y_{q(G)}$ in \eqref{eq:factorize} is a component of $y$.

In this factorization, each component $y_j$ of the response vector $y$
appears exactly once ``in front of the bar'' in a conditional on
the right-hand side (because $\mathcal{G}$ is a partition of $J$ so
each $j \in J$ is in exactly one $G \in \mathcal{G}$).
So every component of $y$ is treated as random (the joint distribution
of $y$ is modeled).
Random variables $y_{q(G)}$ that appear ``behind the bar'' in a conditional
on the right-hand side may or may not be elements of $y$.  They are not
if $q(G) \notin J$.  The distribution of such random variables is not
modeled by \eqref{eq:factorize}.  So they are treated as constant random
variables.

We say \eqref{eq:factorize} is \emph{valid} if what are denoted as
conditional distributions on the right-hand side agree with the conditional
distributions derived from the left-hand side (the joint distribution) by
the usual operations of probability theory.
\begin{theorem} \label{th:factorize}
The factorization \eqref{eq:factorize} is valid if and only if
the partition $\mathcal{G}$ can be totally ordered
by some total ordering $<$ such that $q(G) \in H$ implies $G < H$.
\end{theorem}
A proof of this theorem is straightforward and given
in Appendix~\ref{app:factorize}.
It could also be derived from the discussion of chain graph models
in \citet[equation~3.23]{lauritzen}.

In \eqref{eq:factorize} we have been deliberately vague about what $\pr$ is
supposed to mean, since there are many ways to specify probability
distributions and any of them will do.
\begin{itemize}
\item If $y$ is a discrete random vector,
      then $\pr$ could denote probability mass functions.
\item If $y$ is a continuous random vector,
      then $\pr$ could denote probability density functions.
\item If $y$ is a partly discrete and partly continuous continuous
      random vector (either some components discrete and some components
      continuous or some components a mixture of discrete and continuous)
      then $\pr$ could denote probability mass-density functions.
\item No matter what, $\pr$ could denote cumulative distribution functions.
\item No matter what, $\pr$ could denote probability measures and
      regular conditional probability measures (also called Markov kernels).
\end{itemize}
In any of these cases the multiplication indicated in \eqref{eq:factorize}
is actual multiplication of real-valued thingummies.

\subsection{Topological Sort}

The total order asserted to exist by the theorem need not be unique and
usually is not unique.
We can find such a total order using the algorithm called topological sort
\index{topological sort}
\citep[Section~6.6]{aho-et-al}.

Using R function \code{tsort} in R package \code{pooh}
\citep{pooh-package}
\index{R package!pooh@\code{pooh}}
for each $G \in \mathcal{G}$ such that there exists
a (necessarily unique) $H \in \mathcal{G}$ such that $q(G) \in H$
let $G$ be a component of the vector \code{from} that is an argument to
\code{tsort} and
let $H$ be the corresponding component of the vector \code{to} that is another
argument to \code{tsort} and
neither vector has any other components.  Then invoking \code{tsort} with
these \code{from} and \code{to} arguments and \code{domain} argument that is
$\mathcal{G}$ strung out in a vector in any order
will determine a (not necessarily unique) total order that agrees with
Theorem~\ref{th:factorize}.  If the user has made a mistake and incorrectly
specified the $q$ function so there is no total order that satisfies
Theorem~\ref{th:factorize}, then \code{tsort} will give an error.

Current code in R packages \code{aster} and \code{aster2} does not
actually use the topological sort algorithm but rather forces the user
to input the data so that the numerical order of the components of the
response vector is the total order, that is, considering the index set
of the response vector to be $\{1, \ldots, n\}$ for some integer $n$,
current code requires $q(G) < j$ for any $j \in G$.
It is up to the user to input the data in this way.  The computer is no help.

But we could make the computer figure this out in future versions of the
software.

\subsection{Further Factorization}
\label{sec:further-factorize}

In \citet{lauritzen} chain graph factorizations like our \eqref{eq:factorize}
and his equation (3.23) can be further factorized, his equation (3.24).
But in aster model theory, we shall never be interested in such further
factorizations (even in cases where they are possible) and never use any
notation that allows for them.  So we will never have an analog of equation
(3.24) in \citet{lauritzen}.  For us, factorization is \eqref{eq:factorize}.

\section{Graphs}

Each factorization goes with a graph \citep[Section~3.2.3]{lauritzen}.
\index{aster graph}
\index{graph|see{aster graph}}
The nodes of the graph are either the elements of $N$ or the components
of $y$ corresponding to these elements ($y_j$ for $j \in N$).
There is a directed edge, also called an \emph{arrow},
\index{node}
\index{edge}
\index{arrow}
\index{line}
$q(G) \longrightarrow j$ (or if one prefers $y_{q(G)} \longrightarrow y_j$)
for every $G \in \mathcal{G}$ and every $j \in G$.
There is an undirected edge, also called a \emph{line},
$j \myline k$ (or if one prefers $y_j \myline y_k$)
for every $G \in \mathcal{G}$ and every $j, k \in G$ such that $j \neq k$.

As we have just seen, the function $q$ determines the graph
(the function $q$ knows its domain $\mathcal{G}$ and codomain $N$).
Conversely, the graph determines the function $q$.
\begin{itemize}
\item The set $J = \bigcup \mathcal{G}$ is the set of nodes of the graph
    that have incoming arrows (as we shall see, these nodes are called
    non-initial).
\item The elements of $\mathcal{G}$ are the maximal connected components
    of the graph of lines having node set $J$.
    (The graph of lines is the graph obtained by keeping all the nodes
    and lines but removing all the arrows).
    \index{aster graph!of lines}
    If a node $j \in J$ has no incoming lines, then the singleton set $\{j\}$
    is an element of $\mathcal{G}$.
\item The graph of $q$ is determined by the arrows: $(G, q(G))$ is an
    argument-value pair whenever there is an arrow $j \longrightarrow k$
    with $j = q(G)$ and $k \in G$.
\end{itemize}

Thus we can reason with with graphs or with $q$ functions (which we will
soon learn to call \emph{predecessor functions}, Section~\ref{sec:other} below).
Graphs can be helpful, but we do not have to use them.

\subsection{Exception}
\label{sec:exception-dependence-group-lines}

In theory, as stated above, there is a line between every pair of distinct
elements of every dependence group and no other lines.

In practice, this leads to annoying and unnecessary clutter.
Because we never further factorize dependence groups
(Section~\ref{sec:further-factorize} above), we can find the dependence
groups from the graph if we only include enough lines so that each
dependence group is a connected subgraph of the graph of lines
(again, this is the graph obtained by keeping all the nodes and lines but
removing all the arrows).
\index{aster graph!of lines}

This exception is illustrated in graph \eqref{gr:multi} below
where only two lines
rather than three are used to connect the nodes of each
dependence group of size three.

\section{Graphical Terminology}
\label{sec:graphical-terminology}

In aster theory, we say
\begin{itemize}
\item a node is \emph{initial} if it has no incoming arrows
\index{node!initial}
\index{initial node|seeunder{node}}
    or lines (when thinking about the graph) or if it is not an element
    of $J = \bigcup \mathcal{G}$ (when thinking about the function $q$),
\item a node is \emph{terminal} if it has no outgoing arrows
\index{node!terminal}
\index{terminal node|seeunder{node}}
    (it may have outgoing lines and will have outgoing lines if it is
    an element of an element of $\mathcal{G}$ that is not a singleton set)
    (when thinking about the graph) or if it is not an element
    of $\set{ q(G) : G \in \mathcal{G} }$
    (when thinking about the function $q$),
\item if there is an arrow $j \longrightarrow k$, then we say that $j$
    is the \emph{predecessor} of $k$ (or $y_j$ is the predecessor of $y_k$),
\index{node!predecessor}
\index{predecessor node|seeunder{node}}
\item and, conversely, that $k$ is a \emph{successor} of $j$
    (or $y_k$ is the successor of $y_j$).
\index{node!successor}
\index{successor node|seeunder{node}}
\end{itemize}

In mainstream graphical model theory, a different terminology is more widely
used \citep{lauritzen} root = initial, leaf = terminal, parent = predecessor,
child = successor.  We do not use this terminology in aster model theory
because it can cause serious confusion in biological applications.

As a general policy, we eschew all terminology based on biological analogies
when there is an available alternative (even when that alternative is less
popular).

In any aster graph every node has at most one predecessor and all nodes in
\index{aster model!property!at most one predecessor}
the same $G \in \mathcal{G}$ must have the same predecessor (because $q$
is a function that takes elements of $\mathcal{G}$ as arguments).

In mainstream graph theory, a chain graph with only arrows (no lines) having
the at-most-one-predecessor property is called a \emph{forest} and its maximal
connected components are called \emph{trees}, but we do not use this terminology
either (avoiding serious confusion when the application involves data on
real trees in real forests).  It is enough to say that aster graphs
have the at-most-one-predecessor property.

In mainstream graph theory, there is a term \emph{ancestor} that means
predecessor, or predecessor of predecessor,
or predecessor of predecessor of predecessor,
or predecessor of predecessor of predecessor of predecessor,
or the same with arbitrarily many repetitions of ``predecessor of.''
And there is a converse term \emph{descendant}, that is, $i$ is an ancestor
of $j$ if and only if $j$ is a descendant of $i$.

In aster model theory we avoid these terms too (avoiding confusion when
the application involves real biological organisms with real biological
ancestors and real biological descendants).  If we need the concepts,
then we use the long-winded descriptions
predecessor of predecessor of predecessor and so forth or
successor of successor of successor and so forth.
Fortunately, we rarely need these concepts.
And when we do need these concepts we can avoid the cumbersome verbiage
by using mathematical notation introduced in Section~\ref{sec:closure}
below.

Finally, we need a term for $\mathcal{G}$ and its elements.
The terminology we have been using in our writings about aster models is
elements of $\mathcal{G}$ are \emph{dependence groups}.
\index{dependence group}
The mainstream graphical models terminology \citep{lauritzen} is
\emph{chain components}.  Both have two words and four syllables.
Neither is very elegant.  We don't like the ``chain'' terminology because
we are not using general chain graph theory (aster models are very special
chain graphs).  Our term \emph{dependence group} is not great, but we haven't
thought of a better term.

\subsection{Exception}
\label{sec:exception-root}

R package \code{aster} uses ``root'' node for initial node.
\index{node!root}
\index{root node|seeunder{node}}
We hadn't completely thought through the terminology when that package
was written, and we have kept this inconsistency for reasons of backward
compatibility.

\section{Two Kinds of Aster Graphs}
\label{sec:scare-quotes}

The graphs for aster models are often very large with thousands or tens of
thousands of nodes, but usually they are composed of isomorphic subgraphs.
So drawing one of these isomorphic subgraphs is enough.
If you've seen one, you've seen them all.
(Graphs are isomorphic if a drawing of one can be laid on a drawing of the
other with everything --- nodes, lines, and arrows --- matching up.)

An aster graph need not be composed of all isomorphic subgraphs,
but the only published example of that is, as far as I know,
\citet{aster-hornworm}.

To distinguish these two kinds of graphs, we call the aster graph described
in the preceding section the \emph{full aster graph} (we consider the ``full''
redundant but the emphasis may help avoid confusion).
\index{aster graph!full}

Certain subgraphs of the full aster graph, we then call graphs
for ``individuals'' (in scare quotes for reasons to be explained presently).
These are easier to recognize than describe.

Current aster software (Section~\ref{sec:software} above) forces
$q(G) \neq q(H)$ whenever $G \neq H$ and $q(G)$ and $q(H)$ are initial nodes.
In this case, the graph for an ``individual'' (in scare quotes)
consists of the subgraph consisting of one initial node and all of its
successors or successors of successors or successors of successors
of successors and so forth with arbitrarily many repetitions
of ``successors of'' and all of the arrows and lines in the full graph
connecting these nodes.
\index{aster graph!for ``individual''}
(This is where the term ``descendant'' in its graph-theoretic sense would
come in handy if we allowed ourselves to use it.  The graph for an
``individual'' consists of one initial node, all of its descendant nodes,
and all of the lines and arrows going between these nodes.  But once we
have the idea of the graph for an ``individual'' we no longer need the
term ``descendant.'')

But aster theory as described so far does not force this convention.
If $y_j = 1$, for all initial nodes $j$, which is the case with most
(but not all) aster applications, then it would do no harm if all initial
nodes were fused into one initial node.  That would invalidate nothing but
the way we just described graphs for ``individuals'' (in scare quotes).

Thus we have to be a bit more careful.  If $G$ is a dependence group whose
predecessor $q(G)$ is initial, then the graph for the ``individual''
(in scare quotes) containing $G$ consists of $q(G)$, the nodes in $G$
and their successors or successors of successors or successors of successors
of successors and so forth with arbitrarily many repetitions
of ``successors of'' and all of the arrows and lines in the full graph
connecting these nodes.
\index{aster graph!for ``individual''}
(And it would make this definition a little shorter
if we allowed ourselves to use the word ``descendant'' in its graph-theoretic
sense.)

There are two reasons why the scare quotes.
\begin{itemize}
\item In life history
analysis, the graph for an ``individual'' ideally goes one or more times
around the life cycle (exactly).  Thus it may involve data not only for
one biological individual but also for its offspring and perhaps offspring
of offspring (if the experiment goes twice around the life cycle) or even
perhaps more remote descendants (where here ``descendants'' means real
biological descendants, not the graphical models idea of descendants).
\item If the value of the constant $y_j$ at the initial node of the
graph for an ``individual'' is greater than one, then the data for this
``individual'' is actually cumulative data for $y_j$ real biological
individuals and perhaps their real biological descendants.
\end{itemize}

%%%%%%%%%% NEED FORWARD REFERENCE to example graphs %%%%%%%%%%
%%%%%%%%%% maybe ?????

If one does not like our terminology of ``individual'' in scare quotes,
our advice is to just explain what data the graph is for.  It may actually
be for a biological individual, for a biological individual
and its offspring, or $n$ biological individuals.  Just say what it is.

Or we could use the characterization of Corollary~\ref{cor:markov} in
Appendix~\ref{app:markov}, which says the subgraphs for ``individuals''
are stochastically independent subvectors of the response vector.
\index{aster graph!for ``individual''}

In general, the subgraphs for ``individuals'' are the minimal stochastically
independent subvectors, in the sense that the data for an ``individual'' has
no stochastically independent parts.  But when limiting conditional models
%%%%%%%%%% NEED FORWARD REFERENCE to limiting conditional models %%%%%%%%%%
come into play, this is no longer the case.
Thus independence of data for ``individuals'' is an important property of
aster models, but it does not (in general) characterize
subgraphs for ``individuals.''

\section{The Other Predecessor Function}
\label{sec:other}

It is useful to have not only the set-to-index predecessor function $q$
defined in Section~\ref{sec:factorization} above but also the index-to-index
\index{predecessor function!set-to-index}
\index{predecessor function!index-to-index}
predecessor function $p$ defined as follows
$$
   p(j) = k \ifandonlyif j \in G \in \mathcal{G} \opand q(G) = k.
$$
Clearly, $q$ determines $p$.
The converse is not true because $p$ knows nothing about dependence groups.
But $p$ and $\mathcal{G}$ together determine $q$.

\section{The Transitive Closure of the Predecessor Relation}
\label{sec:closure}

The \emph{predecessor relation} on $N$ is the index-to-index predecessor
function $p$ thought
of as a relation, that is, thinking set-theoretically
\citep[Section~7]{halmos-set-theory}, as the set
$$
   \set{ (j, p(j)) : j \in J }
$$
of its argument-value pairs.

We need a notation from dynamical systems theory for repeated application
of a function.  If $f$ is any function whose domain and codomain are the same,
then it makes sense to compose $f$ with itself.  Then we let $f^0$ denote
the identity function on the domain of $f$, let $f^1 = f$, $f^2 = f \circ f$,
and, in general, $f^{n + 1} = f^n \circ f$.
So
\begin{align*}
   f^0(x) & = x
   \\
   f^1(x) & = f(x)
   \\
   f^2(x) & = f(f(x))
   \\
   f^3(x) & = f(f(f(x)))
\end{align*}
and so forth.

The transitive closure of the predecessor relation
is the smallest transitive relation $R$ containing it.
\index{predecessor relation!transitive closure}
As with most relations, we prefer denoting this relation by infix notation:
saying $j \succ k$ rather than $(j, k) \in R$, that is, $j \succ k$ means
$k = p^n(j)$ for some positive integer $n$.

\begin{theorem} \label{th:transitive-closure}
Under the conditions of Theorem~\ref{th:factorize},
the transitive closure of the predecessor relation is a strict partial order.
\end{theorem}
\begin{proof}
If $j \succ k$, then $j \in G$ for some $G \in \mathcal{G}$ and
$k = p^n(q(G))$ for some natural number $n$ ($n = 0$ is allowed).

If $k \in H$ for some $H \in \mathcal{G}$,
then we have $G < H$ in the total ordering
that Theorem~\ref{th:factorize} uses.
Hence we cannot also have $k \succ j$ because that would imply $G < H$
and $H < G$ contradicting $<$ being a strict total order.

If $k \notin H$ for any $H \in \mathcal{G}$ then $k$ has no predecessor
($k$ is initial) and we cannot have $m \succ k$ for any node $m$.

In either of the preceding cases we never have $k \succ j$ and $j \succ k$.
Since $\succ$ is a transitive relation by definition, it is
a strict partial order \citep[Section~14]{halmos-set-theory}
\end{proof}
\begin{corollary} \label{cor:compatible}
The transitive closure of the predecessor relation is compatible with
the total order on the family of dependence groups defined
in Theorem~\ref{th:factorize} in the sense that
$j \in G \in \mathcal{G}$ and $k \in H \in \mathcal{G}$ and $j \succ k$
implies $G < H$.
\end{corollary}

The non-strict counterpart of this relation
is the reflexive transitive closure of the predecessor relation,
\index{predecessor relation!reflexive transitive closure}
which is denoted $\succeq$.
We have $j \succeq k$ if and only if $j \succ k$ or $j = k$.

The inverse of a relation $R$ considered as a set of argument-value pairs
reverses the order in the pairs, that is $(k, j) \in R^{- 1}$ if and only
if $(j, k) \in R$.
As usual, we denote the inverse of a relation by turning its infix notation
around: $\prec$ is the inverse of $\succ$ and $\preceq$ is the inverse
of $\succeq$.

The inverse of the predecessor relation is the successor relation,
so $\prec$ is the transitive closure of the successor relation
and $\preceq$ is the reflexive transitive closure of the successor relation.
\index{successor relation!transitive closure}
\index{successor relation!reflexive transitive closure}
\index{transitive closure|seeunder{predecessor relation}}
\index{transitive closure|seeunder{successor relation}}
\index{reflexive closure|seeunder{predecessor relation}}
\index{reflexive closure|seeunder{successor relation}}

The choice of whether the transitive closure of the predecessor relation
is denoted $\succ$ or $\prec$ is arbitrary.  Either choice works so long
as one keeps straight which is which.  Our choice is influenced by an
arbitrary choice in the source code for R package \texttt{aster}.  When
the predecessor function is encoded (as the argument \texttt{pred} to the
R function \texttt{aster}) it is required that predecessors have lower indices
than successors (come before them in the \texttt{pred} vector).  Thus we
want to think of predecessors as ``less than'' successors in some sense.
Hence our decision to make $p(j) \prec j$.

In graphical model theory,
$\succ$ is called the ancestor relation,
$\prec$ the descendant relation,
$\succeq$ the ancestor-or-self relation, and
$\preceq$ the descendant-or-self relation.
But, as stated in Section~\ref{sec:graphical-terminology} above,
our policy is to avoid these terms
to avoid confusion in biological applications.
If we need words rather than symbols, we have to use the long winded ones:
``reflexive transitive closure of the predecessor relation'' and so forth.

\section{Predecessor is Sample Size}
\label{sec:piss}

All aster models have the \emph{predecessor is sample size} property.
This is a very important property that separates them from all other
graphical models.  There is a long history of models that have this
property.  Life table analysis and discrete time survival analysis have it.
So does Leslie matrix analysis \citep{caswell} and other methods
of estimation of population growth rate \citep{fisher,goodman,lenski-service}.
But none of those models were regression models, nor did they have
the generality of aster models in their graphical structure.
They do have the basic relationship of conditional and unconditional means
implied by this property (Section~\ref{sec:mu-and-xi} below)
but nothing else of aster model theory.

For one conditional distribution in the factorization \eqref{eq:factorize},
say for the conditional distribution of $y_G$ given $y_{q(G)}$,
\begin{itemize}
\item conditional on $y_{q(G)} = 0$, the distribution of $y_G$ is concentrated
    at zero (the zero vector having all components equal to zero), 
\item conditional on $y_{q(G)} = 1$, the distribution of $y_G$ is whatever
    this distribution is designated to be, and
\item conditional on $y_{q(G)} = n$ with the $n > 1$, the distribution
    of $y_G$ is the $n$-fold convolution of the distribution for sample
    \index{convolution}
    size one.
\end{itemize}
In short, the conditional distribution of $y_G$ given $y_q(G)$ is the
distribution of the sum of $y_{q(G)}$
independent and identically distributed (IID)
random vectors having whatever the distribution is for sample size one.
(By convention, a sum having zero terms is zero, and a sum having one term
is that term.)
Or, even shorter, the predecessor plays the role of sample size for this
conditional distribution.
Or, shorter still, \emph{predecessor is sample size}.
\index{aster model!property!predecessor is sample size}
\index{predecessor is sample size|seeunder{aster model}}

Note that we name families for dependence groups by the conditional
distribution for sample size one.
This is an unusual practice.  It is not the way families are named for
generalized linear models.
And it can seem unnecessarily mysterious at first sight.

All of our example graphs in Section~\ref{sec:graphs} below
have Bernoulli arrows.  For such an arrow
$$
\begin{CD}
   y_i @>\text{Ber}>> y_j
\end{CD}
$$
why not just say the conditional distribution of $y_j$ given $y_i$ is binomial
with sample size $y_i$ (because the sum of IID Bernoulli is binomial)?
For one thing,
it is not clear what sample size zero means without further explanation.
For another thing, for an arrow
$$
\begin{CD}
   y_i @>\text{0-Poi}>> y_j
\end{CD}
$$
the distribution of the sum of IID zero-truncated Poisson random variables
is not a ``brand name distribution.''  And its
probability mass function has no closed-form expression.
So we could not label this arrow with the name of the conditional distribution
of $y_j$ given $y_i$ because there is no such name.

One consequence of the predecessor-is-sample-size property is that $y_j$
that are predecessors (are at nonterminal nodes) must be
nonnegative-integer-valued random variables.
\index{predecessor random variable!nonnegative integer valued}
There is an exception to this requirement that will be discussed in
Section~\ref{sec:infinitely-divisible} below, but that exception has
never been used.

Another consequence of the predecessor-is-sample-size property is
the following section.

\section{Conditional and Unconditional Mean Values}
\label{sec:conditional-and-unconditional-mean-values}

\subsection{Unconditional}

Let $y$ be the response vector of an aster model.  We define a parameter
vector $\mu = E(Y)$.  This is the vector having components
\begin{equation} \label{eq:unconditional-mean-values}
   \mu_j = E(Y_j), \qquad j \in J,
\end{equation}
where, as usual, $J$ is the index set of the response vector (the set
of non-initial nodes of the full aster graph).

This is called the \emph{unconditional mean value parameter vector}.
\index{parameter vector!mean value|seeunder{aster model}}
\index{parameter vector!mean value|seeunder{exponential family}}
\index{aster model!mean value parameter!unconditional}
This name is getting us a little bit ahead of ourselves.
At this point, we don't even know these means exist.
(We will eventually find out they do exist.)
%%%%%%%%%% NEED FORWARD REFERENCE to regular full and moments %%%%%%%%%%
And, at this point, we don't know that means parameterize aster models,
since we haven't yet even completely specified what the distribution
of an aster model is.  We know the fundamental factorization
\eqref{eq:factorize}, and we know each of those factors obeys the
predecessor-is-sample-size property, but we don't yet know anything more.
(We will eventually find out means do parameterize aster models.)
%%%%%%%%%% NEED FORWARD REFERENCE to mean value parameterization %%%%%%%%%%

For now we will just assume these means exist.

\subsection{Conditional}

We define another parameter vector $\xi$ having components
\begin{equation} \label{eq:conditional-mean-values}
   \xi_j = E(Y_j \mid Y_{p(j)} = 1), \qquad j \in J,
\end{equation}
if this expression makes sense.  It will not make sense when the
conditioning event has probability zero (so the conditional expectation
can be defined arbitrarily).  In that case we have to use a different
definition that does not come with an equation.
The predecessor-is-sample-size property says that $y_j$ is the sum
of $y_{p(j)}$ IID random variables, and we say $\xi_j$ is the mean
of those random variables.

This is called the \emph{conditional mean value parameter vector}.
\index{aster model!mean value parameter!conditional}
As in the preceding section, this name is getting us a little bit ahead
of ourselves.
At this point, we don't even know these means exist.
(We will eventually find out they do exist.)
%%%%%%%%%% NEED FORWARD REFERENCE to regular full and moments %%%%%%%%%%
And, at this point, we don't know that means (conditional or unconditional)
parameterize aster models.  But the next section will show the unconditional
means determine conditional means and vice versa.  So if $\mu$ parameterizes,
then so does $\xi$, and vice versa.

\subsection{The Combination of the Two}
\label{sec:mu-and-xi}

It follows from the predecessor-is-sample-size property and linearity of
expectation that
\begin{equation} \label{eq:cond-exp}
   E(Y_j \mid y_{p(j)}) = \xi_j y_{p(j)}, \qquad j \in J.
\end{equation}
Then it follows from the iterated expectation axiom of conditional probability
$$
   E(Y_j)
   =
   E\{E(Y_j \mid Y_{p(j)})\}
$$
that
\begin{equation} \label{eq:mu-and-xi}
   \mu_j = \xi_j \mu_{p(j)}, \qquad j \in J.
\end{equation}
This is the fundamental recursive relation that shows (as we examine in
more detail presently) how $\mu$ is determined by $\xi$ and vice versa.

To map from $\xi$ to $\mu$ we use \eqref{eq:mu-and-xi} recursively
\begin{align*}
   \mu_j
   & =
   \xi_j \mu_{p(j)}
   \\
   & =
   \xi_j \xi_{p(j)} \mu_{p(p(j))}
   \\
   & =
   \xi_j \xi_{p(j)} \xi_{p(p(j))} \mu_{p(p(p(j)))}
\end{align*}
and so forth, with as many recursive applications as necessary.  In practice,
the computer traverses the graph in any order that visits predecessors before
successors using \eqref{eq:mu-and-xi} to determine $\mu_j$ as a function of
$\xi$ ($\mu_{p(j)}$ having already been determined when its node was visited
previously).  To get the recursion started, we need the mean values at initial
nodes, which are given by
\begin{equation} \label{eq:xi-mu-initial}
   \mu_j = y_j, \qquad j \in N \setminus J,
\end{equation}
because the mean value of a constant random variable is its constant value.

Using the reflexive transitive closure of the successor relation $\preceq$
we can rewrite the above as follows
\begin{equation} \label{eq:xi-mu-prod}
    \mu_j
    =
    \left( \prod_{\substack{i \in J \\ i \preceq j}} \xi_i \right)
    \left( \prod_{\substack{i \in N \setminus J \\ i \preceq j}} \mu_i \right),
    \qquad j \in J,
\end{equation}
where we note that the second product always has exactly one term: there
is always exactly one initial node $i$ such that $i \preceq j$.
We could also rewrite \eqref{eq:xi-mu-prod} as
\begin{equation} \label{eq:xi-mu-prod-too}
    \mu_j
    =
    \left( \prod_{\substack{i \in J \\ i \preceq j}} \xi_i \right)
    \left( \prod_{\substack{i \in N \setminus J \\ i \preceq j}} y_i \right),
    \qquad j \in J,
\end{equation}
by \eqref{eq:xi-mu-initial}.

To map from $\mu$ to $\xi$, rewrite \eqref{eq:mu-and-xi} as
\begin{equation} \label{eq:mu-to-xi}
   \xi_j = \frac{\mu_j}{\mu_{p(j)}}
\end{equation}
but for this to make sense, we must know that $\mu_{p(j)}$ is never zero.

We will eventually find out that $\mu_{p(j)}$ is never zero except in
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
limiting conditional models.  So we do not always have this property.
Thus \eqref{eq:mu-to-xi} makes sense when $\mu_{p(j)}$ is never zero,
but otherwise some components of $\xi$ are not determined by $\mu$.

Conversely, multiplication by zero is not a problem (unlike division by zero),
so \eqref{eq:xi-mu-prod} and \eqref{eq:xi-mu-prod-too} always determine
$\mu$ as a function of $\xi$.

Everything in this section up to this point is an elementary consequence
of the laws of conditional and unconditional expectation and the
predecessor-is-sample-size property.  Consequently,
everything in this section up to this point is also true of all previous
models in survival analysis and demography that have also had this property
cited in Sections~\ref{sec:background} and~\ref{sec:piss} above.

\subsection{Confession}

\Citet{aster1} did not define $\xi$
the way we do here.  Instead they used that Greek letter to denote
\eqref{eq:cond-exp}.
A referee said this definition is dumb.  It makes $\xi$ a function
of both random variables and parameters, so it is not a parameter,
and one shouldn't use Greek letters for things that aren't parameters.
We didn't listen then and managed to get the
paper published overriding this objection.  But now we agree with the referee.

The vector $\xi$ as defined here is an important parameterization of
aster models \citep[this has been realized since][]{aster-philosophical}.

R package \texttt{aster} used the same dumb definition until version
1.0-2 of the package, when a new optional argument \code{is.always.parameter}
was added to the method of R generic function \code{predict} that handles
aster model objects.  And, for reasons of backward compatibility,
the dumb definition is still the default.
One must use the optional argument \code{is.always.parameter = TRUE}
to estimate $\xi$ as defined in this section.

R package \texttt{aster2} and recent papers and technical reports use
the definition presented here (the conditional mean value parameter vector
is $\xi$ if they mention conditional mean value parameters at all).

\section{Some Aster Graphs}
\label{sec:graphs}

The first published aster model \citep{aster1} had this graph
\begin{equation} \label{gr:aster1}
\begin{CD}
   1
   @>\text{Ber}>>
   y_1
   @>\text{Ber}>>
   y_2
   @>\text{Ber}>>
   y_3
   \\
   @.
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   \\
   \hphantom{1}
   @.
   y_4
   @.
   y_5
   @.
   y_6
   \\
   @.
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
  \\
   \hphantom{1}
   @.
   y_7
   @.
   y_8
   @.
   y_9
\end{CD}
\end{equation}
\index{arrow!Bernoulli}
\index{arrow!zero-truncated Poisson}
which is for one individual.  There are 570 individuals in the data set,
which is included in the R package \texttt{aster}.  So one can think of the
full aster graph as 570 copies of this graph with the subscripts changed
so the nodes (the $y_j$) are all different.

Because this graph has only arrows, no lines, each node is
a dependence group all by itself.

The individuals are plants of the species \emph{Echinacea angustifolia},
\index{Echinacea angustifolia@\emph{Echinacea angustifolia}}
whose common name is narrow-leaved
purple coneflower.  These data were collected by the Echinacea Project
\index{Echinacea project}
(\url{http://echinaceaproject.org/}), a long-running project funded by
the National Science Foundation (the co-PI's are the second and third authors
of \citet{aster1}).  The way \eqref{gr:aster1} is laid out,
variables in the first column ($y_1$, $y_4$, and $y_7$) are for 2002,
those in the second column are for 2003,
those in the third column are for 2004,
those in the first row ($y_1$, $y_2$, and $y_3$) measure survival
(0 = dead, 1 = alive),
those in the second row indicate flowering
(0 = no flowers, 1 = some flowers),
those in the third row are flower head counts
(actual number of flower heads).

Of course,
the ``rows'' and ``columns'' are not part of the graphical structure.
The only thing that matters is which nodes are connected by which arrows.

Aster graphs can get a lot bigger than \eqref{gr:aster1}.
The Echinacea Project now has data for years since 2004 (which extends
the graph with many more ``columns'') and data for more life history
stages (which extends the graph with more ``rows'').

The node labels (the $y_j$) are random variables, components of the response
vector.  The arrows indicate conditional distributions.
An arrow
\begin{equation} \label{gr:one-arrow-conditional}
\begin{CD}
   y_i @>>> y_j
\end{CD}
\end{equation}
indicates the conditional distribution of $y_j$ given $y_i$.
An arrow
\begin{equation} \label{gr:one-arrow-marginal}
\begin{CD}
   1 @>>> y_i
\end{CD}
\end{equation}
indicates the marginal distribution of $y_i$,
because conditioning on a constant random variable is the same as not
conditioning.

Labels on the arrows name the distribution.
Ber is for Bernoulli (any zero-or-one-valued random variable), and
0-Poi is for zero-truncated Poisson (Poisson conditioned on being nonzero).
\index{Poisson distribution!zero-truncated}
This explanation of arrows and their distributions is incomplete and will
be picked up again in Section~\ref{sec:piss}.
%%%%%%%%%% NEED FORWARD REFERENCE to exponential family assumption %%%%%%%%%%

Here is a more complicated aster graph from \citet{aster3}
\begin{equation} \label{gr:aster3}
\begin{CD}
   1
   @>\text{Ber}>>
   y_1
   @>\text{Ber}>>
   y_2
   @>\text{Ber}>>
   y_3
   @>\text{Ber}>>
   y_4
   \\
   @.
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   \\
   \hphantom{1}
   @.
   y_5
   @.
   y_6
   @.
   y_7
   @.
   y_8
   \\
   @.
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   \\
   \hphantom{1}
   @.
   y_9
   @.
   y_{10}
   @.
   y_{11}
   @.
   y_{12}
   \\
   @.
   @VV\text{Poi}V
   @VV\text{Poi}V
   @VV\text{Poi}V
   @VV\text{Poi}V
   \\
   \hphantom{1}
   @.
   y_{13}
   @.
   y_{14}
   @.
   y_{15}
   @.
   y_{16}
   \\
   @.
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   \\
   \hphantom{1}
   @.
   y_{17}
   @.
   y_{18}
   @.
   y_{19}
   @.
   y_{20}
\end{CD}
\end{equation}
\index{arrow!Bernoulli}
\index{arrow!zero-truncated Poisson}
\index{arrow!Poisson}

Again, because this graph has only arrows, no lines, each node is
a dependence group all by itself.  The label Poi on arrows indicates
the Poisson distribution.

This graph is for simulated data, which \citet{aster3} used because
at the time no data for aster models as complicated as \eqref{gr:aster3} had
been collected by biologists, and it was important to give such
an illustration of the possibilities of aster models.
Like in \eqref{gr:aster1} the ``columns'' in \eqref{gr:aster3} are for
data in successive years.  The first three ``rows'' of \eqref{gr:aster3}
can be taken to be the same as those of \eqref{gr:aster1}: survival,
flowering indicator variables, and flower counts.  The fourth row of
\eqref{gr:aster3} is seed counts, and the fifth row is number of seeds
that germinate (produce new plants).  Of course, since the data
are simulated, the story about these variables is just a story.
It could be told differently, and \citet{aster3} do have a story
where the same graph could be for data about an animal rather than a plant.

This graph did serve as a good example of what was possible.
\citet*[in the on-line appendix]{stanton-geddes-tiffin-shaw}
discuss an aster model with seven life history stages (one of which is
artificial, modeling random sampling in the data collection process, hence
only six are about life history of the organisms) and thus would be
like the graph \eqref{gr:aster3} except with seven ``rows.''
Because the organism in question (\emph{Chamaecrista fasciculata}, common
name partridge pea) was an annual plant, there is only one ``column.''
(As mentioned above, ``rows'' and ``columns'' are not part of the graphical
structure --- the only thing that matters is which nodes are connected
by which arrows and lines (if any) ---
and \citeauthor{stanton-geddes-tiffin-shaw} actually
laid out their graph in a row.)

Statistically, there are some important differences between these graphs.
Graph \eqref{gr:aster3} has both Poisson (Poi) and zero-truncated Poisson
(0-Poi) arrows and hence illustrates when to use which
(see also Section~\ref{sec:zero-inflated} below).
In graph \eqref{gr:aster1} every predecessor node is Bernoulli,
but in graph \eqref{gr:aster3} $y_{13}$ through $y_{16}$ are non-Bernoulli
predecessor nodes.  So \eqref{gr:aster3} shows that predecessor values can
be any nonnegative integer.

The graph \eqref{gr:multi} comes from a still unpublished manuscript for a book
about aster models.  It was the first graph for a model for an animal having
life history stages like an insect's larva, pupa, and adult.  We present this
graph for hypothetical data even though a similar model has been fit to real
data by \citet{aster-hornworm}.  Those data are for the tobacco hornworm
\emph{Manducca sexta}, which is an insect (a moth) that does have these life
history stages.  These data were not collected with the intention of using
an aster model (which were very new when the experiment was done) and so were
not ideal for aster analysis.  Although an aster analysis was done by
\citet{aster-hornworm}, it does not serve as quite as clean an example as
the graph \eqref{gr:multi}.

\begin{equation} \label{gr:multi}
\begin{tikzcd}
  \hphantom{1} & y_1 & y_4 & y_7 & y_{10}
  \\
  1
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_2
  \arrow[dash]{u}
  \arrow[dash]{d}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_5
  \arrow[dash]{u}
  \arrow[dash]{d}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_8
  \arrow[dash]{u}
  \arrow[dash]{d}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_{11}
  \arrow[dash]{u}
  \arrow[dash]{d}
  \\
  \hphantom{1} & y_3 \arrow{d}{\text{Ber}} & y_6 \arrow{d}{\text{Ber}}
  & y_9 \arrow{d}{\text{Ber}} & y_{12} \arrow{d}{\text{Ber}}
  \\
  \hphantom{1} & y_{13} \arrow{d}{\text{0-Poi}}
  & y_{14} \arrow{d}{\text{0-Poi}}
  & y_{15} \arrow{d}{\text{0-Poi}} & y_{16} \arrow{d}{\text{0-Poi}}
  \\
  \hphantom{1} & y_{17} & y_{18} & y_{19} & y_{20}
\end{tikzcd}
\end{equation}
\index{arrow!Bernoulli}
\index{arrow!zero-truncated Poisson}
\index{dependence group!multinomial}

As always, the constant 1 at the initial node
of the graph indicates that the graph is for one individual.
In addition to the notations Ber = Bernoulli
and 0-Poi = zero-truncated Poisson, which we have already met,
we now also have $\mathcal{M}$ = multinomial.
Lines without arrowheads are ``lines'' connecting nodes in the
same dependence group.  Hence the dependence groups containing more than
one node are $\{1, 2, 3\}$, $\{4, 5, 6\}$, $\{7, 8, 9\}$, and $\{10, 11, 12\}$.
Other nodes are dependence groups all by themselves; $\{j\}$ is a dependence
group for $j \ge 13$.

As stated in Section~\ref{sec:exception-dependence-group-lines} above,
this graph does not follow the theory, which requires a line connecting
each pair of distinct elements of each dependence group and which would
require us to add lines $y_1 \myline y_3$ and $y_4 \myline y_6$
and $y_7 \myline y_9$ and $y_{10} \myline y_{12}$ to the graph.
But, also as stated in Section~\ref{sec:exception-dependence-group-lines}
above, we don't need these lines to infer the dependence groups from the
graph.  The lines $y_1 \myline y_2$ and $y_2 \myline y_3$ that are in the
graph say $y_1$ and $y_3$ are in the same dependence group as $y_2$,
and since there are no other lines to these nodes, they must constitute
a dependence group.  Since there doesn't seem to be any room in
the picture \eqref{gr:multi} for these additional lines required by theory,
we omit them.

Each of the multi-node dependence groups has a conditional multinomial
distribution with, as usual, predecessor as sample size.  Since each
predecessor is zero-or-one-valued, if a predecessor (say $y_2$) is
equal to one, then exactly one of its three successor nodes ($y_4$, $y_5$,
and $y_6$) is equal to one, and, if this predecessor
is equal to zero, then all of its three successor nodes are also equal to zero.
In effect, exactly one of the ``exterior nodes'' of this group of switches
($y_1$, $y_4$, $y_7$, $y_{10}$, $y_{11}$, $y_{12}$, $y_9$, $y_6$, and $y_3$)
is equal to one.  There is one path taken by any particular individual,
from the initial node (marked 1) through these four multinomial dependence
groups.

The intended application for this graph \citep{aster-hornworm} is life history
data for an insect.  As in our graphs without dependence groups,
``columns'' of the graph are for different
times (here days, there years).
Nodes in the top ``row'' of this graph ($y_1$, $y_4$, $y_7$, and $y_{10}$)
indicate death.
Nodes in the second ``row'' of this graph ($y_2$, $y_5$, $y_8$, and $y_{11}$)
indicate the individual is a larva (caterpillar).
Nodes in the third ``row'' of this graph ($y_3$, $y_4$, $y_9$, and $y_{12}$)
indicate the individual is an adult (moth, with wings, flying around trying
to mate).
Nodes in the fourth ``row'' of this graph ($y_{13}$ through $y_{16}$)
indicate the mating success.
Nodes in the bottom ``row'' of this graph ($y_{17}$ through $y_{20}$)
count number of eggs laid.  So this graph is for female individuals.
In \citet{aster-hornworm} the same graph with only the multinomial dependence
groups (nodes 1 through $y_{12}$) is used for male individuals because
the sex of individuals was not determined before they reached adulthood.

So this graph illustrates two important points not seen before.
It is not necessary for every individual to
have the same graph (here females and males have different graphs).
And we have non-singleton dependence groups,
multinomial ``switches'' between different life history stages.

Here is yet another graph illustrating normal dependence groups.
\begin{equation} \label{gr:normal}
\begin{tikzcd}
   1
   \arrow{r}{\text{Ber}}
   & y_1
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \arrow{r}{\text{Ber}}
   & y_2
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \arrow{r}{\text{Ber}}
   & y_3
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \arrow{r}{\text{Ber}}
   & y_4
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \\
   \hphantom{1}
   & y_5 \arrow[dash]{d}
   & y_6 \arrow[dash]{d}
   & y_7 \arrow[dash]{d}
   & y_8 \arrow[dash]{d}
   \\
   \hphantom{1} & y_9 & y_{10} & y_{11} & y_{12}
\end{tikzcd}
\end{equation}
\index{arrow!Bernoulli}
\index{dependence group!normal}
\index{dependence group|seealso{arrow}}
Here the top ``row'' indicates survival.  And the next two rows are
for normally distributed something or other given survival.  Here we
model the normal as two-node dependence groups
$\{ 5, 9 \}$,
$\{ 6, 10 \}$.
$\{ 7, 11 \}$, and
$\{ 8, 12 \}$ because
we do not want to assume variance is known.  As we shall see, this permits
but does not require, modeling variance as a function of covariates.

We see that the aster formalism suggests new possibilities.  In order to
have a two-parameter normal distribution, we need two-node dependence groups.
%%%%%%%%%% NEED FORWARD REFERENCE to two-parameter normal %%%%%%%%%%
%%%%%%%%%% NEED FORWARD REFERENCE to one-parameter normal %%%%%%%%%%

\subsection{Zero-Inflated Poisson}
\label{sec:zero-inflated}

Readers may have wondered why the graph \eqref{gr:aster1}
has its middle ``row.''  The variables $y_4$, $y_5$, and $y_6$ are a function
of the variables $y_7$, $y_8$, and $y_9$, respectively,
($y_j = 1$ if and only if $y_{j + 3} > 0$, $j = 4,$ 5, 6).
So why were these variables inserted in the graph?

Consider just the subgraph
\begin{equation} \label{gr:zero-inflated-poisson}
\begin{CD}
   y_1 @>\text{Ber}>> y_4 @>\text{0-Poi}>> y_7
\end{CD}
\end{equation}
The conditional distribution of $y_7$ given $y_1$ (both arrows combined)
is zero-inflated Poisson \citep{lambert}.
\index{Poisson distribution!zero-inflated}
Since $y_7 = 0$ if and only if $y_4 = 0$, and the probability of this event
can be anything
(because the Bernoulli arrow does not have any restrictions on its parameter),
it is, strictly speaking, zero-inflated-or-deflated Poisson, but we will not
be this fussy about this bit of terminology.

So having arrows arranged like this is just the aster way of getting
zero-inflated Poisson random variables into the model.

Why can we not just have zero-inflated Poisson arrows?  Because
zero-inflated Poisson is not an exponential family.  It can be factored
into a product of two exponential families, like
\eqref{gr:zero-inflated-poisson} does.  But it is not itself exponential
family.  Hence we cannot have arrows like that in an aster model
because of the exponential family assumption (Section~\ref{sec:aster-expfam}
below).  If we want zero-inflated Poisson or zero-inflated negative binomial
%%%%%%%%%% NEED FORWARD REFERENCE to zero-truncated negative binomial %%%%%%%%%%
in aster models, then this is the way we have to do it.

Although we have zero-inflated Poisson distributions in aster models,
we do not give them their usual parameterization \citep{lambert}.
No aster model parameterization
%%%%%%%%%% NEED FORWARD REFERENCE to plethora of parameters %%%%%%%%%%
is this usual parameterization.

\subsection{On Not Overusing Zero-Truncated Poisson}

The zero-truncated Poisson distribution is for when \emph{by definition
of the model} a count of zero cannot occur unless the predecessor is zero.
It is not for the case where expected count is so high
that zeros occur infrequently.
It is for the case where zeros are impossible (except when predecessors
are zero).

Some users have been confused about this, using zero-truncated distributions
where they are not appropriate.

\subsection{Not Really Missing Data}
\label{sec:miss}

The property that predecessor zero implies successor zero
(because a sum having zero terms is zero by convention,
Section~\ref{sec:piss} above)
takes care of what people formerly conceived of as a missing data problem:
when $y_{p(j)} = 0$ (for concreteness, say this means the ``individual''
is dead), you cannot ``observe'' $y_j$.

Nevertheless we can infer $y_j = 0$
(if $y_j$ is flower count, then we are inferring that dead plants have
no flowers; if $y_j$ is survival for the following year, then we are
inferring that dead plants stay dead).

So that is not a true missing data problem from the aster model perspective.
Researchers do need to be aware of the need to code their data this way
(dead individuals have 0 flowers not \code{NA} flowers, \code{NA} being
the R value for missing data).

Many researchers have been confused about this when first introduced to
aster models.  If different individuals live different numbers of years,
don't we need different graphs for individuals to reflect this?  No.
We just have a long enough graph to accommodate all life spans.  After
the individual is dead the data are just zero (not \code{NA}).

If we did have truly missing data (not observable and not inferable),
then we would have a problem that aster models are not equipped to solve.

R function \code{aster} in R package \code{aster} does not allow \code{NA}
values in the data it analyzes.  Its help page says
\begin{quotation}
     It was almost always wrong for aster model data to have \code{NA}
     values. Although theoretically possible for the R formula
     mini-language to do the right thing for an aster model with \code{NA}
     values in the data, usually it does some wrong thing.  Thus, since
     version 0.8-20, this function and the \code{reaster} function give
     errors when used with data having \code{NA} values.  Users must remove
     all \code{NA} values (or replace them with what they should be, perhaps
     zero values) ``by hand''.
\end{quotation}
R function \code{asterdata} in R package \code{aster2} also does not
allow \code{NA} values in aster data for analyses done by that package.

As we shall see (Section~\ref{sec:aster-expfam} below)
the property that predecessor zero implies successor zero
makes likelihood inference automatically do the Right Thing.
We do not have to go into contortions to get it to do the Right Thing.
It just does the Right Thing automatically.

\subsection{Bernoulli versus Multinomial}

Bernoulli arrows and multinomial dependence groups are closely related but
work differently.  Bernoulli is related to multinomial but different.

In \eqref{gr:multi} every arrow labeled with an $\mathcal{M}$ is Bernoulli
marginally, but the whole point of dependence groups is that the components
of the response vector in a dependence group are not
conditionally independent given their predecessors, unlike the Bernoulli
arrows labeled Ber in \eqref{gr:multi} or in any of the graphs in this
section (by Lemma~\ref{lem:markov}).

Conversely, if
$$
\begin{CD}
   y_i @>\text{Ber}>> y_j
\end{CD}
$$
is a Bernoulli arrow, then we could replace this with a multinomial dependence
group that does the same thing
$$
\begin{tikzcd}
  \hphantom{1} & y_k
  \\
  y_i
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_j
  \arrow[dash]{u}
\end{tikzcd}
$$
where $k$ is some index that hasn't been used in the rest of the graph.

For example, we could change graph \eqref{gr:aster1} to
\begin{equation} \label{gr:aster1-too}
\begin{tikzcd}
  \hphantom{1} & y_{10} & y_{11} & y_{12}
  \\
  1
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_1
  \arrow[dash]{u}
  \arrow{d}{\text{Ber}}
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_2
  \arrow[dash]{u}
  \arrow{d}{\text{Ber}}
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_3
  \arrow[dash]{u}
  \arrow{d}{\text{Ber}}
  \\
  \hphantom{1} & y_4 \arrow{d}{\text{0-Poi}}
  & y_5 \arrow{d}{\text{0-Poi}}
  & y_6 \arrow{d}{\text{0-Poi}}
  \\
  \hphantom{1} & y_7 & y_8 & y_9
\end{tikzcd}
\end{equation}
and all of the components of the response vector that have the same indices
would have the same interpretation and the same values in the same data
in both graphs \eqref{gr:aster1} and \eqref{gr:aster1-too}.
Then the additional nodes $y_{10}$, $y_{11}$, and $y_{12}$ are determined
by the properties of the multinomial distribution
and the predecessor-is-sample size property (Section~\ref{sec:piss} above)
\begin{equation} \label{eq:bernoulli-multinomial-constraints}
\begin{split}
   y_{10} & = 1 - y_1
   \\
   y_{11} & = y_1 (1 - y_2)
   \\
   y_{12} & = y_2 (1 - y_3)
\end{split}
\end{equation}
The aster models with graphs \eqref{gr:aster1} and \eqref{gr:aster1-too}
can be made equivalent if parameterized to make that so (I think, no proof
%%%%%%%%%% NEED FORWARD REFERENCE to directions of constancy %%%%%%%%%%
here), but they don't have to be equivalent (I think).
Hence scientists have to decide which to use.  The argument of simplicity
argues for \eqref{gr:aster1} (no dependence groups).
But arguments can be made both ways.

Not only can we replace Bernoulli arrows with multinomial dependence groups,
we can, at least partially replace multinomial dependence groups with Bernoulli
arrows.  We could replace the graph \eqref{gr:multi} with the graph
shown in Figure~\ref{fig:gr:multi-too} (p.~\pageref{fig:gr:multi-too}).
\begin{sidewaysfigure}
\begin{equation*}
\begin{tikzcd}
  % \hphantom{1} & y_1 & y_4 & y_7 & y_{10}
  % \\
  1
  \arrow{r}{\text{Ber}}
  & y_1
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_2
  \arrow{r}{\text{Ber}}
  \arrow[dash]{d}
  & y_4
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_5
  \arrow[dash]{d}
  \arrow{r}{\text{Ber}}
  & y_7
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_8
  \arrow[dash]{d}
  \arrow{r}{\text{Ber}}
  & y_{10}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_{11}
  \arrow[dash]{d}
  \\
  \hphantom{1} & \hphantom{y_1}
  & y_3 \arrow{d}{\text{Ber}}
  & \hphantom{y_4}
  & y_6 \arrow{d}{\text{Ber}}
  & \hphantom{y_7}
  & y_9 \arrow{d}{\text{Ber}}
  & \hphantom{y_{10}}
  & y_{12} \arrow{d}{\text{Ber}}
  \\
  \hphantom{1} & \hphantom{y_1}
  & y_{13} \arrow{d}{\text{0-Poi}}
  & \hphantom{y_4}
  & y_{14} \arrow{d}{\text{0-Poi}}
  & \hphantom{y_7}
  & y_{15} \arrow{d}{\text{0-Poi}}
  & \hphantom{y_{10}}
  & y_{16} \arrow{d}{\text{0-Poi}}
  \\
  \hphantom{1} & \hphantom{y_1}
  & y_{17} 
  & \hphantom{y_4}
  & y_{18}
  & \hphantom{y_7}
  & y_{19}
  & \hphantom{y_{10}}
  & y_{20}
\end{tikzcd}
\end{equation*}
\caption{Alternative to Graph \eqref{gr:multi}.}
\label{fig:gr:multi-too}
\end{sidewaysfigure}
In graph \eqref{gr:multi} and the graph in Figure~\ref{fig:gr:multi-too}
all of the components of the response vector that have the same indices
have the same interpretation and the same values in the same data \emph{except}
for $y_1$,  $y_4$,  $y_7$, and $y_{10}$ which have the \emph{opposite}
interpretation and opposite values in the two graphs.
In graph \eqref{gr:multi} the value one
for $y_1$,  $y_4$,  $y_7$, and $y_{10}$ means \emph{dead}.
In the graph in Figure~\ref{fig:gr:multi-too} the value one
for these variables means \emph{alive}.

The aster models with the graph \eqref{gr:multi} and the graph
in Figure~\ref{fig:gr:multi-too} cannot (I think) be made equivalent
by some choice of parameterization.
Hence scientists have to decide which to use.  Now the argument of simplicity
seems to argue for \eqref{gr:multi}.  In either case we have dependence groups
(we must have to track life history stages).  So as long as we have to have
dependence groups, we might as well make them do as much work as possible,
tracking mortality as well as progress through life history stages.
But arguments can be made both ways.

\subsection{Bernoulli versus Bernoulli}

Any composition of Bernoulli arrows is another Bernoulli.  In the following
graph
\begin{equation} \label{gr:one-bernoulli-after-another}
\begin{CD}
   y_1 @>\text{Ber}>> y_2 @>\text{Ber}>> y_3 @>\text{Ber}>> y_4
\end{CD}
\end{equation}
the conditional distribution of $y_4$ given $y_1 = 1$ is Bernoulli.
If $y_1 = 1$, then $y_2$ is either zero or one, hence so is $y_3$,
hence so is $y_4$, and any zero-or-one-valued random variable is Bernoulli.

Hence, we can replace the subgraph \eqref{gr:one-bernoulli-after-another}
by the single arrow
\begin{equation} \label{gr:one-bernoulli}
\begin{CD}
   y_1 @>\text{Ber}>> y_4
\end{CD}
\end{equation}
(deleting the components $y_2$ and $y_3$ from the response vector).

And this change-of-data gives a model in which the conditional distribution
of $y_4$ given $y_1 = 1$ is Bernoulli in either case.

But when these are subgraphs of a larger graph,
the corresponding aster models 
cannot (I think) be made equivalent
by some choice of parameterization.
Hence scientists have to decide which to use.

Of course, had \eqref{gr:one-bernoulli-after-another} been different
\begin{equation*}
\begin{CD}
   y_1 @>\text{Ber}>> y_2 @>\text{Ber}>> y_3 @>\text{Ber}>> y_4
   \\
   @.
   @VVV
   \\
   \hphantom{y_1}
   @.
   \vdots
\end{CD}
\end{equation*}
so $y_2$ had a successor other than $y_3$ (or similarly for $y_3$)
then $y_2$ and $y_3$ could not have been removed from the data.

The observation of this section is only for when we have a straight line
of Bernoullis like \eqref{gr:one-bernoulli-after-another} with no branches.
But this case is seen surprisingly often in published aster analyses.
Again, scientists have to decide which to use.

\subsection{Thinned Poisson}

A Poisson followed by a Bernoulli is Poisson (this is well known in spatial
statistics).  In the graph
\begin{equation} \label{gr:poisson-bernoulli}
\begin{CD}
   y_1 @>\text{Poi}>> y_2 @>\text{Ber}>> y_3
\end{CD}
\end{equation}
the conditional distribution of $y_3$ given $y_1 = 1$ is Poisson.  Thus this
graph can be replaced by the single arrow
\begin{equation} \label{gr:poisson}
\begin{CD}
   y_1 @>\text{Poi}>> y_3
\end{CD}
\end{equation}
(deleting the component $y_2$ from the response vector),

And this change-of-data gives a model in which the conditional distribution
of $y_4$ given $y_1 = 1$ is Poisson in either case.
Again, scientists have to decide which to use.

There are, as far as I can see no general principles for which aster graph
is the one and only Right Thing for any particular data.  Multiple different
aster models may have some rationale supporting them.

\section{Exponential Families of Distributions}

This is a brief overview of the theory
of exponential families of distributions,
just enough to allow us to finish the basic theory of aster models.
We will do some more exponential family theory later as the need arises.

\subsection{Definition}
\label{sec:define-expfam}

The usual definition of exponential families of distributions
(\citealp[Chapter~8]{barndorff-nielsen}; \citealp[Chapter~1]{brown})
involves probability mass-density functions (or measure-theoretic probability
density functions with respect to an arbitrary positive measure).
Here we give a simpler definition from \citet{geyer-gdor}.

A statistical model is a family of probability distributions.
A statistical model is an \emph{exponential family of distributions} if it has
\index{exponential family}
a log likelihood of the form
\begin{equation} \label{eq:logl-expfam}
   l(\theta) = \inner{y, \theta} - c(\theta)
\end{equation}
\index{exponential family!log likelihood}
\index{log likelihood|seeunder{exponential family}}
where
\begin{itemize}
\item $y$ is a vector-valued statistic, which is called
   the \emph{canonical statistic},
\index{exponential family!canonical statistic}
\index{canonical statistic|seeunder{exponential family}}
\item $\theta$ is a vector-valued parameter, which is called
   the \emph{canonical parameter},
\index{exponential family!canonical parameter}
\index{canonical parameter|seeunder{exponential family}}
\index{parameter vector!canonical|seeunder{aster model}}
\index{parameter vector!canonical|seeunder{exponential family}}
\item $c$ is a real-valued function, which is called
   the \emph{cumulant function}, and
\index{exponential family!cumulant function}
\index{cumulant function|seeunder{exponential family}}
\item $\inner{\fatdot, \fatdot}$ is a bilinear form that places
   the vector space where $y$ takes values and the vector
   space where $\theta$ takes values in duality.
\index{dual vector spaces}
\end{itemize}

In equation \eqref{eq:logl-expfam} we have used the rule that additive terms
in the log likelihood that do not contain the parameter may be dropped.
Such terms have been dropped in \eqref{eq:logl-expfam}.

In aster model theory, we always have
$$
   \inner{y, \theta} = \sum_{j \in J} y_j \theta_j
$$
where $J$ is the common index set for $y$ and $\theta$
so both are vectors in $\real^J$.  But the angle brackets notation,
which comes from functional analysis \citep{rudin}, is used to indicate
that we don't think of the the vector space where $y$ takes values
and the vector space where $\theta$ takes values as being the same.
They are dual vector spaces, not the same.

This means that $\inner{y, y}$ or $\inner{\theta, \theta}$ are
glaring errors, unlike what would be the case if we said that
$\inner{\fatdot, \fatdot}$ was an inner product on $\real^J$ or some such.
In any event, your humble author has been using this notation since his
thesis \citep{geyer-thesis} and is not going to stop now.
Moreover, most aster papers also use this notation (if they discuss
aster theory at all).

Although we usually say ``the'' canonical statistic,
``the'' canonical parameter, and ``the'' cumulant function,
these are not uniquely defined:
\begin{itemize}
\item any one-to-one affine function of a canonical statistic vector is another
   canonical statistic vector,
\index{affine function|(}
\index{exponential family!canonical statistic}
\item any one-to-one affine function of a canonical parameter vector is another
   canonical parameter vector, and
\index{exponential family!canonical parameter}
\item any real-valued affine function plus a cumulant function is another
   cumulant function.
\index{exponential family!cumulant function}
\end{itemize}
\index{affine function|)}
(Affine functions are defined in Section~\ref{sec:canonical-affine-submodel}
below.)
These possible changes of statistic, parameter, or cumulant function are not
algebraically independent.  Changes to one may require changes to the others
to keep a log likelihood of the form \eqref{eq:logl-expfam} above.

Usually no fuss is made about this nonuniqueness.  One fixes a choice
of canonical statistic, canonical parameter, and cumulant function
and leaves it at that.

The cumulant function may not be defined by \eqref{eq:logl-expfam}
on the whole vector space where $\theta$ takes values.  In that case
it can be extended to this whole vector space by
\begin{equation} \label{eq:cumfun-expfam}
   c(\theta) = c(\psi) + \log\left\{
   E_\psi\bigl( e^{\inner{Y, \theta - \psi}} \bigr) \right\}
\end{equation}
\index{exponential family!cumulant function}
where $\theta$ varies while $\psi$ is fixed at
a possible value of the canonical parameter vector, and
the expectation and hence $c(\theta)$ are assigned
the value $\infty$ for $\theta$ such that the expectation does not exist.

The family is \emph{full} if its canonical parameter space is
\begin{equation} \label{eq:full-expfam}
   \Theta = \{\, \theta : c(\theta) < \infty \,\}
\end{equation}
\index{exponential family!full}
\index{exponential family!canonical parameter!space}
and a full family is \emph{regular} if its canonical parameter space is an open
\index{exponential family!regular}
subset of the vector space where $\theta$ takes values.

Almost all exponential families used in real applications are full and regular.
So-called \emph{curved exponential families} (smooth non-affine submodels of
\index{exponential family!curved}
full exponential families) are not full.
Constrained exponential families \citep{geyer-constrained} are not full.
A few exponential families used in spatial statistics are full but not regular
\citep{geyer-moller}.

Many people use ``natural'' everywhere this book uses ``canonical.''
In this we are following \citet{barndorff-nielsen}.
It also goes with our policy of avoiding terminology used in biology
if alternatives are available.

Many people also use an older terminology that says a statistical model
is \emph{in the} exponential family, where we say a statistical model
is \emph{an} exponential family.
Thus the older terminology says \emph{the} exponential
family is the collection of all of what the newer terminology calls
exponential families.  The older terminology names a useless
mathematical object, a heterogeneous collection of statistical models not
used in any application.
The newer terminology names an important property of statistical models.
If a statistical model is a regular full exponential family, then it
has all of the properties discussed here.
If a statistical model is an exponential family (not necessarily full or
regular), then it has many of the properties discussed here.
Presumably, that is the reason for the newer terminology.
In this we are again following \citet{barndorff-nielsen}.

\subsection{Independent and Identically Distributed}
\label{sec:iid}

\index{exponential family!independent and identically distributed|(}
Suppose we have an exponential family with vector canonical statistic $z$,
vector canonical parameter $\theta$, and log likelihood
$$
   l(\theta) = \inner{z, \theta} - c(\theta)
$$
And suppose we have an IID sample
from this family with log likelihood
\begin{equation} \label{eq:iid}
\begin{split}
   l_n(\theta)
   & =
   \sum_{i = 1}^n \left[ \inner{z_i, \theta} - c(\theta) \right]
   \\
   & =
   \left\langle \sum_{i = 1}^n z_i, \theta \right\rangle - n c(\theta)
\end{split}
\end{equation}
This tells us that IID sampling from an exponential family gives another
exponential family with
\begin{itemize}
\item canonical statistic vector that is the sum of the canonical statistic
    vectors for the elements of the sample,
\item the same canonical parameter vector, and
\item cumulant function that is $n$ times
    the cumulant function for the elements of the sample.
\end{itemize}

A lot of ``addition rules'' for ``brand name distributions''
are special cases.  For example, sum of IID Bernoulli is binomial, sum of
IID Poisson is Poisson, sum of IID exponential is gamma.

The simple math in this section is important for aster models.
The predecessor-is-sample-size property (Section~\ref{sec:piss} above)
requires us to know
$n$-fold convolutions of the distributions associated with arrows at least
well enough to write down the log likelihood.  This section tells how that
is done.
\index{exponential family!independent and identically distributed|)}

\subsection{Canonical Affine Submodels}
\label{sec:canonical-affine-submodel}

\index{canonical affine submodel|seeunder{exponential family}}
\index{exponential family!canonical affine submodel|(}
In this section we consider \emph{canonical affine submodels} of exponential
families.  If $\theta$ is the canonical parameter vector, then these
submodel parameterizations have the form
\begin{equation} \label{eq:affine}
   \theta = a + M \beta,
\end{equation}
where $a$ is a known vector and $M$ is a known matrix; $a$ is called the
\emph{offset} vector and $M$ is called the \emph{model matrix} in the
terminology of R functions \texttt{lm} and \texttt{glm}.
$M$ is called the \emph{design matrix} by others.
We use the terminology favored by R.
The submodel parameter vector $\beta$ is called the \emph{coefficients}
vector by R.  We will find another name for it in this section.

The term ``canonical affine submodel'' was introduced by \citet{aster1}.
Usually these are called linear models or generalized linear models or
log-linear models in various settings.

\index{affine function|(}
There are two notions of linear used in mathematics.  There is a
sharp dividing line at the beginning of linear algebra.  In calculus
and lower level mathematics (including pre-college) a linear function
is one with a flat graph.  In linear algebra and all higher level mathematics
a linear function is one that preserves the vector space operations.
In particular, if $f$ is a linear function in this higher-level sense,
then $f(0) = 0$.  If one needs the lower-level sense in higher-level
mathematics, it is called an affine function.  An affine function
between vector spaces is
a linear function plus a constant function.  If $a \neq 0$
in \eqref{eq:affine}, then this is a linear change-of-parameter in the
lower-level sense but an affine change-of-parameter in the higher-level sense.
It is unclear (to me) whether the ``linear'' in linear models, generalized
linear models, and log linear models is the lower-level sense allowing
$a \neq 0$ in \eqref{eq:affine} or the higher-level sense assuming $a = 0$
in \eqref{eq:affine}, because $a = 0$ in most applications.
We follow \citet{aster1} in calling these models affine.
\index{affine function|)}

The log likelihood for the canonical affine submodel is
\begin{align*}
   l(\beta)
   & =
   \inner{y, a + M \beta} - c(a + M \beta)
   \\
   & =
   \inner{y, a} + \inner{y, M \beta} - c(a + M \beta)
\end{align*}
and the term $\inner{y, a}$ that does not contain the parameter $\beta$
can be dropped from the log likelihood giving
$$
   l(\beta)
   =
   \inner{y, M \beta} - c(a + M \beta)
$$
and, because
$$
   \inner{y, M \beta} = y^T (M \beta) = y^T M \beta = (M^T y)^T \beta
   = \inner{M^T y, \beta}
$$
we can write the submodel log likelihood in exponential family form
\begin{equation} \label{eq:submodel}
   l(\beta)
   =
   \inner{M^T y, \beta} - c_\text{sub}(\beta)
\end{equation}
where
\begin{equation} \label{eq:submodel-cumfun}
   c_\text{sub}(\beta)
   =
   c(a + M \beta), \qquad \text{for all $\beta$}.
\end{equation}
This shows the canonical affine submodel is itself an exponential family with
\begin{itemize}
\item canonical statistic vector $M^T y$,
\index{exponential family!canonical statistic!submodel}
\item canonical parameter vector $\beta$, and
\index{exponential family!canonical parameter!submodel}
\item cumulant function $c_\text{sub}$.
\index{exponential family!cumulant function!submodel}
\end{itemize}
\index{exponential family!canonical affine submodel|)}

\subsection{Moment and Cumulant Generating Functions}

\index{generating function|seeunder{exponential family}}
The \emph{moment generating function} (MGF) of a random vector $Y$ is given by
$$
   M(t) = E\left(e^{\inner{Y, t}}\right)
$$
provided that the function $M$ so defined is finite on a neighborhood of
zero; otherwise we say the random vector $Y$ does not have an MGF.
Clearly, the MGF only depends on the distribution of $Y$,
so we also this is the MGF of this distribution.

The MGF of the distribution of the canonical statistic of an exponential
\index{exponential family!moment generating function}
family corresponding to canonical parameter $\theta$ is given by
\begin{align*}
   M_\theta(t)
   & =
   E_\theta\left(e^{\inner{Y, t}}\right)
\end{align*}
We rearrange \eqref{eq:cumfun-expfam} obtaining
$$
   e^{c(\theta) - c(\psi)}
   =
   E_\psi\bigl( e^{\inner{Y, \theta - \psi}} \bigr)
$$
and then change $\theta$ to $\theta + t$ and $\psi$ to $\theta$ in that order
obtaining
$$
   e^{c(\theta + t) - c(\theta)}
   =
   E_\theta\bigl( e^{\inner{Y, t}} \bigr)
$$
so the MGF of the distribution of $Y$ for parameter vector $\theta$ is
$$
   M_\theta(t)
   =
   e^{c(\theta + t) - c(\theta)}
$$
provided $M_\theta$ is finite on a neighborhood of zero,
which is when $c$ is finite on a neighborhood of $\theta$,
which is when $\theta$ is in the interior of the full canonical
parameter space \eqref{eq:full-expfam}.
Hence every distribution in a \emph{regular} full exponential
family has an MGF.

The \emph{cumulant generating function} (CGF) of a random vector $Y$ is
\index{exponential family!cumulant generating function}
the log of the MGF provided the MGF exists.
If a distribution has no MGF, then it has no CGF either.
Hence the CGF of the distribution of $Y$ for parameter vector $\theta$ is
$$
   K_\theta(t)
   =
   c(\theta + t) - c(\theta)
$$
provided $K_\theta$ is finite on a neighborhood of zero,
which is when $\theta$ is in the interior of the full canonical
parameter space \eqref{eq:full-expfam}.
Hence every distribution in a \emph{regular} full exponential
family has a CGF.

The MGF is so called because its derivatives evaluated at zero give moments.
The CGF is so called because its derivatives evaluated at zero give cumulants.
Cumulants are polynomial functions of moments and vice versa.
Thus a distribution that has an MGF or a CGF has moments and cumulants of
all orders.
But we will not use moments and cumulants higher than second order
(next section) in our study of aster models.

Observe that derivatives of $K_\theta$ evaluated at zero are derivatives
of $c$ evaluated at $\theta$.  Hence the cumulant function is so called
\index{exponential family!cumulant function}
because its derivatives evaluated at $\theta$ give cumulants.
(Of course, this is only valid when $\theta$ is in the interior of the
full canonical parameter space \eqref{eq:full-expfam}).

This also shows that cumulant functions are infinitely differentiable
at every point in the interior of the full canonical
parameter space \eqref{eq:full-expfam}), and
cumulant functions of a regular full exponential are infinitely differentiable
at every point of the full canonical
parameter space \eqref{eq:full-expfam}).

\subsection{Mean and Variance}

First and second order cumulants are mean and variance
\begin{subequations}
\begin{align}
   E_\theta(Y) & = c'(\theta)
   \label{eq:cumulant-one}
   \\
   \var_\theta(Y) & = c''(\theta)
   \label{eq:cumulant-two}
\end{align}
\end{subequations}
\citep[Theorem~8.1]{barndorff-nielsen}.
\index{exponential family!canonical statistic!mean vector}
\index{exponential family!canonical statistic!variance matrix}
Of course, the formulas are valid only where $c$ is differentiable,
which is when $\theta$ is in the interior of the full canonical
parameter space \eqref{eq:full-expfam}.
On the boundary of the full canonical parameter space, the derivatives
on the right-hand sides of these formulas do not exist and the cumulants
on the left-hand sides might or might not exist but in any case are not
given by these formulas.
In a \emph{regular} full exponential family these formulas are good
for all values of $\theta$ in the full canonical
parameter space \eqref{eq:full-expfam}.

In \eqref{eq:cumulant-one} $Y$ is a vector and its expectation is a vector
having the same index set: $\mu = E_\theta(Y)$ is the vector having components
$\mu_j = E_\theta(Y_j)$.
The right-hand side of \eqref{eq:cumulant-one} must also be a vector having
the same index set: its components are $\partial c(\theta) / \partial \theta_j$.

In \eqref{eq:cumulant-two} $Y$ is a vector and its variance is a square
symmetric matrix having components $\cov_\theta(Y_j, Y_k)$.
The right-hand side of \eqref{eq:cumulant-two} must also be a matrix having
the same index set: its components are
$\partial^2 c(\theta) / \partial \theta_j \partial \theta_k$.

Many people do not like our terminology calling the left-hand side
of \eqref{eq:cumulant-two} the \emph{variance} matrix of $Y$.
Names in common use are \emph{covariance} matrix (but this is horrible
terminology because it uses up a name that should be used for the covariance
of two random vectors), \emph{variance-covariance} matrix,
and \emph{dispersion} matrix.
We use our terminology, because it denotes the multivariate analog
of the variance of a random variable.
This can be seen everywhere in probability theory.
If you can accept our notation $\var_\theta(Y)$ for this mathematical
object, then the same formulas work for univariate and multivariate $Y$.

\section{Aster Models and Exponential Families}
\label{sec:aster-expfam}

\index{canonical affine submodel|seeunder{aster model}}
\index{canonical statistic|seeunder{aster model}}
\index{canonical parameter|seeunder{aster model}}
\index{cumulant function|seeunder{aster model}}
We finally get to the final axiom of aster models.
Each conditional distribution in the factorization \eqref{eq:factorize}
is actually stands for parametric family of distributions,
and each of these is an exponential family of distributions.
As explained in Section~\ref{sec:piss} above,
the distribution for these conditional families is determined
by the distribution for sample size one, and the distributions
for dependence group $G$ for sample size one form an exponential family having
\index{dependence group|seeunder{aster model}}
\begin{itemize}
\item canonical statistic $y_G$,
\index{aster model!dependence group!canonical statistic}
\item canonical parameter $\theta_G$, and
\index{aster model!dependence group!canonical parameter}
\item cumulant function $c_G$.
\index{aster model!dependence group!cumulant function}
\end{itemize}
This notation means that $\theta$ is a vector with the same index set as
the response vector $y$ and that $\theta_G$ are subvectors of $\theta$
in the same way as $y_G$ are subvectors of $y$.
The fact that the cumulant functions are subscripted means every
dependence group may correspond to a different exponential family,
and we saw this in the examples (Section~\ref{sec:graphs} above).

Now Section~\ref{sec:iid} above tells us the cumulant function for sample
size $n$ is $n$ times the cumulant function for sample size one.
Note that this is valid for $n = 1$ and $n = 0$ too.
For the latter we calculate using \eqref{eq:cumfun-expfam} in the case
that $y$ is the random vector concentrated at zero
$$
   c(\theta) = c(\psi) + \log\left\{ E_\psi(1) \right\}
   = c(\psi) + \log(1)
   = c(\psi)
$$
so the cumulant function of the random vector concentrated at zero is
a constant function ($c(\psi)$ is arbitrary but $\psi$ is a constant
in this equation),
and to obtain agreement with the assertion that the cumulant function for
sample size $n$ is $n$ times the cumulant function for sample size one,
we take this arbitrary constant to be zero, so the cumulant function for
sample size zero is always the zero function, which always has the value zero.

Now the predecessor-is-sample-size property says that the sample size
for dependence group $G$ is $y_{q(G)}$, so the cumulant function for this
sample size is $y_{q(G)}$ times the cumulant function for sample size one.

Because the log of a product is the sum of the logs, the log likelihood
for an aster model is the sum of terms, one term for each term in the
factorization \eqref{eq:factorize}, and each term looks like
\eqref{eq:logl-expfam} except with subscripts for the dependence groups,
that is,
\begin{equation} \label{eq:logl-aster-theta}
   l(\theta)
   =
   \sum_{G \in \mathcal{G}}
   \left[ \inner{y_G, \theta_G} - y_{q(G)} c_G(\theta_G) \right]
\end{equation}
\index{aster model!log likelihood}
\index{log likelihood|seeunder{aster model}}
(by the predecessor-is-sample-size property, the conditional distribution
of $y_G$ given $y_{q(G)}$ is the sum of IID random vectors having cumulant
function $c_G$, hence the cumulant function in each term is $y_{q(G)}$
times the cumulant function for sample size one).

As was mentioned in Section~\ref{sec:miss} above, probability theory
just does the Right Thing in case $y_{q(G)} = 0$.
By the predecessor-is-sample-size property, this implies $y_G = 0$ too,
so the whole contribution to the log likelihood of the term for dependence
$G$ is zero (when, as we are discussing here, the predecessor is zero).
And zero is the correct contribution because the conditional distribution
of $y_G$ given $y_{q(G)} = 0$ is concentrated at zero,
that is, it is zero with probability one, so the contribution to the
log likelihood should be $\log(1) = 0$.

So there is no need to do anything special about the case where predecessor
equals zero.  All of our formulas are correct in this case too.

\section{Infinitely Divisible Aster Families}
\label{sec:infinitely-divisible}

As stated in Section~\ref{sec:piss} above,
one consequence of the predecessor-is-sample-size property is that
components of the response vector
that are predecessors must be
nonnegative-integer-valued random variables.

\index{predecessor random variable!real valued|(}
Except \citet{aster1} note an exception to this rule.  If the distribution
in question is infinitely divisible, then $r$-fold convolution makes sense
for any $r \ge 0$.  A distribution having moment generating function $m$
is infinitely divisible if and only if $m(\fatdot)^r$ is a moment generating
\index{infinitely divisible}
function for all real $r \ge 0$ \citep[Corollary~4.2.2]{cuppens}.
Then the distribution having moment
generating function $m(\fatdot)^r$ is defined to be the $r$-fold convolution
of the distribution having moment generating function $m$.
Hence a regular full exponential family having cumulant function $c$
is infinitely divisible if and only if $r c(\fatdot)$ is a cumulant
function for all real $r \ge 0$.
R package \code{aster} \citep{aster-package} has several infinitely divisible
families: Poisson, negative binomial, and normal-location
(normal with unknown mean and known variance).
Hence, if the conditional distribution of $y_G$ given $y_{q(G)}$
is infinitely divisible, then
the distribution of $y_{q(G)}$ can be nonnegative-real-valued.

This observation, however, has played no role in applications of aster models
because R packages \code{aster} and \code{aster2} do not implement any
families of nonnegative-valued random variables that are not
also integer-valued.  Such families exist (the gamma distribution,
for example), so if they were implemented, there could be a role for this
observation.
\index{predecessor random variable!real valued|)}

\section{The Aster Transform}
\label{sec:aster-transform}

The aster log likelihood \eqref{eq:logl-aster-theta} does not have
exponential family form \eqref{eq:logl-expfam}.
But since \eqref{eq:logl-aster-theta} is linear in $y$, it must be an
exponential family having canonical statistic vector $y$.  In order
to figure out the canonical parameter and cumulant function for this family
we rewrite \eqref{eq:logl-aster-theta} as follows
\begin{equation} \label{eq:logl-rewrite}
   l(\theta)
   =
   \left(
   \sum_{j \in J} y_j
   \left[ \theta_j - \sum_{\substack{G \in \mathcal{G} \\ q(G) = j}}
   c_G(\theta_G) \right]
   \right)
   -
   \left(
   \sum_{\substack{G \in \mathcal{G} \\ q(G) \notin J}}
   y_{q(G)} c_G(\theta_G) \right)
\end{equation}
and now we see we do have exponential family form with the canonical
parameters (of the exponential family which is the joint distribution of $y$)
being the terms in square brackets (the multipliers of the components of $y$)
\index{aster transform|(}
\begin{equation} \label{eq:aster-transform}
   \varphi_j
   =
   \theta_j - \sum_{\substack{G \in \mathcal{G} \\ q(G) = j}} c_G(\theta_G),
   \qquad j \in J,
\end{equation}
and the cumulant function being the terms left over
\begin{equation} \label{eq:aster-cumfun}
   c(\varphi)
   =
   \sum_{\substack{G \in \mathcal{G} \\ q(G) \notin J}} y_{q(G)} c_G(\theta_G)
\end{equation}
(the fact that we have $\varphi$ on one side of the equation and $\theta$
on the other will be explained presently).
Note that all of the $y_{q(G)}$ appearing in \eqref{eq:aster-cumfun}
are constant random variables (because they are at initial nodes).
Thus \eqref{eq:aster-cumfun} does define a deterministic (non-random)
function of the parameter, as a cumulant function must be.

This notation means that $\varphi$ is a vector with the same index set as
the response vector $y$, that $\varphi_G$ are subvectors of $\varphi$
in the same way as $y_G$ are subvectors of $y$, and
that $\varphi_j$ are components of $\varphi$
in the same way as $y_j$ are components of $y$.

The map $\theta \mapsto \varphi$ is called the \emph{aster transform}.
\index{aster transform|)}
\index{aster transform!inverse|(}
We claim this parameter transformation is invertible and both the
transform and its inverse are infinitely differentiable.
We invert the function very simply moving terms from the right-hand
side to the left-hand side
\begin{equation} \label{eq:inverse-aster-transform}
   \theta_j
   =
   \varphi_j + \sum_{\substack{G \in \mathcal{G} \\ q(G) = j}} c_G(\theta_G),
   \qquad j \in J.
\end{equation}
How can this define $\theta$ in terms of $\varphi$ when components of $\theta$
appear on both sides of the equation?
Simply use the equations \eqref{eq:inverse-aster-transform} in any order
that visits successors before predecessors.  Theorem~\ref{th:factorize}
guarantees there is such an order (it guarantees an order among dependence
groups, but we can order components within a dependence group arbitrarily).
Then when we are using \eqref{eq:inverse-aster-transform} to compute
$\theta_j$ we will have already have computed $\theta_k$ for all $k$ that
are successors, successors of successors, and so forth of $j$, and,
in particular, we will already have computed $\theta_G$ for all $G$ such
that $q(G) = j$.

The map $\varphi \mapsto \theta$ is called the \emph{inverse aster transform}.
The aster transform is clearly infinitely differentiable because cumulant
functions are infinitely differentiable (when all of the families for
the dependence groups are regular full exponential families).
The inverse function theorem of real analysis \citep[Theorem~8.27]{browder}
says the inverse function
is differentiable as many times as the function it is the inverse of.
Hence the inverse aster transform is also infinitely differentiable.
\index{aster transform!inverse|)}

Now we see why it is OK for \eqref{eq:aster-cumfun} to have $\varphi$
on the left-hand side and $\theta$ on the right-hand side: $\theta$
is given as a function of $\varphi$ by the inverse aster transform
so the right-hand side of \eqref{eq:aster-cumfun} can be considered
a function of $\varphi$.

Now we can express \eqref{eq:logl-rewrite} in exponential family form
\begin{equation} \label{eq:logl-aster-phi}
   l(\varphi) = \inner{y, \varphi} - c(\varphi)
\end{equation}
where the cumulant function $c$ of the joint distribution of the aster
model is \eqref{eq:aster-cumfun}.

We call
\begin{itemize}
\item $\theta$ the \emph{conditional canonical parameter vector} and
\index{aster model!canonical parameter!conditional}
\item $\varphi$ the \emph{unconditional canonical parameter vector}.
\index{aster model!canonical parameter!unconditional}
\end{itemize}
But this terminology makes these parameters more like two kinds of the
same sort of thing than they really are.
\begin{itemize}
\item The subvectors $\theta_G$ are the canonical parameter vectors of
    the conditional distributions of the dependence groups, but, as
    we have seen (this is the whole point of the aster transform),
    $\theta$ is not the canonical parameter vector of the (unconditional,
    joint) distribution of the aster model.
\item The vector $\varphi$ is the canonical parameter vector of the
    (unconditional, joint)
    distribution of the aster model, but the subvectors $\varphi_G$ or the
    components $\varphi_j$ are not separately canonical for anything.
\end{itemize}
In short the subvectors of $\theta$ are groupwise canonical but
$\theta$ is not vectorwise canonical, and, conversely,
$\varphi$ is vectorwise canonical, but its subvectors are not groupwise
canonical.

\section{More On Exponential Families}

\subsection{Directions of Constancy, Identifiability}
\label{sec:direction-of-constancy}

A direction $\delta$ in the vector space where the canonical parameter vector
of an exponential family takes values is called a \emph{direction of constancy}
if $\inner{Y, \delta}$ is constant almost surely, where $Y$ is the canonical
statistic vector.
Because all of the distributions in an exponential family have
the same support (this follows from the log likelihood being finite for
all all data values when the canonical parameter value is in the full
canonical parameter space \eqref{eq:full-expfam}) we don't have to say
which distribution in the family almost surely refers to: if a property
holds almost surely for one distribution in the exponential family,
then it holds almost surely for all distributions in the exponential family.

\citet[Theorem~1]{geyer-gdor} gives many equivalent
characterizations of this concept.  Here are some of them.
In these we use the notation $y$ is the canonical statistic vector,
$\theta$ is the canonical parameter vector, $\delta$ is a vector in
the vector space where $\theta$ takes values, $l$ is the log likelihood
\eqref{eq:logl-expfam}, and $\Theta$ is the full canonical space
\eqref{eq:full-expfam}.
\begin{enumerate}
\item[(a)] For some $\theta \in \Theta$,
    the function $s \mapsto l(\theta + s \delta)$ is finite on some open
    interval and is not strictly concave on that interval.
\item[(b)] For all $\theta \in \Theta$,
    the function $s \mapsto l(\theta + s \delta)$ is constant on the
    whole real line.
\item[(c)] The parameter vectors $\theta$ and $\theta + s \delta$ correspond
    to the same distribution for some $\theta \in \Theta$ and some $s \neq 0$.
\item[(d)] The parameter vectors $\theta$ and $\theta + s \delta$ correspond
    to the same distribution for all $\theta \in \Theta$ and all real $s$.
\item[(e)] The random variable $\inner{Y, \delta}$ is almost surely constant
    for some distribution in the exponential family.
\item[(f)] The random variable $\inner{Y, \delta}$ is almost surely constant
    for all distributions in the exponential family.
\end{enumerate}
We arbitrarily picked (e) to serve as the definition,
but, because they are all equivalent \citep[Theorem~1]{geyer-gdor},
we could have picked any of these to serve as the definition.

Condition (b) explains the terminology; these are directions of
constancy of the log likelihood function as defined in \citet{rockafellar}.
They obviously imply, if $\hat{\theta}$ is a maximum likelihood estimate (MLE),
then so is $\hat{\theta} + s \delta$ for all real $s$.
Condition (a) gives a uniqueness condition for maximum likelihood.
If $\delta$ is not a direction of constancy and $s \neq 0$,
then $\theta$ and $\theta + s \delta$ cannot both be maximum likelihood
estimates because (a) would imply the log likelihood is higher somewhere
between these points.  Thus the MLE for a full
exponential family is unique (if it exists) if and only if there are
no nonzero directions of constancy.

Conditions (c) and (d) connect this concept with identifiability.
If there exist two distinct canonical parameter vectors $\theta$ and $\theta^*$
that correspond to the same distribution, then $\theta - \theta^*$ is a
direction of constancy.  Thus this concept characterizes the only form
of nonidentifiability the canonical parameterization of an exponential
family can have.  Conversely, if there are no nonzero directions of constancy,
then the canonical parameterization is identifiable.

However, we do not insist on identifiability for three reasons.
\begin{itemize}
\item
As \citet{geyer-gdor} says, non-identifiability ``is, at worst, merely
a computational nuisance'' so it is a big mistake to contort theory
and applications to achieve identifiability too early.  The computer
can put it in at the end with no help from humans
(for how see Section~\ref{sec:dealing} below).
\item Multinomial dependence groups require non-identifiable parameterization.
%%%%%%%%%% NEED FORWARD REFERENCE multinomial family %%%%%%%%%%
\item Limiting conditional models require non-identifiable parameterization
if they are to use the same parameterization as the original model.
%%%%%%%%%% NEED FORWARD REFERENCE multinomial family %%%%%%%%%%
\end{itemize}

Conditions (e) and (f) connect this concept with multivariate degeneracy.
The vector $\delta$ is a direction of constancy if and only if the random
variable $\inner{Y, \delta}$ is almost surely constant, which is the
same thing as saying the canonical statistic vector $Y$ is concentrated
on a hyperplane.  So the canonical parameterization of a full exponential
family is identifiable if and only if $Y$ is not concentrated on a hyperplane.
A random variable is constant almost surely if and only if its variance
is zero.  Hence we can also characterize directions of constancy in
terms of the variance matrix of the canonical statistic
\eqref{eq:cumulant-two}.  In a regular full exponential family, let
$$
   I(\theta) = \var_\theta(Y).
$$
(we need a regular full exponential family to guarantee the variance exists).
This is the Fisher information matrix as well as the variance matrix
\index{Fisher information|seeunder{exponential family}}
\index{exponential family!Fisher information}
of the canonical statistic because differentiating the log likelihood
\eqref{eq:logl-expfam} twice gives
$$
   l''(\theta) = - c''(\theta) = - \var_\theta(Y).
$$
Then clearly, $\delta$ is a direction of constancy if and only if
$$
   \var_\theta(\inner{Y, \delta}) = \delta^T I(\theta) \delta.
$$
And by the spectral theorem
\citet[Section~79]{halmos-vector-spaces} this holds if and only if
$I(\theta) \delta = 0$, in words, if and only if $\delta$ is a null
eigenvector of the Fisher information matrix.

\subsection{Mean Value Parameters}

\index{exponential family!mean value parameter|(}
\begin{theorem} \label{th:mean-value}
For a regular full exponential family, every distribution has a mean
value (for the canonical statistic vector), and different distributions
have different mean values.  Hence mean values parameterize a regular
full exponential family, and the mean value parameterization is always
identifiable, whether or not the canonical parameterization is identifiable.
\end{theorem}
\begin{proof}
We already know that mean values exist and are given by \eqref{eq:cumulant-one}.
Let us temporarily adopt a notation for the map from canonical to mean value
parameters: $f : \theta \mapsto c'(\theta)$.
This map is differentiable with derivative matrix
$$
   f'(\theta) = c''(\theta) = I(\theta).
$$
Consider distinct distributions in the exponential family having
canonical parameter vectors $\theta$ and $\theta^*$.
Then $\theta - \theta^*$ is not a direction of constancy.
Define a function $g$ by
$$
   g(s) = f\bigl(s \theta + (1 - s) \theta^*\bigr).
$$
Then the corresponding mean values are
\begin{align*}
   \mu & = g(1) = f(\theta)
   \\
   \mu^* & = g(0) = f(\theta^*)
\end{align*}
Differentiating $g$ gives
\begin{align*}
   g'(s)
   & =
   f'\bigl(s \theta + (1 - s) \theta^*\bigr) (\theta - \theta^*)
   \\
   & =
   I\bigl(s \theta + (1 - s) \theta^*\bigr) (\theta - \theta^*)
\end{align*}
Since $I\bigl(s \theta + (1 - s) \theta^*\bigr)$ is a variance matrix,
it is positive semi-definite.
Since $\theta - \theta^*$ is not a direction of constancy,
\begin{equation} \label{eq:identifiable-foo}
   (\theta - \theta^*)^T
   I\bigl(s \theta + (1 - s) \theta^*\bigr) (\theta - \theta^*)
\end{equation}
is strictly positive for all $s$ in some open interval of the real line
containing 0 and 1 (because $\theta$ and $\theta^*$ are interior points
of the full canonical parameter space).
We note that \eqref{eq:identifiable-foo} is the
derivative of the function $h$ defined by
$$
   h(s) = \inner{g(s), \theta - \theta^*}
$$
Since the derivative of this function is strictly positive it is strictly
increasing on some open interval containing 0 and 1, hence
$$
   h(1) - h(0) = \inner{g(1) - g(0), \theta - \theta^*}
   = \inner{\mu - \mu^*, \theta - \theta^*}
$$
is strictly positive.  Hence $\mu \neq \mu^*$.
\end{proof}
\index{exponential family!mean value parameter|)}

The parameter vector $\mu = c'(\theta)$ is called
the \emph{mean value parameter} of the regular full exponential family.
(A full exponential family that is not regular need not have a mean
value for every distribution.  It must have a mean value for every
distribution whose canonical parameter vector $\theta$ in the interior
of the full canonical parameter space $\Theta$,
but it need not have mean values for distributions whose $\theta$ is
on the boundary of $\Theta$.)

\subsection{Calculating the Inverse Transformation}
\label{sec:calculating-inverse-transformation}

Discussed in the proof of the preceding theorem was the transformation
$\theta \mapsto c'(\theta)$, which the theorem says maps from a not
necessarily identifiable parameter ($\theta$) to a necessarily identifiable
parameter ($\mu$).  In this section we find out this parameter transformation
is invertible if and only if the canonical parameterization is identifiable,
and we also find out how to calculate the inverse if it exists.

Define a function $\tilde{l}$ that is just like the log likelihood
of the exponential family \eqref{eq:logl-expfam} except that we replace
the observed value of the canonical statistic vector $y$ with a possible
mean value
\begin{equation} \label{eq:logl-expfam-expected}
   \tilde{l}(\theta) = \inner{\mu, \theta} - c(\theta),
\end{equation}
where $\mu = E_\theta(Y)$ for some $\theta$.

\begin{lemma}
The cumulant function of a full exponential family is a convex function.
It is strictly convex if and only if the exponential family has no
nonzero directions of constancy.
\end{lemma}
This is Theorem~{7.1} in \citet{barndorff-nielsen}.
An alternative proof for regular full exponential families can
use the second derivative test in Theorem~{2.14} in \citet{rockafellar-wets}.
The function $c$ is convex because $c''(\theta)$ is a variance matrix,
hence positive semi-definite for all $\theta$.
When there are no nonzero directions of constancy,
$c''(\theta)$ is positive definite for all $\theta$,
so $c$ is strictly convex.

The derivatives of \eqref{eq:logl-expfam-expected} are
\begin{subequations}
\begin{align}
   \tilde{l}'(\theta) & = \mu - c'(\theta)
   \\
   \tilde{l}''(\theta) & = - c''(\theta)
\end{align}
\end{subequations}
Applying the second derivative test
in Theorem~{2.14} in \citet{rockafellar-wets} gives
\begin{lemma}
The function $\tilde{l}$ defined by \eqref{eq:logl-expfam-expected}
is a concave function.
It is strictly concave if and only if the exponential family has no
nonzero directions of constancy.
\end{lemma}
\begin{lemma} \label{lem:concave-maximizer}
For a differentiable convex function, any point where the derivative
is zero is a global minimizer.
For a differentiable strictly convex function, any point where the derivative
is zero is a unique global minimizer.

For a differentiable concave function, any point where the derivative
is zero is a global maximizer.
For a differentiable strictly concave function, any point where the derivative
is zero is a unique global maximizer.
\end{lemma}
This is part of Theorem~{10.1} in \citet{rockafellar-wets}.

\begin{theorem} \label{th:mean-value-inversion}
Global maximizers of the function $\tilde{l}$ exist, and any global maximizer
$\theta$ satisfies $\mu = c'(\theta)$ and hence is a canonical parameter
vector corresponding to the mean value parameter $\mu$.

In case the canonical parameterization is identifiable, which is when there
are no nonzero directions of constancy, the function $\tilde{l}$ has
a unique global maximizer.
\end{theorem}
\begin{proof}
By assumption $\mu = E_\theta(Y) = c'(\theta)$ for some $\theta$,
hence by Lemma~\ref{lem:concave-maximizer} this $\theta$ is a global maximizer
of $\tilde{l}$.  If there are other global maximizers $\theta^*$,
then they also satisfy $\mu = c'(\theta^*)$.  Hence,
such $\theta$ and $\theta^*$ correspond to the same $\mu$.
Hence by Theorem~\ref{th:mean-value} $\theta$ and $\theta^*$ correspond
to the same distribution.
Hence, if the canonical parameterization is identifiable,
then $\theta = \theta^*$.
\end{proof}

\section{Aster Mean Value Parameters}

We have already met the mean value parameters of aster models in
Section~\ref{sec:conditional-and-unconditional-mean-values} above.
They are
\begin{itemize}
\item the \emph{unconditional mean value parameter vector} $\mu$ defined by
\index{aster model!mean value parameter!unconditional}
\eqref{eq:unconditional-mean-values} and
\item the \emph{conditional mean value parameter vector} $\xi$ defined by
\index{aster model!mean value parameter!conditional}
\eqref{eq:conditional-mean-values} when the conditioning event in that
equation makes sense and by the discussion following that equation otherwise.
\end{itemize}

In Section~\ref{sec:conditional-and-unconditional-mean-values} above
we were getting ahead of ourselves in two ways.  We didn't yet know
that these mean values existed, and we didn't yet know that they
parameterized the model.  Now we do.  So we fill in the details on that.

\subsection{Unconditional}

Theorem~\ref{th:regular} in Appendix~\ref{app:regular} says, if each
family for a dependence group is a regular full exponential family,
then the (unconditional, joint) distribution of the aster model is
a regular full exponential family.  All of the families for dependence
groups (or arrows for dependence groups that are single nodes) that
have ever been implemented in aster software are regular full exponential
families.
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
Thus all aster models that have ever been implemented are
regular full exponential families.

And hence we know how to map from aster model canonical parameter vectors
to mean value parameter vectors and vice versa.  The maps from canonical
to mean value are
\begin{equation} \label{eq:aster-phi-to-mu}
   \mu = c'(\varphi) = E_\varphi(Y),
\end{equation}
where $c$ is the cumulant function of the (unconditional, joint) aster
model \eqref{eq:aster-cumfun}, and $Y$ is the response vector of the
aster model, and
\begin{equation} \label{eq:aster-theta-to-xi}
   \xi_G = c_G'(\theta) = E_\theta(Y_G),
   \qquad G \in \mathcal{G},
\end{equation}
where $c_G$ is the cumulant function of dependence group $G$.

The inverse mappings to these mappings do not, in general,
have closed form expression
but are described in Section~\ref{sec:calculating-inverse-transformation}
above.  Define $\tilde{l}$ by
\begin{equation} \label{eq:logl-aster-expected}
   \tilde{l}(\varphi) = \inner{\mu, \varphi} - c(\varphi)
\end{equation}
where $\mu$ is a possible value of the unconditional mean value parameter
vector of the aster model and $c$ is the cumulant function of the aster
model \eqref{eq:aster-cumfun}.  Then any point $\varphi$
where the first derivative of $\tilde{l}$ is zero is a global maximizer
of $\tilde{l}$ and is an unconditional canonical parameter vector $\varphi$
that corresponds to $\mu$.  Such a global maximizer always exists.
If the canonical parameterization is identifiable (if there are
no nonzero directions of constancy), then this global maximizer is unique.
All of the above follows from Theorems~\ref{th:mean-value-inversion}
and~\ref{th:regular}.

When the canonical parameterization is identifiable, this process allows
us to calculate the map $\mu \mapsto \varphi$ that is the inverse to
to the map $\varphi \mapsto \mu$ given by $\varphi \mapsto c'(\varphi)$.
We know the forward transformation $\varphi \mapsto \mu$
is infinitely differentiable (because cumulant functions of regular
full exponential families are infinitely differentiable).
Hence it follows from the inverse function theorem of real analysis
\citep[Theorem~8.27]{browder} that the inverse function is infinitely
differentiable if it exists (we are not using the inverse function theorem
to prove the existence of a local inverse; we know a global inverse function
exists if and only if the unconditional canonical parameterization
is identifiable; we are using the assertion of the inverse function theorem
about differentiability of this inverse function).

In summary, we know the inverse mapping $\mu \mapsto \varphi$ exists
and is infinitely differentiable when $\varphi$ is identifiable,
but we generally have no closed-form expression for this mapping and
must calculate it by maximizing $\tilde{l}$ defined
by \eqref{eq:logl-aster-expected}.

Another conclusion of the inverse function theorem is that the derivative
of the inverse is the inverse of the derivative (this is not always
stated in the theorem statement, but does appear in the proof).
In our case, the derivative of the forward mapping $\varphi \to \mu$
is $c''(\varphi) = I(\varphi)$.  We know this matrix is invertible
if and only if the $\varphi$ is identifiable.  And when it is invertible
(when there are no nonzero directions of constancy, when $Y$ is not
concentrated on a hyperplane) the derivative of the inverse mapping
$\mu \to \varphi$ is $c''(\varphi)^{- 1}$.
In more detail, if we temporarily give this inverse mapping a notation
$f : \mu \to \varphi$ given by $\varphi$ is the unique global maximizer
of $\tilde{l}$ defined by \eqref{eq:logl-aster-expected}
(assuming the inverse mapping exists, that is, assuming identifiablilty).
Then
$$
   f'(\mu) = I(\varphi)^{-1}, \qquad \text{when $\varphi = f(\mu)$},
$$
or
$$
   f'(\mu) = I\bigl(f(\mu)\bigr)^{-1}.
$$

\subsection{Conditional}

Now we go over all of this section again for conditional mean values and
dependence groups.  We fuss less about the details because they are the
same \emph{mutatis mutandis}.
For dependence group $G$, define $\tilde{l}_G$ by
\begin{equation} \label{eq:logl-aster-expected-g}
   \tilde{l}_G(\theta_G) = \inner{\xi_G, \theta_G} - c_G(\theta_G)
\end{equation}
where $\xi_G$ is a possible value of the conditional mean value parameter
vector for $G$ and $c_G$ is the cumulant function for $G$.
Then any point $\theta_G$
where the first derivative of $\tilde{l}_G$ is zero is a global maximizer
of $\tilde{l}_G$.  Such a global maximizer always exists.
If the canonical parameterization is identifiable (if there are
no nonzero directions of constancy of the family for dependence group $G$),
then this global maximizer is unique.

When we do this procedure for each dependence group $G$ this defines
a mapping $\xi \to \theta$ if each dependence group has identifiable
canonical parameterization.  This mapping is infinitely differentiable
if it exists, and the derivative is given by inverting the derivative
of the forward mapping.  The forward mapping has block diagonal derivative
with components
\begin{alignat*}{2}
   \frac{\partial \xi_j}{\partial \theta_k}
   & =
   \frac{\partial^2 c_G(\theta_G)}{\partial \theta_j \partial \theta_k},
   & \qquad & j, k \in G,
   \\
   \frac{\partial \xi_j}{\partial \theta_k}
   & =
   0, & & \text{otherwise}.
\end{alignat*}
If we denote the matrix with these components by $I(\theta)$, then
$I(\theta)^{-1}$ is the derivative of the inverse mapping at a point
$\xi$ that corresponds to $\theta$.

\subsection{Dealing with Non-Identifiability}
\label{sec:dealing}

We are used to the computer dealing with non-identifiability, also called
collinearity, automatically.  R function \code{lm} and \code{glm} in core R
\citep{r-core}, which fit linear and generalized linear models automatically
handle it by dropping regressor vectors (columns of the model matrix) until
identifiability is obtained.  They signal this by reporting \code{NA} for
coefficient estimates corresponding to these dropped regressor vectors.

R package \code{aster} does more or less the same thing as R functions
\code{lm} and \code{glm} except that it simply does not mention regression
coefficients for dropped regressors (it does say which regressors these are
in the component \code{dropped} of the object returned by R function
\code{aster}).

In aster models fit by R package \code{aster} (which cannot have dependence
groups corresponding to more than one component of the response vector)
the only way that nonidentifiability of the canonical parameterization
can arise is if the model matrix does not have full column rank,
which is getting ahead of ourselves because we have not discussed
canonical affine submodels of aster models yet (next section).

In aster models fit by R package \code{aster2} the aster model can
have nonidentifiable canonical parameterizations ($\theta$ is nonidentifiable
if and only if $\varphi$ is nonidentifiable because the aster transform
is one-to-one) when the model has multinomial dependence groups or when
we are analyzing a limiting conditional model of the aster model.
%%%%%%%%%% NEED FORWARD REFERENCE multinomial %%%%%%%%%%
%%%%%%%%%% NEED FORWARD REFERENCE limiting conditional model %%%%%%%%%%

R package \code{aster2} does more or less the same thing as R functions
\code{lm} and \code{glm} except that it returns zero for certain
coefficients on the theory that constraining a parameter to be equal to zero
is equivalent to dropping the corresponding regressor. 

From a theoretical point of view there is just one issue: directions of
constancy (Section~\ref{sec:direction-of-constancy} above).
Suppose we have an exponential family in which the canonical parameter
is $\beta$ (this includes canonical affine submodels but also the models
they are submodels of).  If $\delta$ is a direction of constancy, then
$\beta$ and $\beta + s \delta$ correspond to the same distribution for
all $s$.  If $\delta \neq 0$, then we do not have identifiability, and
need to decide how to regain it.  The way to do so that R users (at least)
have been trained to expect is to constrain some $\beta_j$ to be equal
to zero.

If $\delta \neq 0$, then it has a nonzero component, say $\delta_i$.
Then we can constrain $\beta_i$ to be equal to zero, because for any
$\beta$, the parameter vector $\beta + s \delta$ with
$$
   \beta_i + s \delta_i = 0
$$
or
$$
   s = - \beta_i / \delta_i
$$
has $i$-th component zero and corresponds to the same distribution as $\beta$.

So this procedure of obtaining identifiability by constraining some
parameter to be equal to zero works for any exponential family
(and works for GLM that are not exponential families by them being
close enough to exponential families in the theoretical aspects that
are necessary for this to work).

This procedure can always be implemented by the computer provided it has
a way of determining directions of constancy.
It is clear from the definition in Section~\ref{sec:direction-of-constancy}
above that the set of all directions of constancy is a subset of the
vector space where the canonical parameter takes values.
Since that vector space is finite-dimensional, so is the subspace.
The set of all directions of constancy is called the \emph{constancy space}.
Hence the set of all directions of constancy can be characterized by
providing a basis for the constancy space.

So long as aster software has a function like R function \code{constancy}
in R package \code{aster2} that provides a basis for the constancy space,
the computer can do the rest of dealing with non-identifiability.

\section{A Plethora of Parameters}
\label{sec:plethora}

We now have the parameterizations $\theta$, $\varphi$, $\xi$, and $\mu$
for aster models.  We now know that
\begin{itemize}
\item $\mu$ is always identifiable,
\item $\xi$ is always identifiable unless some $\mu_{q(G)} = 0$,
\item $\varphi$ is identifiable if and only if there are no nonzero
    directions of constancy, and
\item $\theta$ is identifiable if and only if $\varphi$ is identifiable.
\end{itemize}


The simple closed-form expressions we have
for these parameter transformations are shown below.
% in tikzcd apostrophe label option means label on opposite side of arrow
% woot!
\begin{equation} \label{eq:plethora-four}
\begin{tikzcd}[column sep=9em]
   \theta
   \arrow[r, "\text{aster transform}", shift left]
   \arrow[d, "\xi_G = c_G'(\theta_G)"', shift right]
   &
   \varphi
   \arrow[l, "\text{inverse aster transform}", shift left]
   \arrow[d, "\mu = c'(\varphi)", shift left]
   \\
   \xi
   \arrow[u, shift right]
   \arrow[r, "\text{multiplication}", shift left]
   &
   \mu
   \arrow[l, "\text{division}", shift left]
   \arrow[u, shift left]
\end{tikzcd}
\end{equation}
where (of course) ``aster transform'' is \eqref{eq:aster-transform} above,
``inverse aster transform'' is \eqref{eq:inverse-aster-transform} above,
``multiplication'' is \eqref{eq:mu-and-xi} or \eqref{eq:xi-mu-prod} or
\eqref{eq:xi-mu-prod-too} above,
``division'' is \eqref{eq:mu-to-xi}, and the equations labeling downwards
arrows refer to \eqref{eq:aster-theta-to-xi} and \eqref{eq:aster-phi-to-mu}
above.

Both upward arrows being unlabeled indicates no closed-form expressions
for going from mean value parameters to canonical parameters.
There is an algorithm (Section~\ref{sec:calculating-inverse-transformation}
above) but (in general) no closed-form expression.
All of the arrows do, however, indicate smooth (infinitely differentiable)
changes of parameter, if they correspond to a change of parameter at all.

The arrows that may not correspond to a change of parameter arrow are
\begin{itemize}
\item the arrow $\mu \mapsto \xi$ labeled ``division'' fails to be a map
    when there is division by zero, in which case $\xi$ is not identifiable,
\item the arrows $\xi \mapsto \theta$ and $\mu \mapsto \varphi$, which
    are unlabeled fail to be maps when the canonical parameterizations
    are not identifiable (and either both are identifiable or neither are).
\end{itemize}

As explained in Section~\ref{sec:dealing} above identifiability of either
canonical parameterization can be recovered by constraining certain parameters
to be equal to zero.  When we do this we need to be careful to not mess up
the other parameter transformations.
\begin{itemize}
\item If we constrain some components of $\varphi$ to be equal to zero in
    order to make $\varphi$ identifiable and the transformation
    $\mu \to \varphi$ a map, then this also makes $\theta$ identifiable,
    but since the inverse aster transform is nonlinear this does not
    correspond to setting any components of $\theta$ to be equal to zero.
\item Conversely, if we constrain some components of $\theta$ to be equal
    to zero in order to make $\theta$ identifiable and the transformation
    $\xi \to \theta$ a map, then this also makes $\varphi$ identifiable,
    but since the aster transform is nonlinear this does not
    correspond to setting any components of $\varphi$ to be equal to zero.
\item When some components of the response vector that are predecessors
    are equal to zero almost surely $\xi$ is not identifiable:
    if $\mu_{q(G)} = 0$, then $\xi_G$ can be chosen arbitrarily,
    but then $\theta_G$ will be determined by this arbitrary choice,
    and then $\varphi$ will be determined by $\theta$.
    Since $\mu$ is always identifiable, it is unchanged when $\xi_G$
    is allowed to be chosen arbitrarily (the arbitrary choice is multiplied
    by zero in the conversion to $\mu$).
\end{itemize}

All of these parameterizations are important.  Any may be crucial in
some applications and irrelevant to other applications.  From this point
forward we now call the statistical model we have been discussing the
\emph{saturated aster model}.  No matter which parameterization we choose,
\index{aster model!saturated}
the length of the parameter vector is the same as the length of the response
vector (both have the same index set $J$).

The saturated aster model has too many parameters chasing too little data.
More parsimonious models are used in applications.

The next few sections introduce these more parsimonious models and
even more parameters,
and Sections~\ref{sec:revisited} and~\ref{sec:re-revisited} below
discuss their transformations similar to how this section does.

\section{Aster Canonical Affine Submodels}

It may come as a shock to the reader that all of the work done so far
concerns models that are of no interest in applications.

\subsection{Unconditional}

\index{aster model!unconditional|(}
\index{aster model!canonical affine submodel!unconditional|(}
In this section we straightforwardly apply the theory
of Section~\ref{sec:canonical-affine-submodel} above,
which describes canonical affine submodels of general exponential families,
to aster models.  In Section~\ref{sec:canonical-affine-submodel} the canonical
parameter vector of the exponential family was denoted $\theta$.
The canonical parameter vector of the (unconditional, joint) aster model
is the \emph{unconditional} canonical parameter vector $\varphi$.
Thus we parameterize an aster \emph{unconditional canonical affine submodel}
\begin{equation} \label{eq:affine-unconditional}
   \varphi = a + M \beta,
\end{equation}
where, as stated in Section~\ref{sec:canonical-affine-submodel} above,
$a$ is the offset vector and $M$ is the model matrix.
Either $a$ or $M$ may depend on covariate data since aster analyses,
like all regression analyses are done conditional on covariate data.
Usually $a$ does not depend on covariate data and $M$ does depend
on covariate data.
This is the only place where covariate data enters
an unconditional aster model (for short we refer to such models
as \emph{unconditional aster models} rather than
\emph{unconditional canonical affine submodels of aster models}).

From the theory in Section~\ref{sec:canonical-affine-submodel} above,
an unconditional aster model is itself a regular full exponential family
with
\begin{itemize}
\item canonical statistic vector $M^T y$,
\item canonical parameter vector $\beta$,
\item cumulant function $c_\text{sub}$ defined by
$$
   c_\text{sub}(\beta) = c(a + M \beta)
$$
where $c$ is the cumulant function of the saturated aster model given by
\eqref{eq:aster-cumfun} above, and
\item mean value parameter
$$
   \tau = c_\text{sub}'(\beta) = E_\beta(M^T Y) = M^T \mu,
$$
where $\mu$ is the saturated model mean value parameter.
\end{itemize}

Theorem~\ref{th:regular-submodel} in Appendix~\ref{app:regular} says
that every unconditional aster model is a regular full exponential family
if the saturated model is a regular full exponential.
Theorem~\ref{th:regular} in Appendix~\ref{app:regular} says the latter
happens when every family for every dependence group is a regular full
exponential family, which is the case for all families currently implemented
in aster software.
\index{aster model!unconditional|)}
\index{aster model!canonical affine submodel!unconditional|)}

\subsection{A Plethora of Parameters Revisited}
\label{sec:revisited}

With unconditional canonical affine submodels (preceding section) we get
two more parameters, so we can change our picture to
\begin{equation} \label{eq:plethora-six}
\begin{tikzcd}[column sep=9em]
   \theta
   \arrow[r, "\text{aster transform}", shift left]
   \arrow[d, "\xi_G = c_G'(\theta_G)"', shift right]
   &
   \varphi
   \arrow[l, "\text{inverse aster transform}", shift left]
   \arrow[d, "\mu = c'(\varphi)", shift left]
   \arrow[r, shift right]
   &
   \beta
   \arrow[l, "\varphi = a + M \beta"', shift right]
   \arrow[d, "\tau = c_\text{sub}'(\beta)", shift left]
   \\
   \xi
   \arrow[u, shift right]
   \arrow[r, "\text{multiplication}", shift left]
   &
   \mu
   \arrow[l, "\text{division}", shift left]
   \arrow[u, shift left]
   \arrow[r, "\tau = M^T \mu"', shift right]
   &
   \tau
   \arrow[l, shift right]
   \arrow[u, shift left]
\end{tikzcd}
\end{equation}
where (of course) all of the parameters and arrows for the ``left square''
are the same as in \eqref{eq:plethora-four}, where the three of the arrows
for the ``right square'' give parameter transformations discussed in the
preceding section, and the three unlabeled arrows are parameter transformations
that have no closed-form expression.

The inverse mappings to the the mapping $\beta \mapsto \tau$ is calculated
just like the other upward arrows in the diagram.
Define
\begin{equation} \label{eq:logl-aster-expected-submodel}
   \tilde{l}_\text{sub}(\beta) = \inner{\tau, \beta} - c_\text{sub}(\beta)
\end{equation}
as with the other $\tilde{l}$ functions, $\tilde{l}_\text{sub}$ is concave
and strictly concave when the canonical parameter $\beta$ of the unconditional
canonical affine submodel is identifiable, which is when there does not
exist a nonzero direction of constancy, a nonzero vector $\eta$ such that
$\inner{M^T Y, \eta}$ is almost surely constant.
Whenever $\tau$ is a possible value of the mean value parameter vector,
that is, $\tau = E_\beta(M^T Y)$ for some $\beta$, then that $\beta$
is a global maximizer of \eqref{eq:logl-aster-expected-submodel}.
That $\beta$ is the unique maximizer if and only if $\beta$ is identifiable
(no nonzero directions of constancy exist).

As discussed in Section~\ref{sec:dealing} above, we can always deal with
non-uniqueness (non-identifiability) by constraining some components of
$\beta$ to be equal to zero.

The other labeled arrows on the ``right square'' in the diagram are the
inverses of linear transformations, but these generally do not have solutions.

The mapping $\varphi = a + M \beta$ need be neither one-to-one nor onto.
In applications, it is never onto.  That is the whole point of submodels,
to be a \emph{proper} submodel.  It will not be one-to-one if there is
``collinearity'' (if $M$ does not have full column rank).  Let $B$ denote
the full canonical parameter space for the parameter $\beta$ described in
the Theorem~\ref{th:regular-submodel} in Appendix~\ref{app:regular} and its
proof.  Temporarily, let $f$ denote the mapping defined
by $f(\beta) = a + M \beta$.  As we just said, $f$ is one-to-one if and only
if $M$ has full column rank, but $f$ is never onto (in applications).
Thus $f$ is invertible (if one-to-one) only when it is considered a mapping
$B \to f(B)$, that is, its codomain is its range.  Then (assuming one-to-one)
the inverse $f^{-1}$ exists and is a mapping $f(B) \to B$.

But this is a triviality.  It says that if $\varphi = f(\beta)$, then we
know that $\varphi$ is in the domain of the inverse mapping
and $\beta = f^{-1}(\varphi)$.  But otherwise, when just given a $\varphi$
in the saturated model unconditional canonical parameter space, we don't know
whether it is in $f(B)$ or not, since the only way we can know is if we know
it is $f(\beta)$ for some $\beta$.

The mapping $\tau = M^T \mu$ need be neither one-to-one nor onto.
In applications, it is never one-to-one.  Again, that is the whole
point of submodels.  If $M$ is not onto, then $M^T$ is not one-to-one
(considered as the linear transformations these matrices represent).
Thus the equation $\tau = M^T \mu$ never has a unique solution for $\mu$.
The only way to find a $\mu$ that corresponds to a given $\tau$ is to go
the other way around
the square $\tau \mapsto \beta \mapsto \varphi \mapsto \mu$.

For an unconditional canonical affine submodel
\begin{itemize}
\item $\tau$ is always identifiable,
\item $\beta$ may or may not be identifiable, if not, identifiability can
    be forced by constraining some components of $\beta$ to be equal to zero,
\item the set of allowed $\varphi = a + M \beta$ values is either an affine
    subspace of $\real^J$ or an open subset of such an affine subspace,
\item the set of allowed $\mu$ values is a smooth manifold having the same
    dimension as $\beta$, but is a curved manifold since the mapping
    $\varphi \to \mu$ is nonlinear,
\item similarly, the set of allowed $\theta$ values is a smooth manifold
    having the same dimension as $\beta$, and
\item similarly, the set of allowed $\xi$ values is a smooth manifold
    having the same dimension as $\beta$.
\end{itemize}

\subsection{Conditional}

Since all of the theory in the preceding two sections works so smoothly,
it may be surprising that we have a competing view of how to do submodels.
We almost don't have a competing view.  As far as I know, there are only
two published examples of conditional aster analyses.  Example~{1} in
\citet{aster2} was done as a conditional aster analysis but can easily
be redone as an unconditional aster analysis
\citep[Slide Deck~4, slides 47ff.]{geyer-8931-aster}.
The analysis in \citet{aster-aphid} titled
``Relating Plant Fitness to Aphid-Load'' was done as a conditional aster
analysis and had to be so done because time-dependent covariates (in this
case aphid load) do not mesh well with unconditional aster models.
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%

So in rare cases (one so far published, as far as I know) conditional
aster models are necessary.  But in the vast majority of applications
unconditional aster models (which are the default for aster software)
are the ones used and the only ones users consider.

\index{aster model!unconditional|(}
\index{aster model!canonical affine submodel!unconditional|(}
Nevertheless, if we have conditional aster models at all, then we need their
theory.  In \emph{conditional canonical affine submodels} of aster models
we do not use the submodel parameterization \eqref{eq:affine-unconditional}.
Instead we use
\begin{equation} \label{eq:affine-conditional}
   \theta = a + M \beta,
\end{equation}
This is undeniably a TTD (thing to do) but has no theoretical justification.
It does not connect with exponential family theory except, as a smooth
submodel of a regular full exponential family (the saturated model),
this is a \emph{curved exponential family}.
As we shall see, there are a few good properties that are shared by
conditional and unconditional aster models, but only a few.
%%%%%%%%%% NEED FORWARD REFERENCE to likelihood inference %%%%%%%%%%
The theory of conditional aster models is impoverished compared to that
of unconditional aster models.
\index{aster model!unconditional|)}
\index{aster model!canonical affine submodel!unconditional|)}

\subsection{A Plethora of Parameters Re-Revisited}
\label{sec:re-revisited}

The analog of \eqref{eq:plethora-four} or \eqref{eq:plethora-six}
for conditional aster models is
\begin{equation} \label{eq:plethora-five}
\begin{tikzcd}[column sep=9em]
   \beta
   \arrow[r, "\theta = a + M \beta", shift left]
   &
   \theta
   \arrow[r, "\text{aster transform}", shift left]
   \arrow[d, "\xi_G = c_G'(\theta_G)"', shift right]
   \arrow[l, shift left]
   &
   \varphi
   \arrow[l, "\text{inverse aster transform}", shift left]
   \arrow[d, "\mu = c'(\varphi)", shift left]
   \\
   &
   \xi
   \arrow[u, shift right]
   \arrow[r, "\text{multiplication}", shift left]
   &
   \mu
   \arrow[l, "\text{division}", shift left]
   \arrow[u, shift left]
\end{tikzcd}
\end{equation}
There is no analog of the submodel mean value parameter $\tau$
for a conditional aster model (because a conditional aster model is not
a regular full exponential family).

\section{Maximum Likelihood}

\subsection{Exponential Families}

Maximum likelihood estimation in a regular full exponential family is
much like the problem of inverting the parameter transformation from
canonical to mean value parameters discussed in
Section~\ref{sec:calculating-inverse-transformation} above.
We simply maximize $l$ given by \eqref{eq:logl-expfam} rather than
$\tilde{l}$ given by \eqref{eq:logl-expfam-expected}.
Both of these functions are always concave and strictly concave if and
only if the canonical parameterization is identifiable.

A point is a global maximizer of $l$ or $\tilde{l}$ if and only if the first
derivative is equal to zero.  Hence $\theta$ is an MLE
if and only if it satisfies the \emph{observed equals expected property}
\begin{equation} \label{eq:observed-equals-expected-expfam}
   y = E_\theta(Y)
\end{equation}
where the left~hand side is the observed value of the canonical statistic
and the right-hand side is the expected value corresponding to parameter
value $\theta$.  If the canonical parameterization is identifiable (if the
canonical statistic is not concentrated on a hyperplane, if there are no
nonzero directions of constancy), the solution $\theta$
of \eqref{eq:observed-equals-expected-expfam}.

The only difference between the problem of this section and
Section~\ref{sec:calculating-inverse-transformation} above is that
there we \emph{assumed} there was a solution, that is, we assumed
that the $\mu$ in \eqref{eq:logl-expfam-expected} satisfied the
expected equals expected condition (which is a triviality) $\mu = E_\theta(Y)$.
Here we do not assume \eqref{eq:observed-equals-expected-expfam} has a solution.
The data are what they are.  We assume they are possible data under the
statistical model, but we do not assume anything else.

All of the same arguments apply to canonical affine submodels,
only the notation changes.  Replace $y$ by $M^T y$ and replace $\theta$ by
$\beta$.  Thus $\beta$ is a global maximizer of the submodel log likelihood
if and only if
\begin{equation} \label{eq:observed-equals-expected-expfam-submodel}
   M^T y = M^T E_\beta(Y)
\end{equation}

\subsection{Directions of Recession, Existence}
\label{sec:direction-of-recession}

A direction $\delta$ in the vector space where the canonical parameter vector
of an exponential family takes values is called a \emph{direction of recession}
of the log likelihood if
\begin{equation} \label{eq:direction-of-recession}
   \inner{Y, \delta} \le \inner{y, \delta}, \qquad \text{almost surely},
\end{equation}
where $y$ is the observed value of the canonical statistic vector
and $Y$ is a random value of the canonical statistic vector.

Suppose $\delta$ is a nonzero direction of recession that is not a direction
of constancy.  By monotonicity of expectation we have
$$
   E_\theta\{ \inner{Y, \delta} \} < \inner{y, \delta}
$$
for all parameter values $\theta$, because we know $\inner{y - Y, \delta}$
is nonnegative almost surely, so if it had zero expectation, it would have
to be zero almost surely (which would imply that $\delta$ is a direction
of constancy).  Consequently, if $\delta$ is a nonzero
direction of recession that is not a direction of constancy, then
\eqref{eq:observed-equals-expected-expfam} has no solutions and maximum
likelihood estimates for $\theta$ do not exist.
It turns out this is the only way the MLE for a regular full exponential family
can fail to exist.

\citet[Theorem~3]{geyer-gdor} gives many equivalent
characterizations of this concept.  Here are some of them.
In these we use the notation $y$ is the observed value of the
canonical statistic vector, $Y$ is a random value of the
canonical statistic vector,
$\theta$ is the canonical parameter vector, $\delta$ is a vector in
the vector space where $\theta$ takes values, $l$ is the log likelihood
\eqref{eq:logl-expfam}, and $\Theta$ is the full canonical space
\eqref{eq:full-expfam}.
\begin{enumerate}
\item[(a)] For some $\theta \in \Theta$,
    $$
        \limsup_{s \to \infty} l(\theta + s \delta) > - \infty.
    $$
\item[(b)] For all $\theta \in \Theta$,
    the function $s \mapsto l(\theta + s \delta)$ is nondecreasing on the
    whole real line.
\item[(c)] $\inner{Y - y, \delta} \le 0$ almost surely
    for some distribution in the exponential family.
\item[(d)] $\inner{Y - y, \delta} \le 0$ almost surely
    for every distribution in the exponential family.
\end{enumerate}
We arbitrarily picked (c) to serve as the definition,
but, because they are all equivalent
we could have picked any of these to serve as the definition
of direction of recession.
All of these equivalences are a consequence of Theorem~{3} in
\citet{geyer-gdor}, except the implication that (a) implies the others
uses Theorem~{8.6} in \citet{rockafellar}.

By Theorem~{4} in \citet{geyer-gdor} the MLE exists if and only if
every direction of recession is a direction of constancy.
(It is clear from the definitions that every direction of constancy is
a direction of recession.)

By Theorem~{5} in \citet{geyer-gdor} if $\delta$ is a direction of recession
that is not a direction of constancy, then the function
$$
   s \mapsto l(\theta + s \delta)
$$
is strictly increasing on the interval where it is finite, and this is
true for all $\theta \in \Theta$.  This gives us another argument,
besides the argument given above why a direction of recession that is
not a direction of constancy implies nonexistence of an MLE.
But we need Theorem~{4} in \citet{geyer-gdor} for the reverse conclusion
that nonexistence of the MLE cannot occur for any other reason.

All of the same arguments apply to canonical affine submodels,
only the notation changes.  Replace $y$ by $M^T y$ and replace $\theta$ by
$\beta$.  Thus a direction $\delta$ in the vector space where the submodel
canonical parameter $\beta$ takes values is a direction of recession if and
only if $\inner{y - Y, M \beta} \ge 0$ almost surely (for any one distribution
in the submodel and hence for all distributions).  And MLE for $\beta$ exist
if and only if $\inner{y - Y, M \beta} \ge 0$ almost surely
implies $\inner{y - Y, M \beta} = 0$ almost surely.
Moreover, $\beta$ is an MLE if and only if
\eqref{eq:observed-equals-expected-expfam-submodel} holds.

A lot more can be said and will be said on this subject.
%%%%%%%%%% NEED FORWARD REFERENCE to chapter on solutions at infinity %%%%%%%%%%

\subsection{Unconditional Aster Models}

Almost nothing needs to be said in this section that hasn't already been said.
Unconditional canonical affine submodels of aster models are regular full
exponential families (under the conditions of Theorems~\ref{th:regular}
and~\ref{th:regular-submodel} in Appendix~\ref{app:regular}).  So the
penultimate paragraph of the preceding section says it all.

\subsection{Conditional Aster Models}
\label{sec:conditional-aster-model-mle}

Conditional aster models are not regular full exponential families
(only curved exponential families).  At least, they are not when
considered statistically, probabilistically.

But if we play a trick, they are when considered numerically, algebraically.
Rewrite \eqref{eq:logl-aster-theta} as
\begin{equation} \label{eq:logl-aster-theta-tricky}
   l(\theta)
   =
   \sum_{G \in \mathcal{G}}
   \left[ \inner{y_G, \theta_G} - n_{q(G)} c_G(\theta_G) \right]
\end{equation}
and pretend that all of the $n_{q(G)}$ are constants.  This makes no
sense statistically, probabilistically because those $n_{q(G)}$ are actually
$y_{q(G)}$ and some of them are the same variables some components of the
$y_G$ that also appear in this expression.  But if we are just treating
this log likelihood as a numerical, algebraic function to maximize to find
MLE, then this doesn't matter.

We call this the \emph{associated independence model} of the conditional
aster model.  In this model $y$ factorizes as
\begin{equation} \label{eq:factorize-tricky}
   \pr(y) = \prod_{G \in \mathcal{G}} \pr(y_G \mid n_{q(G)})
\end{equation}
where all of the $n_{q(G)}$ are considered constants and not components of $y$.
Then of course, we have the parameter transformation
\eqref{eq:affine-conditional} of the canonical affine submodel.
This is a regular full exponential family (Theorem~\ref{th:regular-tricky}
in Appendix~\ref{app:regular}).

So now we can apply the theory of \citet{geyer-gdor} to the associated
independence model.
\begin{theorem} \label{th:aim-mle}
If $y_{q(G)} = 0$ for any dependence group $G$, replace
the family for this dependence group by the degenerate family concentrated
at zero so $c_G$ is the zero function.
Then $\eta$ is a direction of recession of the associated independence model
if and only if, writing $\delta = M \eta$,
$$
   E_\beta\{ \inner{y_G - Y_G, \delta_G} \mid y_{q(G)} \} \ge 0,
   \qquad \text{for all $G \in \mathcal{G}$ such that $y_{q(G)} > 0$},
$$
and $\eta$ is a direction of constancy of the associated independence model
if and only if
$$
   E_\beta\{ \inner{y_G - Y_G, \delta_G} \mid y_{q(G)} \} = 0,
   \qquad \text{for all $G \in \mathcal{G}$ such that $y_{q(G)} > 0$}.
$$
Then the MLE for $\beta$ exists if and only if every direction of recession
is a direction of constancy, and the MLE is unique if and only if no nonzero
directions of constancy exist.
\end{theorem}
Note that the theorem ignores $\delta_G$ for $G$ such that $y_{q(G)} = 0$,
hence unless the model matrix $M$ is such that $\delta_G = 0$ for all such $G$
there will be non-uniqueness whenever there are such $G$.

\subsection{Fisher Information}

\subsubsection{Exponential Family}

Differentiating twice the log likelihood \eqref{eq:logl-expfam} of
an exponential family we get
$$
   l''(\theta) = - c''(\theta)
$$
and minus this is observed Fisher information, and the expectation is
expected Fisher information.  Since the right-hand side is constant,
observed and expected Fisher information are the same and are given by
either side of \eqref{eq:cumulant-two}.

\subsubsection{Unconditional}

The Fisher information matrix for the saturated model unconditional
canonical parameter $\varphi$ is given by either side
of \eqref{eq:cumulant-two} with $\theta$ replaced by $\varphi$ (because
$\varphi$ is the canonical parameter vector of the (unconditional, joint)
aster model).   Following \citet{aster1}, their equations (17) and (18)
we choose to calculate Fisher information using probability theory
(calculating covariances) rather than calculus (calculating derivatives).

This is done using the factorization and the iterated covariance theorem,
which says for any random variables $X$, $Y$, and $Z$
$$
   \cov(X, Y) = E\{ \cov(X, Y \mid Z) \} + \cov\{E(X \mid Z), E(Y \mid Z)\}
$$
From this, for $i$ and $j$ in the same dependence group $G$
\begin{equation}
\begin{split} \label{eq:variance-unconditional-same-g}
   \cov_\varphi(Y_i, Y_j)
   & =
   E_\varphi\{ \cov_\varphi(Y_i, Y_j \mid Y_{q(G)}) \}
   \\
   & \quad
   + \cov_\varphi\{E_\varphi(Y_i \mid Y_{q(G)}), E_\varphi(Y_j \mid Y_{q(G)})\}
   \\
   & =
   E_\varphi\{ Y_{q(G)} \gamma_{i j} \}
   + \cov_\varphi\{ Y_{q(G)} \xi_i, Y_{q(G)} \xi_j\}
   \\
   & =
   \mu_{q(G)} \gamma_{i j} + \var_\varphi(Y_{q(G}) \xi_i \xi_j
\end{split}
\end{equation}
where $\mu$ and $\xi$ are the unconditional and conditional mean value
parameter vectors, respectively, corresponding to $\varphi$ and we introduce
the notation
$$
   \gamma_{i j}
   =
   \frac{\partial^2 c_G(\theta_G)}{\partial \theta_i \partial \theta_j}
$$
which can also be written
\begin{equation} \label{eq:sigma}
   \gamma_{i j}
   =
   \cov_\varphi(Y_i, Y_j \mid Y_{q(G)} = 1)
\end{equation}
provided the conditioning event does not have probability zero (so the
latter equation makes no sense) but we always have
$$
   \cov_\varphi(Y_i, Y_j \mid Y_{q(G)} ) = Y_{q(G)} \gamma_{i j}
$$
by the predecessor-is-sample-size property
(this was used in deriving \eqref{eq:variance-unconditional-same-g}).

For future use we also define $\gamma_{i j} = 0$ when $i$ and $j$ are
not in the same dependence group.  This makes sense because if $i \in G$
and $j \in H$ and $G \neq H$
$$
   \frac{\partial^2 c_G(\theta)}{\partial \theta_i \partial \theta_j}
   =
   \frac{\partial^2 c_H(\theta)}{\partial \theta_i \partial \theta_j}
   =
   0
$$

And, for $i$ and $j$ not in the same dependence group, say $i \in G$
and $j \in H$ and $G < H$ in the order on $\mathcal{G}$ asserted to exist
by Theorem~\ref{th:factorize},
\begin{equation}
\begin{split} \label{eq:variance-unconditional-not-same-g}
   \cov_\varphi(Y_i, Y_j)
   & =
   E_\varphi\{ \cov_\varphi(Y_i, Y_j \mid Y_{q(G)}) \}
   \\
   & \quad
   + \cov_\varphi\{E_\varphi(Y_i \mid Y_{q(G)}), E_\varphi(Y_j \mid Y_{q(G)})\}
   \\
   & =
   \cov_\varphi\{E_\varphi(Y_i \mid Y_{q(G)}), E_\varphi(Y_j \mid Y_{q(G)})\}
   \\
   & =
   \cov_\varphi\{\xi_i Y_{q(G)}, E_\varphi(Y_j \mid Y_{q(G)})\}
   \\
   & =
   \xi_i \cov_\varphi\{Y_{q(G)}, E_\varphi(Y_j \mid Y_{q(G)})\}
   \\
   & =
   \xi_i E_\varphi\{[Y_{q(G)} - \mu_{q(G)}] E_\varphi(Y_j \mid Y_{q(G)})\}
   \\
   & =
   \xi_i E_\varphi\{[Y_{q(G)} - \mu_{q(G)}] Y_j \}
   \\
   & =
   \xi_i \cov_\varphi(Y_{q(G)}, Y_j)
   \\
   & =
   \xi_i \cov_\varphi(Y_{p(i)}, Y_j)
\end{split}
\end{equation}
where the second equality is the Markov property (Theorem~\ref{th:markov}
in Appendix~\ref{app:markov}): $Y_i$ and $Y_j$ are conditionally independent
given $Y_{q(G)}$.

Because covariances are symmetric in their arguments, we do not need to
do the case $G > H$.

It should be clear that we can calculate the whole
Fisher information matrix using \eqref{eq:variance-unconditional-same-g}
and \eqref{eq:variance-unconditional-not-same-g} traversing the full
aster graph in any order that visits predecessors before successors
(the inverse of the order in Theorem~\ref{th:factorize}).

For computational efficiency, we should note that if $i$ and $j$ are
nodes of the full aster graph for different ``individuals'' in scare quotes
(defined in Section~\ref{sec:scare-quotes} above), then they are
unconditionally independent by Corollary~\ref{cor:markov}
in Appendix~\ref{app:markov}, so
$$
   \cov(Y_i, Y_j) = 0
$$
in this case.  The Fisher information matrix for $\varphi$ is block diagonal
with the blocks being for ``individuals'' (in scare quotes).

Now we revert to calculus to find the Fisher information matrix for the
unconditional submodel canonical parameter $\beta$.
The map $\beta \mapsto \varphi$ given by \eqref{eq:affine-unconditional}
has derivative $M$ the model matrix, that is, if $m_{i j}$ are components
of $M$, then
$$
   \frac{\partial{\varphi_i}}{\partial \beta_j} = m_{i j}.
$$
It follows from the chain rule that
$$
   c_\text{sub}''(\beta) = M^T c''(\varphi) M
$$
or
$$
   I(\beta) = M^T I(\varphi) M
$$
where we abuse notation using $I$ for both Fisher information for $\beta$
and Fisher information for $\varphi$.

\subsubsection{Conditional}

Differentiating twice the log likelihood \eqref{eq:logl-aster-theta} for
the conditional canonical parameter vector $\theta$ of
an aster model we get
$$
   \frac{\partial^2 l(\theta)}{\partial \theta_i \partial \theta_j}
   =
   - \sum_{G \in \mathcal{G}} y_{q(G)}
   \frac{\partial^2 c_G(\theta_G)}{\partial \theta_i \partial \theta_j}
   =
   - \sum_{G \in \mathcal{G}} y_{q(G)} \gamma_{i j}
$$
Since this is random, observed and expected Fisher information are not
the same.  Also noting that $\gamma_{i j} = 0$ unless $i, j \in G$ we have
the following.
\begin{itemize}
\item The observed Fisher information matrix for $\theta$ is block diagonal
    with nonzero blocks corresponding to the dependence groups.  The $i, j$
    component is $y_{q(G)} \gamma_{i j}$ for $G$ such that $i, j \in G$ and
    zero otherwise.
\item The expected Fisher information matrix for $\theta$ is similarly
    block diagonal.  The $i, j$
    component is $\mu_{q(G)} \gamma_{i j}$ for $G$ such that $i, j \in G$ and
    zero otherwise.
\end{itemize}

If in either case we denote this matrix by $I(\theta)$, then
the Fisher information matrix for $\beta$ is
$$
   I(\beta) = M^T I(\theta) M
$$
for the same reason as in the preceding section (and $I$ denotes the observed
Fisher information matrix in both instances or the expected
Fisher information matrix in both instances).

