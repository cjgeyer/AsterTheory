
\chapter{Introduction}
\label{ch:introduction}

\section{Background}

Aster models \citep*{aster1,aster2,reaster} \index{aster model}
are parametric statistical models
specifically designed for life history analysis.  They are exponential family
models that generalize generalized linear models (GLM) that are also
exponential family models (for example, logistic regression and
Poisson regression with log link) in two ways
\begin{itemize}
\item in GLM components of the response vector are
    necessarily conditionally independent given covariate data
    but in aster models they need not be, and
\item in GLM the conditional distributions of components of the
    response vector given covariate data all come from the same family
    but in aster models they need not.
\end{itemize}
As generalizations of GLM, aster models are also regression models.
They model the conditional distribution of the response vector given
covariate data.  The marginal distribution of covariate data is not
modeled.

In life history analysis, \index{life history analysis}
the data are about survival and reproduction
of biological organisms.  Thus aster models also generalize discrete time
survival analysis (aster models model not only survival but also
what happens conditional on survival).
Aster models unify many disparate kinds of life history analysis that have
appeared in the biological literature: comparison of Darwinian fitness between
various groups \citep{aster1,aster2}, estimation of fitness landscapes
(\citealp{lande-arnold}; \citealp{aster2,aster3}, \citealp*{aster-hornworm}),
Leslie matrix analysis
\citep{caswell}, life table analysis in demography \citep{goodman},
and estimation of population growth rates
\citep{fisher,lenski-service,aster2,aster-hornworm}.
Aster models also generalize zero-inflated Poisson regression \citep{lambert},
negative binomial regression (overdispersed Poisson regression),
and zero-inflated negative binomial regression.

Aster models are a special case of graphical models \citep{lauritzen}.
In particular, they are statistical models for which the joint distribution
of the response vector factorizes completely as a product of marginal and
conditional distributions (equation~\eqref{eq:factorize} below).
This makes aster models a special case of chain graph models
\citep[Sections~2.1.1 and~3.2.3]{lauritzen}.
Aster models also have the predecessor-is-sample-size property
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
that makes the joint distribution of the response vector an exponential
family.  This property can be seen to generalize unnamed properties
of survival analysis, life-table analysis, and Leslie matrix analysis.

\section{Software}
\label{sec:software}

Currently, all software for aster models is written in the R statistical
computing language \citep{r-core}.  There are two CRAN
(\url{cran.r-project.org}) packages, \code{aster} \citep{aster-package} and
\code{aster2} \citep{aster2-package}.
\index{R package!aster@\code{aster}}
\index{R package!aster2@\code{aster2}}

Both R and these packages can be installed in minutes on any computer,
so any user can get started with aster models in almost no time.
R package \code{aster} is the most complete.
It does everything except dependence groups
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
and limiting conditional models.
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%

R package \code{aster2} is the very incomplete.
It does do dependence groups and limiting conditional models, but everything
else is either missing or much harder to use than in R package \code{aster}.

So any aster model that can be done with R package \code{aster} should
be done with that package.

\section{Vectors and Subvectors}

We adopt a notation from \citet{lauritzen} for subvectors, but fuss about it
more.  As in set theory \citep[Section~8]{halmos-set-theory}, if $A$ and $B$
are sets, then $A^B$ denotes the set of all functions $B \to A$.
In particular, if $J$ is a finite set, then we let $\real^J$ denote
the set of all functions $J \to \real$.  This set can also be considered
a finite-dimensional vector space.  That functions $J \to V$ where $J$ is
any set and $V$ is a field or a vector space can be considered
vectors is the reason the study of infinite-dimensional topological vector
spaces is called functional analysis.

Another way of looking at this distinction is that the usual view of
finite-dimensional vector spaces is that they are $\real^d$ for some
natural number $d$, which is tantamount to insisting that the index
set for vectors in this space must be the set $\{1, \ldots, d\}$.
Here we are saying the index set can be any finite set $J$.

Even though we consider vectors to be functions, we write evaluation
of these functions $y_j$ so it looks like usual notation.  We even say
that $y_j$ is a component of the vector $y$ rather than the value of
the function $y$ at the point $j$.  But behind the scenes our vectors
are also functions, and we could write $y(j)$ instead of $y_j$.

We need a notation for subvectors of a vector.  If $y$ is an element
of $\real^J$ and $A \subset J$, then we let $y_A$ denote the restriction
of $y$ to the set $A$.
As such, it is an element of the vector space $\real^A$.
Like all functions, it knows its domain and codomain.
It knows it is a function $A \to \real$.
So it knows its components are $y_j$, $j \in A$.
And these are also the components of $y$ for $j \in A$.
Since the components of $y_A$ are a subset of the components of $y$,
we say $y_A$ is a \emph{subvector} of $y$.
\index{subvector}

If we were to insist that all vectors, including subvectors, have
index sets $\{1, \ldots, k\}$ for some natural number $k$.  Then we could
not distinguish different subvectors of the same length, or at least could
not without ugly and cumbersome extra decoration of the notation.

Our notation does have the drawback that we have only the convention
that lower case letters denote elements of sets and upper case letters
denote sets, which is why $y_j$ is clearly a component of a vector or subvector
(the value of a function at the index $j$) and $y_A$ is a subvector
(so is still a function, not the value of a function).
We also consider any subscript notation that clearly denotes a set
as indicating a subvector, for example, $y_{\{1, 3, 5\}}$ or $y_{\{j\}}$ or
$y_{\set{ j \in J : j \prec i }}$.

\section{Regression Notation}

Strictly speaking, in regression theory, every probability and expectation
is conditional on covariate data, at least on the part of the covariate data
that is considered random rather than fixed by the design of the experiment.
Thus to be hyperpedantic, we should always write
\begin{gather*}
   E(y_A \mid \text{the part of covariate data that is random})
   \\
   \Pr(y_A \in B \mid \text{the part of covariate data that is random})
\end{gather*}
rather than $E(y_A)$ or $\Pr(y_A \in B)$.  But, like most regression books,
we will not do this.  The dependence of probabilities and expectations on
covariates is usually not made explicit in the notation.

This is especially important in aster models when components of the response
vector depend on the values of other components, so we frequently write
\begin{gather*}
   E(y_A \mid y_j)
   \\
   \Pr(y_A \in B \mid y_j)
\end{gather*}
and the like.  And we do not want this dependence confused with dependence
on covariate data.

When necessary for clarity, as in the discussion of fitness landscapes,
which are regression functions,
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
we can explicitly denote the dependence on covariate data in conditional
probabilities and expectations.

\section{Factorization}
\label{sec:factorization}

If $J$ is the index set of the response vector $y$ of an aster model,
then there is a partition $\mathcal{G}$ of $J$
and a function $q : \mathcal{G} \to N$, where $N \supset J$, such that
\index{aster model!property!factorization}
\index{factorization|seeunder{aster model}}
the joint distribution of $y$ factorizes as
\begin{equation} \label{eq:factorize}
   \pr(y) = \prod_{G \in \mathcal{G}} \pr(y_G \mid y_{q(G)})
\end{equation}

In this factorization, each component $y_j$ of the response vector $y$
appears exactly once ``in front of the bar'' in a conditional on
the right-hand side (because $\mathcal{G}$ is a partition of $J$ so
each $j \in J$ is in exactly one $G \in \mathcal{G}$).
So every component of $y$ is treated as random (the joint distribution
of $y$ is modeled).
Random variables $y_{q(G)}$ that appear ``behind the bar'' in a conditional
on the right-hand side may or may not be elements of $y$.  They are not
if $q(G) \notin J$.  The distribution of such random variables is not
modeled by \eqref{eq:factorize}.  So they are treated as constant random
variables.

We say \eqref{eq:factorize} is \emph{valid} if what are denoted as
conditional distributions on the right-hand side agree with the conditional
distributions derived from the left-hand side (the joint distribution) by
the usual operations of probability theory.
\begin{theorem} \label{th:factorize}
The factorization \eqref{eq:factorize} is valid if and only if
the partition $\mathcal{G}$ can be totally ordered
by some total ordering $<$ such that $q(G) \in H$ implies $G < H$.
\end{theorem}
A proof of this theorem is straightforward and given
in Appendix~\ref{app:factorize}.
It could also be derived from the discussion of chain graph models
in \citet[equation~3.23]{lauritzen}.

In \eqref{eq:factorize} we have been deliberately vague about what $\pr$ is
supposed to mean, since there are many ways to specify probability
distributions and any of them will do.
\begin{itemize}
\item If $y$ is a discrete random vector,
      then $\pr$ could denote probability mass functions.
\item If $y$ is a continuous random vector,
      then $\pr$ could denote probability density functions.
\item If $y$ is a partly discrete and partly continuous continuous
      random vector (either some components discrete and some components
      continuous or some components a mixture of discrete and continuous)
      then $\pr$ could denote probability mass-density functions.
\item No matter what, $\pr$ could denote cumulative distribution functions.
\item No matter what, $\pr$ could denote probability measures.
\end{itemize}
In any of these cases the multiplication indicated in \eqref{eq:factorize}
is actual multiplication of real-valued thingummies.

The total order asserted to exist by the theorem need not be unique and
usually is not unique.
We can find such a total order using the algorithm called topological sort
\index{topological sort}
\citep[Section~6.6]{aho-et-al} Using R function \code{tsort} in R package
\code{pooh}
\citep{pooh-package}
\index{R package!pooh@\code{pooh}}
for each $G \in \mathcal{G}$ such that there exists
a (necessarily unique) $H \in \mathcal{G}$ such that $q(G) \in H$
let $G$ be a component of the vector \code{from} that is an argument to
\code{tsort} and
let $H$ be the corresponding component of the vector \code{to} that is another
argument to \code{tsort} and
neither vector has any other components.  Then invoking \code{tsort} with
these \code{from} and \code{to} arguments and \code{domain} argument that is
$\mathcal{G}$ strung out in a vector in any order
will determine a (not necessarily unique) total order that agrees with
Theorem~\ref{th:factorize}.  If the user has made a mistake and incorrectly
specified the $q$ function so there is no total order that satisfies
Theorem~\ref{th:factorize}, then \code{tsort} will give an error.

Current code in R packages \code{aster} and \code{aster2} does not
actually use the topological sort algorithm but rather forces the user
to input the data so that the numerical order of the components of the
response vector is the total order, that is, considering the index set
of the response vector to be $\{1, \ldots, n\}$ for some integer $n$
current code requires $q(G) < j$ for any $j \in G$.
It is up to the user to present the data in this way.  The computer is no help.
But we could make the computer figure this out in future versions of the
software.

\section{Further Factorization}

In \citet{lauritzen} chain graph factorizations like our \eqref{eq:factorize}
and his equation (3.23) can be further factorized, his equation (3.24).
But in aster model theory, we shall never be interested in such further
factorizations (even in cases where they are possible) and never use any
notation that allows for them.  So we will never have an analog of equation
(3.24) in \citet{lauritzen}.  For us, factorization is \eqref{eq:factorize}.

\section{Graphs}

Each factorization goes with a graph \citep[Section~3.2.3]{lauritzen}.
\index{aster graph}
\index{graph|see{aster graph}}
The nodes of the graph are either the elements of $N$ or the components
of $y$ corresponding to these elements ($y_j$ for $j \in N$).
There is a directed edge, also called an \emph{arrow},
\index{node}
\index{edge}
\index{arrow}
\index{line}
$q(G) \longrightarrow j$ (or if one prefers $y_{q(G)} \longrightarrow y_j$)
for every $G \in \mathcal{G}$ and every $j \in G$.
There is an undirected edge, also called a \emph{line},
$j \myline k$ (or if one prefers $y_j \myline y_k$)
for every $G \in \mathcal{G}$ and every $j, k \in G$ such that $j \neq k$.

As we have just seen, the function $q$ determines the graph
(the function $q$ knows its domain $\mathcal{G}$).
Conversely, the graph determines the function $q$.
\begin{itemize}
\item The set $J = \bigcup \mathcal{G}$ is the set of nodes of the graph
    that have incoming arrows (as we shall see, these nodes are called
    non-initial).
\item The elements of $\mathcal{G}$ are the maximal connected components
    of the graph of lines having nodeset $J$.
    This includes any singleton sets of $J$ that have no incoming lines.
\item The graph of $q$ is determined by the arrows: $(G, q(G))$ is an
    argument-value pair whenever there is an arrow $j \longrightarrow k$
    with $j = q(G)$ and $k \in G$.
\end{itemize}

Thus we can reason with with graphs or with $q$ functions (which we will
soon learn to call \emph{predecessor functions}).
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
Graphs can be helpful, but we do not have to use them.

\section{Graphical Terminology}
\label{sec:graphical-terminology}

In aster theory, we say
\begin{itemize}
\item a node is \emph{initial} if it has no incoming arrows
\index{node!initial}
\index{initial node|seeunder{node}}
    or lines (when thinking about the graph) or if it is not an element
    of $J = \bigcup \mathcal{G}$ (when thinking about the function $q$),
\item a node is \emph{terminal} if it has no outgoing arrows
\index{node!terminal}
\index{terminal node|seeunder{node}}
    (it may have outgoing lines and will have outgoing lines if it is
    an element of an element of $\mathcal{G}$ that is not a singleton set),
\item if there is an arrow $j \longrightarrow k$, then we say that $j$
    is the \emph{predecessor} of $k$ (or $y_j$ is the predecessor of $y_k$),
\index{node!predecessor}
\index{predecessor node|seeunder{node}}
\item and, conversely, that $k$ is a \emph{successor} of $j$
    (or $y_k$ is the successor of $y_j$).
\index{node!successor}
\index{successor node|seeunder{node}}
\end{itemize}

In mainstream graphical model theory, a different terminology is more widely
used \citep{lauritzen} root = initial, leaf = terminal, parent = predecessor,
child = successor.  We do not use this terminology in aster model theory
because it can cause serious confusion in biological applications.

As a general policy, we eschew all terminology based on biological analogies
when there is an available alternative (even when that alternative is less
popular).

In any aster graph every node has at most one predecessor and all nodes in
the same $G \in \mathcal{G}$ must have the same predecessor (because $q$
is a function that takes elements of $\mathcal{G}$ as arguments).

In mainstream graph theory, a chain graph with only arrows (no lines) having
the at-most-one-predecessor property is called a \emph{tree} and a disjoint
\index{aster model!property!at most one predecessor}
union of trees is called a \emph{forest}, but we do not use this terminology
either (avoiding serious confusion when the application involves data on
real trees in real forests).  It is enough to say that aster graphs
have the at-most-one-predecessor property.

In mainstream graph theory, there is a term \emph{ancestor} that means
predecessor, or predecessor of predecessor,
or predecessor of predecessor of predecessor,
or predecessor of predecessor of predecessor of predecessor,
or the same with arbitrarily many repetitions of ``predecessor of.''
And there is a converse term \emph{descendant}, that is, $i$ is an ancestor
of $j$ if and only if $j$ is a descendant of $i$.

In aster model theory we avoid these terms too (avoiding confusion when
the application involves real biological organisms with real biological
ancestors and real biological descendants).  If we need the concepts,
then we use the long-winded descriptions
predecessor of predecessor of predecessor and so forth or
successor of successor of successor and so forth.
Fortunately, we rarely need these concepts.
And when we do need these concepts we can avoid the cumbersome verbiage
by using mathematical notation introduced in the next section.

Finally, we need a term for $\mathcal{G}$ and its elements.
The terminology we have been using in our writings about aster models is
elements of $\mathcal{G}$ are \emph{dependence groups}.
\index{dependence group}
The mainstream graphical models terminology \citep{lauritzen} is
\emph{chain components}.  Both have two words and four syllables.
Neither is very elegant.  We don't like the ``chain'' terminology because
we are not using general chain graph theory (aster models are very special
chain graphs).  Our term \emph{dependence group} is not great, but we haven't
thought of a better term.

\section{The Other Predecessor Function}

It is useful to have not only the set-to-index predecessor function $q$
defined in Section~\ref{sec:factorization} above but also the index-to-index
\index{predecessor function!set-to-index}
\index{predecessor function!index-to-index}
predecessor function $p$ defined as follows
$$
   p(j) = k \ifandonlyif j \in G \in \mathcal{G} \opand q(G) = k.
$$
Clearly, $q$ determines $p$.
The converse is not true because $p$ knows nothing about dependence groups.
But $p$ and $\mathcal{G}$ together determine $q$.

\section{The Transitive Closure of the Predecessor Relation}
\label{sec:closure}

The \emph{predecessor relation} on $N$ is the function $p$ thought
of as a relation, that is, thinking set-theoretically
\citep[Section~7]{halmos-set-theory} as the set
$$
   \set{ (j, p(j)) : j \in J }
$$
of its argument-value pairs.

We need a notation from dynamical systems theory for repeated application
of a function.  If $f$ is any function whose domain and codomain are the same,
then it makes sense to compose $f$ with itself.  Then we let $f^0$ denote
the identity function on the domain of $f$, let $f^1 = f$, $f^2 = f \circ f$,
and, in general, $f^{n + 1} = f^n \circ f$.
So
\begin{align*}
   f^0(x) & = x
   \\
   f^1(x) & = f(x)
   \\
   f^2(x) & = f(f(x))
   \\
   f^3(x) & = f(f(f(x)))
\end{align*}
and so forth.

The transitive closure of the predecessor relation
is the smallest transitive relation $R$ containing it.
\index{predecessor relation!transitive closure}
As with most relations, we prefer denoting this relation by infix notation:
saying $j \succ k$ rather than $(j, k) \in R$, that is, $j \succ k$ means
$k = p^n(j)$ for some positive integer $n$.

\begin{theorem} \label{th:transitive-closure}
Under the conditions of Theorem~\ref{th:factorize},
the transitive closure of the predecessor relation is a strict partial order.
\end{theorem}
\begin{proof}
If $j \succ k$, then $j \in G$ for some $G \in \mathcal{G}$ and
$k = p^n(q(G))$ for some natural number $n$ ($n = 0$ is allowed).

If $k \in H$ for some $H \in \mathcal{G}$,
then we have $G < H$ in the total ordering
that Theorem~\ref{th:factorize} uses.
Hence we cannot also have $k \succ j$ because that would imply $G < H$
and $H < G$ contradicting $<$ being a strict total order.

If $k \notin H$ for any $H \in \mathcal{G}$ then $k$ has no predecessor
($k$ is initial) and we cannot have $m \succ k$ for any node $m$.

In either of the preceding cases we never have $k \succ j$ and $j \succ k$.
Since $\succ$ is a transitive relation by definition, it is
a strict partial order \citep[Section~14]{halmos-set-theory}
\end{proof}
\begin{corollary} \label{cor:compatible}
The transitive closure of the predecessor relation is compatible with
the total order on the family of dependence groups defined
in Theorem~\ref{th:factorize} in the sense that
$j \in G \in \mathcal{G}$ and $k \in H \in \mathcal{G}$ and $j \succ k$
implies $G < H$.
\end{corollary}

The non-strict counterpart of this relation
is the reflexive transitive closure of the predecessor relation,
\index{predecessor relation!reflexive transitive closure}
which is denoted $\succeq$.
We have $j \succeq k$ if and only if $j \succ k$ or $j = k$.

The inverse of a relation $R$ considered as a set of argument-value pairs
reverses the order in the pairs, that is $(k, j) \in R^{- 1}$ if and only
if $(j, k) \in R$.
As usual, we denote the inverse of a relation by turning its infix notation
around: $\prec$ is the inverse of $\succ$ and $\preceq$ is the inverse
of $\succeq$.

The inverse of the predecessor relation is the successor relation,
so $\prec$ is the transitive closure of the successor relation
and $\preceq$ is the reflexive transitive closure of the successor relation.
\index{successor relation!transitive closure}
\index{successor relation!reflexive transitive closure}
\index{transitive closure|seeunder{predecessor relation}}
\index{transitive closure|seeunder{successor relation}}
\index{reflexive closure|seeunder{predecessor relation}}
\index{reflexive closure|seeunder{successor relation}}

The choice of whether the transitive closure of the predecessor relation
is denoted $\succ$ or $\prec$ is arbitrary.  Either choice works so long
as one keeps straight which is which.  Our choice is influenced by an
arbitrary choice in the source code for R package \texttt{aster}.  When
the predecessor function is encoded (as the argument \texttt{pred} to the
R function \texttt{aster}) it is required that predecessors have lower indices
than successors (come before them in the \texttt{pred} vector).  Thus we
want to think of predecessors as ``less than'' successors in some sense.
Hence our decision to make $p(j) \prec j$.

In graphical model theory,
$\succ$ is called the ancestor relation,
$\prec$ the descendant relation,
$\succeq$ the ancestor-or-self relation, and
$\preceq$ the descendant-or-self relation.
But, as stated in Section~\ref{sec:graphical-terminology} above,
our policy is to avoid these terms
to avoid confusion in biological applications.
If we need words rather than symbols, we have to use the long winded ones:
``reflexive transitive closure of the predecessor relation'' and so forth.

\section{Two Kinds of Aster Graphs}

The graphs for aster models are often very large with thousands or tens of
thousands of nodes, but usually they are composed of isomorphic subgraphs.
So drawing one of these isomorphic subgraphs is enough.
If you've seen one, you've seen them all.
(Graphs are isomorphic if a drawing of one can be laid on a drawing of the
other with everything --- nodes, lines, and arrows --- matching up.)

An aster graph need not be composed of all isomorphic subgraphs,
but the only published example of that is, as far as I know,
\citet{aster-hornworm}.

To distinguish these two kinds of graphs, we call the aster graph described
in the preceding section the \emph{full aster graph} (we consider the ``full''
redundant but the emphasis may help avoid confusion).

Certain subgraphs of the full aster graph, we then call graphs
for ``individuals'' (in scare quotes for reasons to be explained presently).
These are easier to recognize than describe.

Current aster software (Section~\ref{sec:software} above) forces
$q(G) \neq q(H)$ whenever $G \neq H$ and $q(G)$ and $q(H)$ are initial nodes.
In this case, the graph for an ``individual'' (in scare quotes)
consists of the subgraph consisting of one initial node and all of its
successors or successors of successors or successors of successors
of successors and so forth with arbitrarily many repetitions
of ``successors of'' and all of the arrows and lines in the full graph
connecting these nodes.  (This is where the term ``descendant'' in its
graph-theoretic sense would
come in handy if we allowed ourselves to use it.  The graph for an
``individual'' consists of one initial node, all of its descendant nodes,
and all of the lines and arrows going between these nodes.  But once we
have the idea of the graph for an ``individual'' we no longer need the
term ``descendant.'')

But aster theory as described so far does not force this convention.
If $y_j = 1$, for all initial nodes $j$, which is the case with most
(but not all) aster applications, then it would do no harm if all initial
nodes were fused into one initial node.  That would invalidate nothing but
the way we just described graphs for ``individuals'' (in scare quotes).

Thus we have to be a bit more careful.  If $G$ is a dependence group whose
predecessor $q(G)$ is initial, then the graph for the ``individual''
(in scare quotes) containing $G$ consists of $q(G)$, the nodes in $G$
and their successors or successors of successors or successors of successors
of successors and so forth with arbitrarily many repetitions
of ``successors of'' and all of the arrows and lines in the full graph
connecting these nodes.  (And it would make this definition a little shorter
if we allowed ourselves to use the word ``descendant'' in its graph-theoretic
sense.)

There are two reasons why the scare quotes.
\begin{itemize}
\item In life history
analysis, the graph for an ``individual'' ideally goes one or more times
around the life cycle (exactly).  Thus it may involve data not only for
one biological individual but also for its offspring and perhaps offspring
of offspring (if the experiment goes twice around the life cycle) or even
perhaps more remote descendants (where here ``descendants'' means real
biological descendants, not the graphical models idea of descendants).
\item If the value of the constant $y_j$ at the initial node of the
graph for an ``individual'' is greater than one, then the data for this
``individual'' is actually cumulative data for $y_j$ real biological
individuals.
\end{itemize}

%%%%%%%%%% NEED FORWARD REFERENCE to example graphs %%%%%%%%%%

If one does not like our terminology of ``individual'' in scare quotes,
our advice is to just explain what data the graph is for.  It may actually
be for a biological individual, for a biological individual
and its offspring, or $n$ biological individuals.  Just say what it is.

Or we could use the following characterization.
\begin{theorem}
Subgraphs for ``individuals'' are maximal stochastically independent
subvectors of the response vector.
\end{theorem}
\begin{proof}
Define
$$
   \mathcal{H} = \set{ G \in \mathcal{G} : q(G) \notin J }
$$
and for $H \in \mathcal{H}$
$$
   A_H = \set{ j \in J : (\exists k \in H)(j \succeq k) }.
$$
The elements of $\mathcal{H}$ are disjoint because the elements of
$\mathcal{G}$ are disjoint.
The elements of $\set{A_H : H \in \mathcal{H}}$ are disjoint
because, if $i \in A_{H_1} \cap A_{H_2}$, then $p^{n_1}(i) \in H_1$
and $p^{n_2}(i) \in H_2$ for some natural numbers $n_1$ and $n_2$
using the notation defined in the proof of Theorem~\ref{th:transitive-closure}.
But then $n_1 \le n_2$ implies $A_{H_1} \subset A_{H_2}$ hence $H_1 = H_2$,
hence $A_{H_1} = A_{H_2}$ and the same with 1 and 2 swapped.
Furthermore $\set{A_H : H \in \mathcal{H}}$ is a partition of $J$ because
$J$ is a finite set (aster graphs are not allowed to be infinite).
Hence for every $j \in J$ there exists a natural number $n$ such that
$p^n(j)$ is an initial node.


Furthermore each $G \in \mathcal{G}$ is contained some $A_H$.
If $G \in \mathcal{H}$, then $G \subset A_G$.

Now \eqref{eq:factorize} implies
$$
   \pr(y)
   =
   \prod_{G \in \mathcal{G}} \pr(y_G \mid y_{q(G)})
$$
\end{proof}

