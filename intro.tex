
\chapter{Introduction}
\label{ch:introduction}

\section{Background}
\label{sec:background}

Aster models \citep*{aster1,aster2,reaster} \index{aster model}
are parametric statistical models
specifically designed for life history analysis.  They are exponential family
models that generalize generalized linear models (GLM) that are also
exponential family models (for example, logistic regression and
Poisson regression with log link) in two ways
\begin{itemize}
\item in GLM components of the response vector are
    necessarily conditionally independent given covariate data
    but in aster models they need not be, and
\item in GLM the conditional distributions of components of the
    response vector given covariate data all come from the same family
    but in aster models they need not.
\end{itemize}
As generalizations of GLM, aster models are also regression models.
They model the conditional distribution of the response vector given
covariate data.  The marginal distribution of covariate data is not
modeled.

In life history analysis, \index{life history analysis}
the data are about survival and reproduction
of biological organisms.  Thus aster models also generalize discrete time
survival analysis (aster models model not only survival but also
what happens conditional on survival).
Aster models unify many disparate kinds of life history analysis that have
appeared in the biological literature: comparison of Darwinian fitness between
various groups \citep{aster1,aster2}, estimation of fitness landscapes
(\citealp{lande-arnold}; \citealp{aster2,aster3}, \citealp*{aster-hornworm}),
Leslie matrix analysis
\citep{caswell}, life table analysis in demography \citep{goodman},
and estimation of population growth rates
\citep{fisher,lenski-service,aster2,aster-hornworm}.
Aster models also generalize zero-inflated Poisson regression \citep{lambert},
negative binomial regression (overdispersed Poisson regression),
and zero-inflated negative binomial regression.

Aster models are a special case of graphical models \citep{lauritzen}.
In particular, they are statistical models for which the joint distribution
of the response vector factorizes completely as a product of marginal and
conditional distributions (equation~\eqref{eq:factorize} below).
This makes aster models a special case of chain graph models
\citep[Sections~2.1.1 and~3.2.3]{lauritzen}.
Aster models also have the predecessor-is-sample-size property
(Section~\ref{sec:piss} below)
that makes the joint distribution of the response vector an exponential
family.  This property can be seen to generalize unnamed properties
of survival analysis, life-table analysis, Leslie matrix analysis,
and population growth rate analysis (Section~\ref{sec:mu-and-xi} below).

\section{Software}
\label{sec:software}

Currently, all software for aster models is written in the R statistical
computing language \citep{r-core}.  There are two CRAN
(\url{cran.r-project.org}) packages, \code{aster} \citep{aster-package} and
\code{aster2} \citep{aster2-package}.
\index{R package!aster@\code{aster}}
\index{R package!aster2@\code{aster2}}
Both R and these packages can be installed in minutes on any computer,
so any user can get started with aster models in almost no time.

R package \code{aster} is the most complete.
It does everything except dependence groups
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
and limiting conditional models.
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%

R package \code{aster2} is the very incomplete.
It does do dependence groups and limiting conditional models, but everything
else is either missing or much harder to use than in R package \code{aster}.

So any aster model that can be done with R package \code{aster} should
be done with that package.

\section{Summary}

Aster models combine three ideas
\begin{itemize}
\item factorization of a joint distribution into a product of marginals
    and conditionals (Section~\ref{sec:factorization} below),
\item the predecessor is sample size property (Section~\ref{sec:piss} below),
    which says the conditioning variables in the conditional distributions
    in the factorization act like sample sizes, and
\item the exponential family property (Section~\ref{sec:aster-expfam} below),
    which says the conditional distributions in the factorization are
    exponential families of distributions,
\end{itemize}
into one big idea.  Together the make the joint distribution of the
response vector also an exponential family.  And this makes aster models
as well-behaved as generalized linear models or log-linear models for
categorical data analysis, even though they are far more more complicated.

\section{Vectors and Subvectors}
\label{sec:subvector}

We adopt a notation from \citet{lauritzen} for subvectors, but fuss about it
more.

As in set theory \citep[Section~8]{halmos-set-theory}, if $A$ and $B$
are sets, then $A^B$ denotes the set of all functions $B \to A$.
In particular, if $J$ is a finite set, then we let $\real^J$ denote
the set of all functions $J \to \real$.  This set can also be considered
a finite-dimensional vector space.  That functions $J \to V$ where $J$ is
any set and $V$ is a field or a vector space can be considered
vectors is the reason the study of infinite-dimensional topological vector
spaces is called functional analysis.

Another way of looking at this distinction is that the usual view of
finite-dimensional vector spaces is that they are $\real^d$ for some
natural number $d$, which is tantamount to insisting that the index
set for vectors in this space must be the set $\{1, \ldots, d\}$.
Here we are saying the index set can be any finite set $J$.

Even though we consider vectors to be functions, we write evaluation
of these functions $y_j$ so it looks like usual notation.  We even say
that $y_j$ is a component of the vector $y$ rather than the value of
the function $y$ at the point $j$.  But behind the scenes our vectors
are also functions, and we could write $y(j)$ instead of $y_j$.

We need a notation for subvectors of a vector.  If $y$ is an element
of $\real^J$ and $A \subset J$, then we let $y_A$ denote the restriction
of $y$ to the set $A$.
As such, it is an element of the vector space $\real^A$.
Like all functions, it knows its domain and codomain.
It knows it is a function $A \to \real$.
So it knows its components are $y_j$, $j \in A$.
And these are also the components of $y$ for $j \in A$.
Since the components of $y_A$ are a subset of the components of $y$,
we say $y_A$ is a \emph{subvector} of $y$.
\index{subvector}

If we were to insist that all vectors, including subvectors, have
index sets $\{1, \ldots, k\}$ for some natural number $k$.  Then we could
not distinguish different subvectors of the same length, or at least could
not without ugly and cumbersome extra decoration of the notation.
(It is hard to explain how elegant this notation is with simple examples,
but a perusal of Appendix~\ref{app:markov} will show this notation is
extremely powerful, and that appendix would be much longer and more confusing
if we had to use conventional notation with indices going from 1 to $k$.)

Our notation does have the drawback that we have only the convention
that lower case letters denote elements of sets and upper case letters
denote sets, which is why $y_j$ is clearly a component of a vector or subvector
(the value of a function at the index $j$) and $y_A$ is a subvector
(so is still a function, not the value of a function).
We also consider any subscript notation that clearly denotes a set
as indicating a subvector, for example, $y_{\{1, 3, 5\}}$ or $y_{\{j\}}$ or
$y_{\set{ j \in J : j \prec i }}$.

\section{Regression Notation}

Strictly speaking, in regression theory, every probability and expectation
is conditional on covariate data, at least on the part of the covariate data
that is considered random rather than fixed by the design of the experiment.
Thus to be hyperpedantic, we should always write
\begin{gather*}
   E(y_A \mid \text{the part of covariate data that is random})
   \\
   \Pr(y_A \in B \mid \text{the part of covariate data that is random})
\end{gather*}
rather than $E(y_A)$ or $\Pr(y_A \in B)$.  But, like most regression books,
we will not do this.  The dependence of probabilities and expectations on
covariates is usually not made explicit in the notation.

This is especially important in aster models when components of the response
vector depend on the values of other components, so we frequently write
\begin{gather*}
   E(y_A \mid y_j)
   \\
   \pr(y_A \mid y_j)
\end{gather*}
and the like.  And we do not want this dependence confused with dependence
on covariate data.

When necessary for clarity, as in the discussion of fitness landscapes,
which are regression functions,
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
we can explicitly denote the dependence on covariate data in conditional
probabilities and expectations.

\section{Factorization}
\label{sec:factorization}

If $J$ is the index set of the response vector $y$ of an aster model,
then there is a partition $\mathcal{G}$ of $J$
and a function $q : \mathcal{G} \to N$, where $N \supset J$, such that
\index{aster model!property!factorization}
\index{factorization|seeunder{aster model}}
the joint distribution of $y$ factorizes as
\begin{equation} \label{eq:factorize}
   \pr(y) = \prod_{G \in \mathcal{G}} \pr(y_G \mid y_{q(G)})
\end{equation}

In this factorization, each component $y_j$ of the response vector $y$
appears exactly once ``in front of the bar'' in a conditional on
the right-hand side (because $\mathcal{G}$ is a partition of $J$ so
each $j \in J$ is in exactly one $G \in \mathcal{G}$).
So every component of $y$ is treated as random (the joint distribution
of $y$ is modeled).
Random variables $y_{q(G)}$ that appear ``behind the bar'' in a conditional
on the right-hand side may or may not be elements of $y$.  They are not
if $q(G) \notin J$.  The distribution of such random variables is not
modeled by \eqref{eq:factorize}.  So they are treated as constant random
variables.

We say \eqref{eq:factorize} is \emph{valid} if what are denoted as
conditional distributions on the right-hand side agree with the conditional
distributions derived from the left-hand side (the joint distribution) by
the usual operations of probability theory.
\begin{theorem} \label{th:factorize}
The factorization \eqref{eq:factorize} is valid if and only if
the partition $\mathcal{G}$ can be totally ordered
by some total ordering $<$ such that $q(G) \in H$ implies $G < H$.
\end{theorem}
A proof of this theorem is straightforward and given
in Appendix~\ref{app:factorize}.
It could also be derived from the discussion of chain graph models
in \citet[equation~3.23]{lauritzen}.

In \eqref{eq:factorize} we have been deliberately vague about what $\pr$ is
supposed to mean, since there are many ways to specify probability
distributions and any of them will do.
\begin{itemize}
\item If $y$ is a discrete random vector,
      then $\pr$ could denote probability mass functions.
\item If $y$ is a continuous random vector,
      then $\pr$ could denote probability density functions.
\item If $y$ is a partly discrete and partly continuous continuous
      random vector (either some components discrete and some components
      continuous or some components a mixture of discrete and continuous)
      then $\pr$ could denote probability mass-density functions.
\item No matter what, $\pr$ could denote cumulative distribution functions.
\item No matter what, $\pr$ could denote probability measures.
\end{itemize}
In any of these cases the multiplication indicated in \eqref{eq:factorize}
is actual multiplication of real-valued thingummies.

\subsection{Topological Sort}

The total order asserted to exist by the theorem need not be unique and
usually is not unique.
We can find such a total order using the algorithm called topological sort
\index{topological sort}
\citep[Section~6.6]{aho-et-al}.

Using R function \code{tsort} in R package \code{pooh}
\citep{pooh-package}
\index{R package!pooh@\code{pooh}}
for each $G \in \mathcal{G}$ such that there exists
a (necessarily unique) $H \in \mathcal{G}$ such that $q(G) \in H$
let $G$ be a component of the vector \code{from} that is an argument to
\code{tsort} and
let $H$ be the corresponding component of the vector \code{to} that is another
argument to \code{tsort} and
neither vector has any other components.  Then invoking \code{tsort} with
these \code{from} and \code{to} arguments and \code{domain} argument that is
$\mathcal{G}$ strung out in a vector in any order
will determine a (not necessarily unique) total order that agrees with
Theorem~\ref{th:factorize}.  If the user has made a mistake and incorrectly
specified the $q$ function so there is no total order that satisfies
Theorem~\ref{th:factorize}, then \code{tsort} will give an error.

Current code in R packages \code{aster} and \code{aster2} does not
actually use the topological sort algorithm but rather forces the user
to input the data so that the numerical order of the components of the
response vector is the total order, that is, considering the index set
of the response vector to be $\{1, \ldots, n\}$ for some integer $n$,
current code requires $q(G) < j$ for any $j \in G$.
It is up to the user to input the data in this way.  The computer is no help.

But we could make the computer figure this out in future versions of the
software.

\subsection{Further Factorization}
\label{sec:further-factorize}

In \citet{lauritzen} chain graph factorizations like our \eqref{eq:factorize}
and his equation (3.23) can be further factorized, his equation (3.24).
But in aster model theory, we shall never be interested in such further
factorizations (even in cases where they are possible) and never use any
notation that allows for them.  So we will never have an analog of equation
(3.24) in \citet{lauritzen}.  For us, factorization is \eqref{eq:factorize}.

\section{Graphs}

Each factorization goes with a graph \citep[Section~3.2.3]{lauritzen}.
\index{aster graph}
\index{graph|see{aster graph}}
The nodes of the graph are either the elements of $N$ or the components
of $y$ corresponding to these elements ($y_j$ for $j \in N$).
There is a directed edge, also called an \emph{arrow},
\index{node}
\index{edge}
\index{arrow}
\index{line}
$q(G) \longrightarrow j$ (or if one prefers $y_{q(G)} \longrightarrow y_j$)
for every $G \in \mathcal{G}$ and every $j \in G$.
There is an undirected edge, also called a \emph{line},
$j \myline k$ (or if one prefers $y_j \myline y_k$)
for every $G \in \mathcal{G}$ and every $j, k \in G$ such that $j \neq k$.

As we have just seen, the function $q$ determines the graph
(the function $q$ knows its domain $\mathcal{G}$).
Conversely, the graph determines the function $q$.
\begin{itemize}
\item The set $J = \bigcup \mathcal{G}$ is the set of nodes of the graph
    that have incoming arrows (as we shall see, these nodes are called
    non-initial).
\item The elements of $\mathcal{G}$ are the maximal connected components
    of the graph of lines having node set $J$.
    (The graph of lines is the graph obtained by keeping all the nodes
    and lines but removing all the arrows).
    \index{aster graph!of lines}
    This includes any singleton sets of $J$ that have no incoming lines.
\item The graph of $q$ is determined by the arrows: $(G, q(G))$ is an
    argument-value pair whenever there is an arrow $j \longrightarrow k$
    with $j = q(G)$ and $k \in G$.
\end{itemize}

Thus we can reason with with graphs or with $q$ functions (which we will
soon learn to call \emph{predecessor functions}, Section~\ref{sec:other} below).
Graphs can be helpful, but we do not have to use them.

\subsection{Exception}
\label{sec:exception-dependence-group-lines}

In theory, as stated above, there is a line between every pair of distinct
elements of every dependence group and no other lines.

In practice, this leads to annoying and unnecessary clutter.
Because we never further factorize dependence groups
(Section~\ref{sec:further-factorize} above), we can find the dependence
groups from the graph if we only include enough lines so that each
dependence group is a connected subgraph of the graph of lines
(again, this is the graph obtained by keeping all the nodes and lines but
removing all the arrows).
\index{aster graph!of lines}

This exception is illustrated in graph \eqref{gr:multi} below
where only two lines
rather than three are used to connect the nodes of each
dependence group of size three.

\section{Graphical Terminology}
\label{sec:graphical-terminology}

In aster theory, we say
\begin{itemize}
\item a node is \emph{initial} if it has no incoming arrows
\index{node!initial}
\index{initial node|seeunder{node}}
    or lines (when thinking about the graph) or if it is not an element
    of $J = \bigcup \mathcal{G}$ (when thinking about the function $q$),
\item a node is \emph{terminal} if it has no outgoing arrows
\index{node!terminal}
\index{terminal node|seeunder{node}}
    (it may have outgoing lines and will have outgoing lines if it is
    an element of an element of $\mathcal{G}$ that is not a singleton set)
    (when thinking about the graph) or if it is not an element
    of $\set{ q(G) : G \in \mathcal{G} }$
    (when thinking about the function $q$),
\item if there is an arrow $j \longrightarrow k$, then we say that $j$
    is the \emph{predecessor} of $k$ (or $y_j$ is the predecessor of $y_k$),
\index{node!predecessor}
\index{predecessor node|seeunder{node}}
\item and, conversely, that $k$ is a \emph{successor} of $j$
    (or $y_k$ is the successor of $y_j$).
\index{node!successor}
\index{successor node|seeunder{node}}
\end{itemize}

In mainstream graphical model theory, a different terminology is more widely
used \citep{lauritzen} root = initial, leaf = terminal, parent = predecessor,
child = successor.  We do not use this terminology in aster model theory
because it can cause serious confusion in biological applications.

As a general policy, we eschew all terminology based on biological analogies
when there is an available alternative (even when that alternative is less
popular).

In any aster graph every node has at most one predecessor and all nodes in
\index{aster model!property!at most one predecessor}
the same $G \in \mathcal{G}$ must have the same predecessor (because $q$
is a function that takes elements of $\mathcal{G}$ as arguments).

In mainstream graph theory, a chain graph with only arrows (no lines) having
the at-most-one-predecessor property is called a \emph{forest} and its maximal
connected components are called \emph{trees}, but we do not use this terminology
either (avoiding serious confusion when the application involves data on
real trees in real forests).  It is enough to say that aster graphs
have the at-most-one-predecessor property.

In mainstream graph theory, there is a term \emph{ancestor} that means
predecessor, or predecessor of predecessor,
or predecessor of predecessor of predecessor,
or predecessor of predecessor of predecessor of predecessor,
or the same with arbitrarily many repetitions of ``predecessor of.''
And there is a converse term \emph{descendant}, that is, $i$ is an ancestor
of $j$ if and only if $j$ is a descendant of $i$.

In aster model theory we avoid these terms too (avoiding confusion when
the application involves real biological organisms with real biological
ancestors and real biological descendants).  If we need the concepts,
then we use the long-winded descriptions
predecessor of predecessor of predecessor and so forth or
successor of successor of successor and so forth.
Fortunately, we rarely need these concepts.
And when we do need these concepts we can avoid the cumbersome verbiage
by using mathematical notation introduced in Section~\ref{sec:closure}
below.

Finally, we need a term for $\mathcal{G}$ and its elements.
The terminology we have been using in our writings about aster models is
elements of $\mathcal{G}$ are \emph{dependence groups}.
\index{dependence group}
The mainstream graphical models terminology \citep{lauritzen} is
\emph{chain components}.  Both have two words and four syllables.
Neither is very elegant.  We don't like the ``chain'' terminology because
we are not using general chain graph theory (aster models are very special
chain graphs).  Our term \emph{dependence group} is not great, but we haven't
thought of a better term.

\subsection{Exception}
\label{sec:exception-root}

R package \code{aster} uses ``root'' node for initial node.
\index{node!root}
\index{root node|seeunder{node}}
We hadn't completely thought through the terminology when that package
was written, and we have kept this inconsistency for reasons of backward
compatibility.

\section{Two Kinds of Aster Graphs}
\label{sec:scare-quotes}

The graphs for aster models are often very large with thousands or tens of
thousands of nodes, but usually they are composed of isomorphic subgraphs.
So drawing one of these isomorphic subgraphs is enough.
If you've seen one, you've seen them all.
(Graphs are isomorphic if a drawing of one can be laid on a drawing of the
other with everything --- nodes, lines, and arrows --- matching up.)

An aster graph need not be composed of all isomorphic subgraphs,
but the only published example of that is, as far as I know,
\citet{aster-hornworm}.

To distinguish these two kinds of graphs, we call the aster graph described
in the preceding section the \emph{full aster graph} (we consider the ``full''
redundant but the emphasis may help avoid confusion).
\index{aster graph!full}

Certain subgraphs of the full aster graph, we then call graphs
for ``individuals'' (in scare quotes for reasons to be explained presently).
These are easier to recognize than describe.

Current aster software (Section~\ref{sec:software} above) forces
$q(G) \neq q(H)$ whenever $G \neq H$ and $q(G)$ and $q(H)$ are initial nodes.
In this case, the graph for an ``individual'' (in scare quotes)
consists of the subgraph consisting of one initial node and all of its
successors or successors of successors or successors of successors
of successors and so forth with arbitrarily many repetitions
of ``successors of'' and all of the arrows and lines in the full graph
connecting these nodes.
\index{aster graph!for ``individual''}
(This is where the term ``descendant'' in its graph-theoretic sense would
come in handy if we allowed ourselves to use it.  The graph for an
``individual'' consists of one initial node, all of its descendant nodes,
and all of the lines and arrows going between these nodes.  But once we
have the idea of the graph for an ``individual'' we no longer need the
term ``descendant.'')

But aster theory as described so far does not force this convention.
If $y_j = 1$, for all initial nodes $j$, which is the case with most
(but not all) aster applications, then it would do no harm if all initial
nodes were fused into one initial node.  That would invalidate nothing but
the way we just described graphs for ``individuals'' (in scare quotes).

Thus we have to be a bit more careful.  If $G$ is a dependence group whose
predecessor $q(G)$ is initial, then the graph for the ``individual''
(in scare quotes) containing $G$ consists of $q(G)$, the nodes in $G$
and their successors or successors of successors or successors of successors
of successors and so forth with arbitrarily many repetitions
of ``successors of'' and all of the arrows and lines in the full graph
connecting these nodes.
\index{aster graph!for ``individual''}
(And it would make this definition a little shorter
if we allowed ourselves to use the word ``descendant'' in its graph-theoretic
sense.)

There are two reasons why the scare quotes.
\begin{itemize}
\item In life history
analysis, the graph for an ``individual'' ideally goes one or more times
around the life cycle (exactly).  Thus it may involve data not only for
one biological individual but also for its offspring and perhaps offspring
of offspring (if the experiment goes twice around the life cycle) or even
perhaps more remote descendants (where here ``descendants'' means real
biological descendants, not the graphical models idea of descendants).
\item If the value of the constant $y_j$ at the initial node of the
graph for an ``individual'' is greater than one, then the data for this
``individual'' is actually cumulative data for $y_j$ real biological
individuals and perhaps their real biological descendants.
\end{itemize}

%%%%%%%%%% NEED FORWARD REFERENCE to example graphs %%%%%%%%%%
%%%%%%%%%% maybe ?????

If one does not like our terminology of ``individual'' in scare quotes,
our advice is to just explain what data the graph is for.  It may actually
be for a biological individual, for a biological individual
and its offspring, or $n$ biological individuals.  Just say what it is.

Or we could use the characterization of Corollary~\ref{cor:markov} in
Appendix~\ref{app:markov}, which says the subgraphs for ``individuals''
are stochastically independent subvectors of the response vector.
\index{aster graph!for ``individual''}

In general, the subgraphs for ``individuals'' are the minimal stochastically
independent subvectors, in the sense that the data for an ``individual'' has
no stochastically independent parts.  But when limiting conditional models
%%%%%%%%%% NEED FORWARD REFERENCE to limiting conditional models %%%%%%%%%%
come into play, this is no longer the case.
Thus independence of data for ``individuals'' is an important property of
aster models, but it does not (in general) characterize
subgraphs for ``individuals.''

\section{The Other Predecessor Function}
\label{sec:other}

It is useful to have not only the set-to-index predecessor function $q$
defined in Section~\ref{sec:factorization} above but also the index-to-index
\index{predecessor function!set-to-index}
\index{predecessor function!index-to-index}
predecessor function $p$ defined as follows
$$
   p(j) = k \ifandonlyif j \in G \in \mathcal{G} \opand q(G) = k.
$$
Clearly, $q$ determines $p$.
The converse is not true because $p$ knows nothing about dependence groups.
But $p$ and $\mathcal{G}$ together determine $q$.

\section{The Transitive Closure of the Predecessor Relation}
\label{sec:closure}

The \emph{predecessor relation} on $N$ is the function $p$ thought
of as a relation, that is, thinking set-theoretically
\citep[Section~7]{halmos-set-theory} as the set
$$
   \set{ (j, p(j)) : j \in J }
$$
of its argument-value pairs.

We need a notation from dynamical systems theory for repeated application
of a function.  If $f$ is any function whose domain and codomain are the same,
then it makes sense to compose $f$ with itself.  Then we let $f^0$ denote
the identity function on the domain of $f$, let $f^1 = f$, $f^2 = f \circ f$,
and, in general, $f^{n + 1} = f^n \circ f$.
So
\begin{align*}
   f^0(x) & = x
   \\
   f^1(x) & = f(x)
   \\
   f^2(x) & = f(f(x))
   \\
   f^3(x) & = f(f(f(x)))
\end{align*}
and so forth.

The transitive closure of the predecessor relation
is the smallest transitive relation $R$ containing it.
\index{predecessor relation!transitive closure}
As with most relations, we prefer denoting this relation by infix notation:
saying $j \succ k$ rather than $(j, k) \in R$, that is, $j \succ k$ means
$k = p^n(j)$ for some positive integer $n$.

\begin{theorem} \label{th:transitive-closure}
Under the conditions of Theorem~\ref{th:factorize},
the transitive closure of the predecessor relation is a strict partial order.
\end{theorem}
\begin{proof}
If $j \succ k$, then $j \in G$ for some $G \in \mathcal{G}$ and
$k = p^n(q(G))$ for some natural number $n$ ($n = 0$ is allowed).

If $k \in H$ for some $H \in \mathcal{G}$,
then we have $G < H$ in the total ordering
that Theorem~\ref{th:factorize} uses.
Hence we cannot also have $k \succ j$ because that would imply $G < H$
and $H < G$ contradicting $<$ being a strict total order.

If $k \notin H$ for any $H \in \mathcal{G}$ then $k$ has no predecessor
($k$ is initial) and we cannot have $m \succ k$ for any node $m$.

In either of the preceding cases we never have $k \succ j$ and $j \succ k$.
Since $\succ$ is a transitive relation by definition, it is
a strict partial order \citep[Section~14]{halmos-set-theory}
\end{proof}
\begin{corollary} \label{cor:compatible}
The transitive closure of the predecessor relation is compatible with
the total order on the family of dependence groups defined
in Theorem~\ref{th:factorize} in the sense that
$j \in G \in \mathcal{G}$ and $k \in H \in \mathcal{G}$ and $j \succ k$
implies $G < H$.
\end{corollary}

The non-strict counterpart of this relation
is the reflexive transitive closure of the predecessor relation,
\index{predecessor relation!reflexive transitive closure}
which is denoted $\succeq$.
We have $j \succeq k$ if and only if $j \succ k$ or $j = k$.

The inverse of a relation $R$ considered as a set of argument-value pairs
reverses the order in the pairs, that is $(k, j) \in R^{- 1}$ if and only
if $(j, k) \in R$.
As usual, we denote the inverse of a relation by turning its infix notation
around: $\prec$ is the inverse of $\succ$ and $\preceq$ is the inverse
of $\succeq$.

The inverse of the predecessor relation is the successor relation,
so $\prec$ is the transitive closure of the successor relation
and $\preceq$ is the reflexive transitive closure of the successor relation.
\index{successor relation!transitive closure}
\index{successor relation!reflexive transitive closure}
\index{transitive closure|seeunder{predecessor relation}}
\index{transitive closure|seeunder{successor relation}}
\index{reflexive closure|seeunder{predecessor relation}}
\index{reflexive closure|seeunder{successor relation}}

The choice of whether the transitive closure of the predecessor relation
is denoted $\succ$ or $\prec$ is arbitrary.  Either choice works so long
as one keeps straight which is which.  Our choice is influenced by an
arbitrary choice in the source code for R package \texttt{aster}.  When
the predecessor function is encoded (as the argument \texttt{pred} to the
R function \texttt{aster}) it is required that predecessors have lower indices
than successors (come before them in the \texttt{pred} vector).  Thus we
want to think of predecessors as ``less than'' successors in some sense.
Hence our decision to make $p(j) \prec j$.

In graphical model theory,
$\succ$ is called the ancestor relation,
$\prec$ the descendant relation,
$\succeq$ the ancestor-or-self relation, and
$\preceq$ the descendant-or-self relation.
But, as stated in Section~\ref{sec:graphical-terminology} above,
our policy is to avoid these terms
to avoid confusion in biological applications.
If we need words rather than symbols, we have to use the long winded ones:
``reflexive transitive closure of the predecessor relation'' and so forth.

\section{Predecessor is Sample Size}
\label{sec:piss}

All aster models have the \emph{predecessor is sample size} property.
This is a very important property that separates them from all other
graphical models.  There is a long history of models that have this
property.  Life table analysis and discrete time survival analysis have it.
So does Leslie matrix analysis \citep{caswell} and other methods
of estimation of population growth rate \citep{fisher,goodman,lenski-service}.
But none of those models were regression models, nor did they have
the generality of aster models in their graphical structure.
They do have the basic relationship of conditional and unconditional means
implied by this property (Section~\ref{sec:mu-and-xi} below)
but nothing else of aster model theory.

For one conditional distribution in the factorization \eqref{eq:factorize},
say for the conditional distribution of $y_G$ given $y_{q(G)}$,
\begin{itemize}
\item conditional on $y_{q(G)} = 0$, the distribution of $y_G$ is concentrated
    at zero (the zero vector having all components equal to zero), 
\item conditional on $y_{q(G)} = 1$, the distribution of $y_G$ is whatever
    this distribution is designated to be, and
\item conditional on $y_{q(G)} = n$ with the $n > 1$, the distribution
    of $y_G$ is the $n$-fold convolution of the distribution for sample
    \index{convolution}
    size one.
\end{itemize}
In short, the conditional distribution of $y_G$ given $y_q(G)$ is the
distribution of the sum of $y_{q(G)}$
independent and identically distributed (IID)
random vectors having whatever the distribution is for sample size one.
(By convention, a sum having zero terms is zero, and a sum having one term
is that term.)
Or, even shorter, the predecessor plays the role of sample size for this
conditional distribution.
Or, shorter still, \emph{predecessor is sample size}.
\index{aster model!property!predecessor is sample size}
\index{predecessor is sample size|seeunder{aster model}}

Note that we name families for dependence groups by the conditional
distribution for sample size one.
This is an unusual practice.  It is not the way families are named for
generalized linear models.
And it can seem unnecessarily mysterious at first sight.

All of our example graphs in Section~\ref{sec:graphs} below
have Bernoulli arrows.  For such an arrow
$$
\begin{CD}
   y_i @>\text{Ber}>> y_j
\end{CD}
$$
why not just say the conditional distribution of $y_j$ given $y_i$ is binomial
with sample size $y_i$ (because the sum of IID Bernoulli is binomial)?
For one thing,
it is not clear what sample size zero means without further explanation.
For another thing, for an arrow
$$
\begin{CD}
   y_i @>\text{0-Poi}>> y_j
\end{CD}
$$
the distribution of the sum of IID zero-truncated Poisson random variables
is not a ``brand name distribution.''  And its
probability mass function has no closed-form expression.
So we could not label this arrow with the name of the conditional distribution
of $y_j$ given $y_i$ because there is no such name.

One consequence of the predecessor-is-sample-size property is that $y_j$
that are predecessors (are at nonterminal nodes) must be
nonnegative-integer-valued random variables.
There is an exception to this requirement that will be discussed in
Section~\ref{sec:infinitely-divisible} below, but that exception has
never been used.

Another consequence of the predecessor-is-sample-size property is
the following section.

\section{Conditional and Unconditional Mean Values}

\subsection{Unconditional}

Let $y$ be the response vector of an aster model.  We define a parameter
vector $\mu = E(y)$.  This is the vector having components
$$
   \mu_j = E(y_j), \qquad j \in J,
$$
where, as usual, $J$ is the index set of the response vector (the set
of non-initial nodes of the full aster graph).

This is called the \emph{unconditional mean value parameter vector}.
\index{parameter vector!mean value!unconditional}
This name is getting us a little bit ahead of ourselves.
At this point, we don't even know these means exist.
(We will eventually find out they do exist.)
%%%%%%%%%% NEED FORWARD REFERENCE to regular full and moments %%%%%%%%%%
And, at this point, we don't know that means parameterize aster models,
since we haven't yet even completely specified what the distribution
of an aster model is.  We know the fundamental factorization
\eqref{eq:factorize}, and we know each of those factors obeys the
predecessor-is-sample-size property, but we don't yet know anything more.
(We will eventually find out means do parameterize aster models.)
%%%%%%%%%% NEED FORWARD REFERENCE to mean value parameterization %%%%%%%%%%

For now we will just assume these means exist.

\subsection{Conditional}

We define another parameter vector $\xi$ having components
$$
   \xi_j = E(y_j \mid y_{p(j)} = 1), \qquad j \in J,
$$
if this expression makes sense.  It will not make sense when the
conditioning event has probability zero (so the conditional expectation
can be defined arbitrarily).  In that case we have to use a different
definition that does not come with an equation.
The predecessor-is-sample-size property says that $y_j$ is the sum
of $y_{p(j)}$ IID random variables, and we say $\xi_j$ is the mean
of those random variables.

This is called the \emph{conditional mean value parameter vector}.
\index{parameter vector!mean value!conditional}
As in the preceding section, this name is getting us a little bit ahead
of ourselves.
At this point, we don't even know these means exist.
(We will eventually find out they do exist.)
%%%%%%%%%% NEED FORWARD REFERENCE to regular full and moments %%%%%%%%%%
And, at this point, we don't know that means (conditional or unconditional)
parameterize aster models.  But the next section will show the unconditional
means determine conditional means and vice versa.  So if $\mu$ parameterizes,
then so does $\xi$, and vice versa.

\subsection{The Combination of the Two}
\label{sec:mu-and-xi}

It follows from the predecessor-is-sample-size property and linearity of
expectation that
\begin{equation} \label{eq:cond-exp}
   E(y_j \mid y_{p(j)}) = \xi_j y_{p(j)}, \qquad j \in J.
\end{equation}
Then it follows from the iterated expectation axiom of conditional probability
$$
   E(y_j)
   =
   E\{E(y_j \mid y_{p(j)})\}
$$
that
\begin{equation} \label{eq:mu-and-xi}
   \mu_j = \xi_j \mu_{p(j)}, \qquad j \in J.
\end{equation}
This is the fundamental recursive relation that shows (as we examine in
more detail presently) how $\mu$ is determined by $\xi$ and vice versa.

To map from $\xi$ to $\mu$ we use \eqref{eq:mu-and-xi} recursively
\begin{align*}
   \mu_j
   & =
   \xi_j \mu_{p(j)}
   \\
   & =
   \xi_j \xi_{p(j)} \mu_{p(p(j))}
   \\
   & =
   \xi_j \xi_{p(j)} \xi_{p(p(j))} \mu_{p(p(p(j)))}
\end{align*}
and so forth, with as many recursive applications as necessary.  In practice,
the computer traverses the graph in any order that visits predecessors before
successors using \eqref{eq:mu-and-xi} to determine $\mu_j$ as a function of
$\xi$ ($\mu_{p(j)}$ having already been determined when its node was visited
previously).  To get the recursion started, we need the mean values at initial
nodes, which are given by
\begin{equation} \label{eq:xi-mu-initial}
   \mu_j = y_j, \qquad j \in N \setminus J,
\end{equation}
because the mean value of a constant random variable is its constant value.

Using the reflexive transitive closure of the successor relation $\preceq$
we can rewrite the above as follows
\begin{equation} \label{eq:xi-mu-prod}
    \mu_j
    =
    \left( \prod_{\substack{i \in J \\ i \preceq j}} \xi_i \right)
    \left( \prod_{\substack{i \in N \setminus J \\ i \preceq j}} \mu_i \right),
    \qquad j \in J,
\end{equation}
where we note that the second product always has exactly one term: there
is always exactly one initial node $i$ such that $i \preceq j$.
We could also rewrite \eqref{eq:xi-mu-prod} as
\begin{equation} \label{eq:xi-mu-prod-too}
    \mu_j
    =
    \left( \prod_{\substack{i \in J \\ i \preceq j}} \xi_i \right)
    \left( \prod_{\substack{i \in N \setminus J \\ i \preceq j}} y_i \right),
    \qquad j \in J,
\end{equation}
by \eqref{eq:xi-mu-initial}.

To map from $\mu$ to $\xi$, rewrite \eqref{eq:mu-and-xi} as
\begin{equation} \label{eq:mu-to-xi}
   \xi_j = \frac{\mu_j}{\mu_{p(j)}}
\end{equation}
but for this to make sense, we must know that $\mu_{p(j)}$ is never zero.

We will eventually find out that $\mu_{p(j)}$ is never zero except in
%%%%%%%%%% NEED FORWARD REFERENCE %%%%%%%%%%
limiting conditional models.  So we do not always have this property.
Thus \eqref{eq:mu-to-xi} makes sense when $\mu_{p(j)}$ is never zero,
but otherwise some components of $\xi$ are not determined by $\mu$.

Conversely, multiplication by zero is not a problem (unlike division by zero),
so \eqref{eq:xi-mu-prod} and \eqref{eq:xi-mu-prod-too} always determine
$\mu$ as a function of $\xi$.

Everything in this section up to this point is an elementary consequence
of the laws of conditional and unconditional expectation and the
predecessor-is-sample-size property.  Consequently,
everything in this section up to this point is also true of all previous
models in survival analysis and demography that have also had this property
cited in Sections~\ref{sec:background} and~\ref{sec:piss} above.

\subsection{Confession}

\Citet{aster1} did not define $\xi$
the way we do here.  Instead they used that Greek letter to denote
\eqref{eq:cond-exp}.
A referee said this definition is dumb.  It makes $\xi$ a function
of both random variables and parameters, so it is not a parameter,
and one shouldn't use Greek letters for things that aren't parameters.
We didn't listen then and managed to get the
paper published overriding this objection.  But now we agree with the referee.

The vector $\xi$ as defined here is an important parameterization of
aster models \citep[this has been realized since][]{aster-philosophical}.

R package \texttt{aster} used the same dumb definition until version
1.0-2 of the package, when a new optional argument \code{is.always.parameter}
was added to the method of R generic function \code{predict} that handles
aster model objects.  And, for reasons of backward compatibility,
the dumb definition is still the default.
One must use the optional argument \code{is.always.parameter = TRUE}
to estimate $\xi$ as defined in this section.

R package \texttt{aster2} and recent papers and technical reports use
the definition presented here (the conditional mean value parameter vector
is $\xi$ if they mention conditional mean value parameters at all).

\section{Some Aster Graphs}
\label{sec:graphs}

The first published aster model \citep{aster1} had this graph
\begin{equation} \label{gr:aster1}
\begin{CD}
   1
   @>\text{Ber}>>
   y_1
   @>\text{Ber}>>
   y_2
   @>\text{Ber}>>
   y_3
   \\
   @.
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   \\
   \hphantom{1}
   @.
   y_4
   @.
   y_5
   @.
   y_6
   \\
   @.
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
  \\
   \hphantom{1}
   @.
   y_7
   @.
   y_8
   @.
   y_9
\end{CD}
\end{equation}
which is for one individual.  There are 570 individuals in the data set,
which is included in the R package \texttt{aster}.  So one can think of the
full aster graph as 570 copies of this graph with the subscripts changed
so the nodes (the $y_j$) are all different.

Because this graph has only arrows, no lines, each node is
a dependence group all by itself.

The individuals are plants of the species \emph{Echinacea angustifolia},
whose common name is narrow-leaved
purple coneflower.  These data were collected by the Echinacea Project
(\url{http://echinaceaproject.org/}), a long-running project funded by
the National Science Foundation (the co-PI's are the second and third authors
of \citet{aster1}).  The way \eqref{gr:aster1} is laid out,
variables in the first column ($y_1$, $y_4$, and $y_7$) are for 2002,
those in the second column are for 2003,
those in the third column are for 2004,
those in the first row ($y_1$, $y_2$, and $y_3$) measure survival
(0 = dead, 1 = alive),
those in the second row indicate flowering
(0 = no flowers, 1 = some flowers),
those in the third row are flower head counts
(actual number of flower heads).

Of course,
the ``rows'' and ``columns'' are not part of the graphical structure.
The only thing that matters is which nodes are connected by which arrows.

Aster graphs can get a lot bigger than \eqref{gr:aster1}.
The Echinacea Project now has data for years since 2004 (which extends
the graph with many more ``columns'') and data for more life history
stages (which extends the graph with more ``rows'').

The node labels (the $y_j$) are random variables, components of the response
vector.  The arrows indicate conditional distributions.
An arrow
\begin{equation} \label{gr:one-arrow-conditional}
\begin{CD}
   y_i @>>> y_j
\end{CD}
\end{equation}
indicates the conditional distribution of $y_j$ given $y_i$.
An arrow
\begin{equation} \label{gr:one-arrow-marginal}
\begin{CD}
   1 @>>> y_i
\end{CD}
\end{equation}
indicates the marginal distribution of $y_i$,
because conditioning on a constant random variable is the same as not
conditioning.

Labels on the arrows name the distribution.
Ber is for Bernoulli (any zero-or-one-valued random variable), and
0-Poi is for zero-truncated Poisson (Poisson conditioned on being nonzero).
This explanation of arrows and their distributions is incomplete and will
be picked up again in Section~\ref{sec:piss}.
%%%%%%%%%% NEED FORWARD REFERENCE to exponential family assumption %%%%%%%%%%

Here is a more complicated aster graph from \citet{aster3}
\begin{equation} \label{gr:aster3}
\begin{CD}
   1
   @>\text{Ber}>>
   y_1
   @>\text{Ber}>>
   y_2
   @>\text{Ber}>>
   y_3
   @>\text{Ber}>>
   y_4
   \\
   @.
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   \\
   \hphantom{1}
   @.
   y_5
   @.
   y_6
   @.
   y_7
   @.
   y_8
   \\
   @.
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   @VV\text{0-Poi}V
   \\
   \hphantom{1}
   @.
   y_9
   @.
   y_{10}
   @.
   y_{11}
   @.
   y_{12}
   \\
   @.
   @VV\text{Poi}V
   @VV\text{Poi}V
   @VV\text{Poi}V
   @VV\text{Poi}V
   \\
   \hphantom{1}
   @.
   y_{13}
   @.
   y_{14}
   @.
   y_{15}
   @.
   y_{16}
   \\
   @.
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   @VV\text{Ber}V
   \\
   \hphantom{1}
   @.
   y_{17}
   @.
   y_{18}
   @.
   y_{19}
   @.
   y_{20}
\end{CD}
\end{equation}

Again, because this graph has only arrows, no lines, each node is
a dependence group all by itself.  The label Poi on arrows indicates
the Poisson distribution.

This graph is for simulated data, which \citet{aster3} used because
at the time no data for aster models as complicated as \eqref{gr:aster3} had
been collected by biologists, and it was important to give such
an illustration of the possibilities of aster models.
Like in \eqref{gr:aster1} the ``columns'' in \eqref{gr:aster3} are for
data in successive years.  The first three ``rows'' of \eqref{gr:aster3}
can be taken to be the same as those of \eqref{gr:aster1}: survival,
flowering indicator variables, and flower counts.  The fourth row of
\eqref{gr:aster3} is seed counts, and the fifth row is number of seeds
that germinate (produce new plants).  Of course, since the data
are simulated, the story about these variables is just a story.
It could be told differently, and \citet{aster3} do have a story
where the same graph could be for data about an animal rather than a plant.

This graph did serve as a good example of what was possible.
\citet*[in the on-line appendix]{stanton-geddes-tiffin-shaw}
discuss an aster model with seven life history stages (one of which is
artificial, modeling random sampling in the data collection process, hence
only six are about life history of the organisms) and thus would be
like the graph \eqref{gr:aster3} except with seven ``rows.''
Because the organism in question (\emph{Chamaecrista fasciculata}, common
name partridge pea) was an annual plant, there is only one ``column.''
(As mentioned above, ``rows'' and ``columns'' are not part of the graphical
structure --- the only thing that matters is which nodes are connected
by which arrows and lines (if any) ---
and \citeauthor{stanton-geddes-tiffin-shaw} actually
laid out their graph in a row.)

Statistically, there are some important differences between these graphs.
Graph \eqref{gr:aster3} has both Poisson (Poi) and zero-truncated Poisson
(0-Poi) arrows and hence illustrates when to use which
(see also Section~\ref{sec:zero-inflated} below).
In graph \eqref{gr:aster1} every predecessor node is Bernoulli,
but in graph \eqref{gr:aster3} $y_{13}$ through $y_{16}$ are non-Bernoulli
predecessor nodes.  So \eqref{gr:aster3} shows that predecessor values can
be any nonnegative integer.

The graph \eqref{gr:multi} comes from a still unpublished manuscript for a book
about aster models.  It was the first graph for a model for an animal having
life history stages like an insect's larva, pupa, and adult.  We present this
graph for hypothetical data even though a similar model has been fit to real
data by \citet{aster-hornworm}.  Those data are for the tobacco hornworm
\emph{Manducca sexta}, which is an insect (a moth) that does have these life
history stages.  These data were not collected with the intention of using
an aster model (which were very new when the experiment was done) and so were
not ideal for aster analysis.  Although an aster analysis was done by
\citet{aster-hornworm}, it does not serve as quite as clean an example as
the graph \eqref{gr:multi}.

\begin{equation} \label{gr:multi}
\begin{tikzcd}
  \hphantom{1} & y_1 & y_4 & y_7 & y_{10}
  \\
  1
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_2
  \arrow[dash]{u}
  \arrow[dash]{d}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_5
  \arrow[dash]{u}
  \arrow[dash]{d}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_8
  \arrow[dash]{u}
  \arrow[dash]{d}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_{11}
  \arrow[dash]{u}
  \arrow[dash]{d}
  \\
  \hphantom{1} & y_3 \arrow{d}{\text{Ber}} & y_6 \arrow{d}{\text{Ber}}
  & y_9 \arrow{d}{\text{Ber}} & y_{12} \arrow{d}{\text{Ber}}
  \\
  \hphantom{1} & y_{13} \arrow{d}{\text{0-Poi}}
  & y_{14} \arrow{d}{\text{0-Poi}}
  & y_{15} \arrow{d}{\text{0-Poi}} & y_{16} \arrow{d}{\text{0-Poi}}
  \\
  \hphantom{1} & y_{17} & y_{18} & y_{19} & y_{20}
\end{tikzcd}
\end{equation}

As always, the constant 1 at the initial node
of the graph indicates that the graph is for one individual.
In addition to the notations Ber = Bernoulli
and 0-Poi = zero-truncated Poisson, which we have already met,
we now also have $\mathcal{M}$ = multinomial.
Lines without arrowheads are ``lines'' connecting nodes in the
same dependence group.  Hence the dependence groups containing more than
one node are $\{1, 2, 3\}$, $\{4, 5, 6\}$, $\{7, 8, 9\}$, and $\{10, 11, 12\}$.
Other nodes are dependence groups all by themselves; $\{j\}$ is a dependence
group for $j \ge 13$.

As stated in Section~\ref{sec:exception-dependence-group-lines} above,
this graph does not follow the theory, which requires a line connecting
each pair of distinct elements of each dependence group and which would
require us to add lines $y_1 \myline y_3$ and $y_4 \myline y_6$
and $y_7 \myline y_9$ and $y_{10} \myline y_{12}$ to the graph.
But, also as stated in Section~\ref{sec:exception-dependence-group-lines}
above, we don't need these lines to infer the dependence groups from the
graph.  The lines $y_1 \myline y_2$ and $y_2 \myline y_3$ that are in the
graph say $y_1$ and $y_3$ are in the same dependence group as $y_2$,
and since there are no other lines to these nodes, they must constitute
a dependence group.  Since there doesn't seem to be any room in
the picture \eqref{gr:multi} for these additional lines required by theory,
we omit them.

Each of the multi-node dependence groups has a conditional multinomial
distribution with, as usual, predecessor as sample size.  Since each
predecessor is zero-or-one-valued, if a predecessor (say $y_2$) is
equal to one, then exactly one of its three successor nodes ($y_4$, $y_5$,
and $y_6$) is equal to one, and, if this predecessor
is equal to zero, then all of its three successor nodes are also equal to zero.
In effect, exactly one of the ``exterior nodes'' of this group of switches
($y_1$, $y_4$, $y_7$, $y_{10}$, $y_{11}$, $y_{12}$, $y_9$, $y_6$, and $y_3$)
is equal to one.  There is one path taken by any particular individual,
from the initial node (marked 1) through these four multinomial dependence
groups.

The intended application for this graph \citep{aster-hornworm} is life history
data for an insect.  As in our graphs without dependence groups,
``columns'' of the graph are for different
times (here days, there years).
Nodes in the top ``row'' of this graph ($y_1$, $y_4$, $y_7$, and $y_{10}$)
indicate death.
Nodes in the second ``row'' of this graph ($y_2$, $y_5$, $y_8$, and $y_{11}$)
indicate the individual is a larva (caterpillar).
Nodes in the third ``row'' of this graph ($y_3$, $y_4$, $y_9$, and $y_{12}$)
indicate the individual is an adult (moth, with wings, flying around trying
to mate).
Nodes in the fourth ``row'' of this graph ($y_{13}$ through $y_{16}$)
indicate the mating success.
Nodes in the bottom ``row'' of this graph ($y_{17}$ through $y_{20}$)
count number of eggs laid.  So this graph is for female individuals.
In \citet{aster-hornworm} the same graph with only the multinomial dependence
groups (nodes 1 through $y_{12}$) is used for male individuals because
the sex of individuals was not determined before they reached adulthood.

So this graph illustrates two important points not seen before.
It is not necessary for every individual to
have the same graph (here females and males have different graphs).
And we have non-singleton dependence groups,
multinomial ``switches'' between different life history stages.

Here is yet another graph illustrating normal dependence groups.
\begin{equation} \label{gr:normal}
\begin{tikzcd}
   1
   \arrow{r}{\text{Ber}}
   & y_1
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \arrow{r}{\text{Ber}}
   & y_2
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \arrow{r}{\text{Ber}}
   & y_3
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \arrow{r}{\text{Ber}}
   & y_4
   \arrow{d}[swap]{\mathcal{N}}
   \arrow[bend left]{dd}{\mathcal{N}}
   \\
   \hphantom{1}
   & y_5 \arrow[dash]{d}
   & y_6 \arrow[dash]{d}
   & y_7 \arrow[dash]{d}
   & y_8 \arrow[dash]{d}
   \\
   \hphantom{1} & y_9 & y_{10} & y_{11} & y_{12}
\end{tikzcd}
\end{equation}
Here the top ``row'' indicates survival.  And the next two rows are
for normally distributed something or other given survival.  Here we
model the normal as two-node dependence groups
$\{ 5, 9 \}$,
$\{ 6, 10 \}$.
$\{ 7, 11 \}$, and
$\{ 8, 12 \}$ because
we do not want to assume variance is known.  As we shall see, this permits
but does not require, modeling variance as a function of covariates.

We see that the aster formalism suggests new possibilities.  In order to
have a two-parameter normal distribution, we need two-node dependence groups.
%%%%%%%%%% NEED FORWARD REFERENCE to two-parameter normal %%%%%%%%%%
%%%%%%%%%% NEED FORWARD REFERENCE to one-parameter normal %%%%%%%%%%

\subsection{Zero-Inflated Poisson}
\label{sec:zero-inflated}

Readers may have wondered why the graph \eqref{gr:aster1}
has its middle ``row.''  The variables $y_4$, $y_5$, and $y_6$ are a function
of the variables $y_7$, $y_8$, and $y_9$, respectively,
($y_j = 1$ if and only if $y_{j + 3} > 0$, $j = 4,$ 5, 6).
So why were these variables inserted in the graph?

Consider just the subgraph
\begin{equation} \label{gr:zero-inflated-poisson}
\begin{CD}
   y_1 @>\text{Ber}>> y_4 @>\text{0-Poi}>> y_7
\end{CD}
\end{equation}
The conditional distribution of $y_7$ given $y_1$ (both arrows combined)
is zero-inflated Poisson \citep{lambert}.
Since $y_7 = 0$ if and only if $y_4 = 0$, and the probability of this event
can be anything
(because the Bernoulli arrow does not have any restrictions on its parameter),
it is, strictly speaking, zero-inflated-or-deflated Poisson, but we will not
be this fussy about this bit of terminology.

So having arrows arranged like this is just the aster way of getting
zero-inflated Poisson random variables into the model.

Why can we not just have zero-inflated Poisson arrows?  Because
zero-inflated Poisson is not an exponential family.  It can be factored
into a product of two exponential families, like
\eqref{gr:zero-inflated-poisson} does.  But it is not itself exponential
family.  Hence we cannot have arrows like that in an aster model
because of the exponential family assumption (Section~\ref{sec:aster-expfam}
below).  If we want zero-inflated Poisson or zero-inflated negative binomial
%%%%%%%%%% NEED FORWARD REFERENCE to zero-truncated negative binomial %%%%%%%%%%
in aster models, then this is the way we have to do it.

Although we have zero-inflated Poisson distributions in aster models,
we do not give them their usual parameterization \citep{lambert}.
No aster model parameterization
%%%%%%%%%% NEED FORWARD REFERENCE to plethora of parameters %%%%%%%%%%
is this usual parameterization.

\subsection{On Not Overusing Zero-Truncated Poisson}

The zero-truncated Poisson distribution is for when \emph{by definition
of the model} a count of zero cannot occur unless the predecessor is zero.
It is not for the case where expected count is so high
that zeros occur infrequently.
It is for the case where zeros are impossible (except when predecessors
are zero).

Some users have been confused about this, using zero-truncated distributions
where they are not appropriate.

\subsection{Not Really Missing Data}
\label{sec:miss}

The property that predecessor zero implies successor zero
(because a sum having zero terms is zero by convention,
Section~\ref{sec:piss} above)
takes care of what people formerly conceived of as a missing data problem:
when $y_{p(j)} = 0$ (for concreteness, say this means the ``individual''
is dead), you cannot ``observe'' $y_j$.

Nevertheless we can infer $y_j = 0$
(if $y_j$ is flower count, then we are inferring that dead plants have
no flowers; if $y_j$ is survival for the following year, then we are
inferring that dead plants stay dead).

So that is not a true missing data problem from the aster model perspective.
Researchers do need to be aware of the need to code their data this way
(dead individuals have 0 flowers not \code{NA} flowers, \code{NA} being
the R value for missing data).

Many researchers have been confused about this when first introduced to
aster models.  If different individuals live different numbers of years,
don't we need different graphs for individuals to reflect this?  No.
We just have a long enough graph to accommodate all life spans.  After
the individual is dead the data are just zero (not \code{NA}).

If we did have truly missing data (not observable and not inferable),
then we would have a problem that aster models are not equipped to solve.

R function \code{aster} in R package \code{aster} does not allow \code{NA}
values in the data it analyzes.  Its help page says
\begin{quotation}
     It was almost always wrong for aster model data to have \code{NA}
     values. Although theoretically possible for the R formula
     mini-language to do the right thing for an aster model with \code{NA}
     values in the data, usually it does some wrong thing.  Thus, since
     version 0.8-20, this function and the \code{reaster} function give
     errors when used with data having \code{NA} values.  Users must remove
     all \code{NA} values (or replace them with what they should be, perhaps
     zero values) ``by hand''.
\end{quotation}
R function \code{asterdata} in R package \code{aster2} also does not
allow \code{NA} values in aster data for analyses done by that package.

As we shall see (Section~\ref{sec:aster-expfam} below)
the property that predecessor zero implies successor zero
makes likelihood inference automatically do the Right Thing.
We do not have to go into contortions to get it to do the Right Thing.
It just does the Right Thing automatically.

\subsection{Bernoulli versus Multinomial}

Bernoulli arrows and multinomial dependence groups are closely related but
work differently.  Bernoulli is related to multinomial but different.

In \eqref{gr:multi} every arrow labeled with an $\mathcal{M}$ is Bernoulli
marginally, but the whole point of dependence groups is that the components
of the response vector in a dependence group are not
conditionally independent given their predecessors, unlike the Bernoulli
arrows labeled Ber in \eqref{gr:multi} or in any of the graphs in this
section (by Lemma~\ref{lem:markov}).

Conversely, if
$$
\begin{CD}
   y_i @>\text{Ber}>> y_j
\end{CD}
$$
is a Bernoulli arrow, then we could replace this with a multinomial dependence
group that does the same thing
$$
\begin{tikzcd}
  \hphantom{1} & y_k
  \\
  y_i
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_j
  \arrow[dash]{u}
\end{tikzcd}
$$
where $k$ is some index that hasn't been used in the rest of the graph.

For example, we could change graph \eqref{gr:aster1} to
\begin{equation} \label{gr:aster1-too}
\begin{tikzcd}
  \hphantom{1} & y_{10} & y_{11} & y_{12}
  \\
  1
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_1
  \arrow[dash]{u}
  \arrow{d}{\text{Ber}}
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_2
  \arrow[dash]{u}
  \arrow{d}{\text{Ber}}
  \arrow{r}{\mathcal{M}}
  \arrow{ru}{\mathcal{M}}
  & y_3
  \arrow[dash]{u}
  \arrow{d}{\text{Ber}}
  \\
  \hphantom{1} & y_4 \arrow{d}{\text{0-Poi}}
  & y_5 \arrow{d}{\text{0-Poi}}
  & y_6 \arrow{d}{\text{0-Poi}}
  \\
  \hphantom{1} & y_7 & y_8 & y_9
\end{tikzcd}
\end{equation}
and all of the components of the response vector that have the same indices
would have the same interpretation and the same values in the same data
in both graphs \eqref{gr:aster1} and \eqref{gr:aster1-too}.
Then the additional nodes $y_{10}$, $y_{11}$, and $y_{12}$ are determined
by the properties of the multinomial distribution
and the predecessor-is-sample size property (Section~\ref{sec:piss} above)
\begin{equation} \label{eq:bernoulli-multinomial-constraints}
\begin{split}
   y_{10} & = 1 - y_1
   \\
   y_{11} & = y_1 (1 - y_2)
   \\
   y_{12} & = y_2 (1 - y_3)
\end{split}
\end{equation}
The aster models with graphs \eqref{gr:aster1} and \eqref{gr:aster1-too}
can be made equivalent if parameterized to make that so (I think, no proof
%%%%%%%%%% NEED FORWARD REFERENCE to directions of constancy %%%%%%%%%%
here), but they don't have to be equivalent (I think).
Hence scientists have to decide which to use.  The argument of simplicity
argues for \eqref{gr:aster1} (no dependence groups).
But arguments can be made both ways.

Not only can we replace Bernoulli arrows with multinomial dependence groups,
we can, at least partially replace multinomial dependence groups with Bernoulli
arrows.  We could replace the graph \eqref{gr:multi} with the graph
shown in Figure~\ref{fig:gr:multi-too} (p.~\pageref{fig:gr:multi-too}).
\begin{sidewaysfigure}
\begin{equation*}
\begin{tikzcd}
  % \hphantom{1} & y_1 & y_4 & y_7 & y_{10}
  % \\
  1
  \arrow{r}{\text{Ber}}
  & y_1
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_2
  \arrow{r}{\text{Ber}}
  \arrow[dash]{d}
  & y_4
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_5
  \arrow[dash]{d}
  \arrow{r}{\text{Ber}}
  & y_7
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_8
  \arrow[dash]{d}
  \arrow{r}{\text{Ber}}
  & y_{10}
  \arrow{r}{\mathcal{M}}
  \arrow{rd}{\mathcal{M}}
  & y_{11}
  \arrow[dash]{d}
  \\
  \hphantom{1} & \hphantom{y_1}
  & y_3 \arrow{d}{\text{Ber}}
  & \hphantom{y_4}
  & y_6 \arrow{d}{\text{Ber}}
  & \hphantom{y_7}
  & y_9 \arrow{d}{\text{Ber}}
  & \hphantom{y_{10}}
  & y_{12} \arrow{d}{\text{Ber}}
  \\
  \hphantom{1} & \hphantom{y_1}
  & y_{13} \arrow{d}{\text{0-Poi}}
  & \hphantom{y_4}
  & y_{14} \arrow{d}{\text{0-Poi}}
  & \hphantom{y_7}
  & y_{15} \arrow{d}{\text{0-Poi}}
  & \hphantom{y_{10}}
  & y_{16} \arrow{d}{\text{0-Poi}}
  \\
  \hphantom{1} & \hphantom{y_1}
  & y_{17} 
  & \hphantom{y_4}
  & y_{18}
  & \hphantom{y_7}
  & y_{19}
  & \hphantom{y_{10}}
  & y_{20}
\end{tikzcd}
\end{equation*}
\caption{Alternative to Graph \eqref{gr:multi}.}
\label{fig:gr:multi-too}
\end{sidewaysfigure}
In graph \eqref{gr:multi} and the graph in Figure~\ref{fig:gr:multi-too}
all of the components of the response vector that have the same indices
have the same interpretation and the same values in the same data \emph{except}
for $y_1$,  $y_4$,  $y_7$, and $y_{10}$ which have the \emph{opposite}
interpretation and opposite values in the two graphs.
In graph \eqref{gr:multi} the value one
for $y_1$,  $y_4$,  $y_7$, and $y_{10}$ means \emph{dead}.
In the graph in Figure~\ref{fig:gr:multi-too} the value one
for these variables means \emph{alive}.

The aster models with the graph \eqref{gr:multi} and the graph
in Figure~\ref{fig:gr:multi-too} cannot (I think) be made equivalent
by some choice of parameterization.
Hence scientists have to decide which to use.  Now the argument of simplicity
seems to argue for \eqref{gr:multi}.  In either case we have dependence groups
(we must have to track life history stages).  So as long as we have to have
dependence groups, we might as well make them do as much work as possible,
tracking mortality as well as progress through life history stages.
But arguments can be made both ways.

\subsection{Bernoulli versus Bernoulli}

Any composition of Bernoulli arrows is another Bernoulli.  In the following
graph
\begin{equation} \label{gr:one-bernoulli-after-another}
\begin{CD}
   y_1 @>\text{Ber}>> y_2 @>\text{Ber}>> y_3 @>\text{Ber}>> y_4
\end{CD}
\end{equation}
the conditional distribution of $y_4$ given $y_1 = 1$ is Bernoulli.
If $y_1 = 1$, then $y_2$ is either zero or one, hence so is $y_3$,
hence so is $y_4$, and any zero-or-one-valued random variable is Bernoulli.

Hence, we can replace the subgraph \eqref{gr:one-bernoulli-after-another}
by the single arrow
\begin{equation} \label{gr:one-bernoulli}
\begin{CD}
   y_1 @>\text{Ber}>> y_4
\end{CD}
\end{equation}
(deleting the components $y_2$ and $y_3$ from the response vector).

And this change-of-data gives a model in which the conditional distribution
of $y_4$ given $y_1 = 1$ is Bernoulli in either case.

But when these are subgraphs of a larger graph,
the corresponding aster models 
cannot (I think) be made equivalent
by some choice of parameterization.
Hence scientists have to decide which to use.

Of course, had \eqref{gr:one-bernoulli-after-another} been different
\begin{equation*}
\begin{CD}
   y_1 @>\text{Ber}>> y_2 @>\text{Ber}>> y_3 @>\text{Ber}>> y_4
   \\
   @.
   @VVV
   \\
   \hphantom{y_1}
   @.
   \vdots
\end{CD}
\end{equation*}
so $y_2$ had a successor other than $y_3$ (or similarly for $y_3$)
then $y_2$ and $y_3$ could not have been removed from the data.

The observation of this section is only for when we have a straight line
of Bernoullis like \eqref{gr:one-bernoulli-after-another} with no branches.
But this case is seen surprisingly often in published aster analyses.
Again, scientists have to decide which to use.

\subsection{Thinned Poisson}

A Poisson followed by a Bernoulli is Poisson (this is well known in spatial
statistics).  In the graph
\begin{equation} \label{gr:poisson-bernoulli}
\begin{CD}
   y_1 @>\text{Poi}>> y_2 @>\text{Ber}>> y_3
\end{CD}
\end{equation}
the conditional distribution of $y_3$ given $y_1 = 1$ is Poisson.  Thus this
graph can be replaced by the single arrow
\begin{equation} \label{gr:poisson}
\begin{CD}
   y_1 @>\text{Poi}>> y_3
\end{CD}
\end{equation}
(deleting the component $y_2$ from the response vector),

And this change-of-data gives a model in which the conditional distribution
of $y_4$ given $y_1 = 1$ is Poisson in either case.
Again, scientists have to decide which to use.

There are, as far as I can see no general principles for which aster graph
is the one and only Right Thing for any particular data.  Multiple different
aster models may have some rationale supporting them.

\section{Exponential Families of Distributions}

This is a brief overview of the theory
of exponential families of distributions,
just enough to allow us to finish the basic theory of aster models.
We will do some more exponential family later as the need arises.

\subsection{Definition}
\label{sec:define-expfam}

The usual definition of exponential families of distributions
(\citealp[Chapter~8]{barndorff-nielsen}; \citealp[Chapter~1]{brown})
involves probability mass-density functions (or measure-theoretic probability
density functions with respect to an arbitrary positive measure).
Here we give a simpler definition from \citet{geyer-gdor}.

A statistical model is a family of probability distributions.
A statistical model is an \emph{exponential family of distributions} if it has
a log likelihood of the form
\begin{equation} \label{eq:logl-expfam}
   l(\theta) = \inner{y, \theta} - c(\theta)
\end{equation}
where
\begin{itemize}
\item $y$ is a vector-valued statistic, which is called
   the \emph{canonical statistic},
\item $\theta$ is a vector-valued parameter, which is called
   the \emph{canonical parameter},
\item $c$ is a real-valued function, which is called
   the \emph{cumulant function}, and
\item $\inner{\fatdot, \fatdot}$ is a bilinear form that places
   the vector space where $y$ takes values and the vector
   space where $\theta$ takes values in duality.
\end{itemize}

In equation \eqref{eq:logl-expfam} we have used the rule that additive terms
in the log likelihood that do not contain the parameter may be dropped.
Such terms have been dropped in \eqref{eq:logl-expfam}.

In aster model theory, we always have
$$
   \inner{y, \theta} = \sum_{j \in J} y_j \theta_j
$$
where $J$ is the common index set for $y$ and $\theta$
so both are vectors in $\real^J$.  But the angle brackets notation,
which comes from functional analysis \citep{rudin}, is used to indicate
that we don't think of the the vector space where $y$ takes values
and the vector space where $\theta$ takes values as being the same.
They are dual vector spaces, not the same.

This means that $\inner{y, y}$ or $\inner{\theta, \theta}$ are
glaring errors, unlike what would be the case if we said that
$\inner{\fatdot, \fatdot}$ was an inner product on $\real^J$ or some such.
In any event, your humble author has been using this notation since his
thesis \citep{geyer-thesis} and is not going to stop now.
Moreover, most aster papers also use this notation (if they discuss
aster theory at all).

Although we usually say ``the'' canonical statistic,
``the'' canonical parameter, and ``the'' cumulant function,
these are not uniquely defined:
\begin{itemize}
\item any one-to-one affine function of a canonical statistic vector is another
   canonical statistic vector,
\item any one-to-one affine function of a canonical parameter vector is another
   canonical parameter vector, and
\item any real-valued affine function plus a cumulant function is another
   cumulant function.
\end{itemize}
(Affine functions are defined in Section~\ref{sec:canonical-affine-submodel}
below.)
These possible changes of statistic, parameter, or cumulant function are not
algebraically independent.  Changes to one may require changes to the others
to keep a log likelihood of the form \eqref{eq:logl-expfam} above.

Usually no fuss is made about this nonuniqueness.  One fixes a choice
of canonical statistic, canonical parameter, and cumulant function
and leaves it at that.

The cumulant function may not be defined by \eqref{eq:logl-expfam}
on the whole vector space where $\theta$ takes values.  In that case
it can be extended to this whole vector space by
\begin{equation} \label{eq:cumfun-expfam}
   c(\theta) = c(\psi) + \log\left\{
   E_\psi\bigl( e^{\inner{y, \theta - \psi}} \bigr) \right\}
\end{equation}
where $\theta$ varies while $\psi$ is fixed at
a possible value of the canonical parameter vector, and
the expectation and hence $c(\theta)$ are assigned
the value $\infty$ for $\theta$ such that the expectation does not exist.

The family is \emph{full} if its canonical parameter space is
\begin{equation} \label{eq:full-expfam}
   \Theta = \{\, \theta : c(\theta) < \infty \,\}
\end{equation}
and a full family is \emph{regular} if its canonical parameter space is an open
subset of the vector space where $\theta$ takes values.

Almost all exponential families used in real applications are full and regular.
So-called \emph{curved exponential families} (smooth non-affine submodels of
full exponential families) are not full.
Constrained exponential families \citep{geyer-constrained} are not full.
A few exponential families used in spatial statistics are full but not regular
\citep{geyer-moller}.

Many people use ``natural'' everywhere this book uses ``canonical.''
In this we are following \citet{barndorff-nielsen}.
It also goes with our policy of avoiding terminology used in biology
if alternatives are available.

Many people also use an older terminology that says a statistical model
is \emph{in the} exponential family, where we say a statistical model
is \emph{an} exponential family.
Thus the older terminology says \emph{the} exponential
family is the collection of all of what the newer terminology calls
exponential families.  The older terminology names a useless
mathematical object, a heterogeneous collection of statistical models not
used in any application.
The newer terminology names an important property of statistical models.
If a statistical model is a regular full exponential family, then it
has all of the properties discussed here.
If a statistical model is an exponential family (not necessarily full or
regular), then it has many of the properties discussed here.
Presumably, that is the reason for the newer terminology.
In this we are again following \citet{barndorff-nielsen}.

\subsection{Independent and Identically Distributed}
\label{sec:iid}

Suppose we have an exponential family with vector canonical statistic $z$,
vector canonical parameter $\theta$, and log likelihood
$$
   l(\theta) = \inner{z, \theta} - c(\theta)
$$
And suppose we have an IID sample
from this family with log likelihood
\begin{equation} \label{eq:iid}
\begin{split}
   l_n(\theta)
   & =
   \sum_{i = 1}^n \left[ \inner{z_i, \theta} - c(\theta) \right]
   \\
   & =
   \left\langle \sum_{i = 1}^n z_i, \theta \right\rangle - n c(\theta)
\end{split}
\end{equation}
This tells us that IID sampling from an exponential family gives another
exponential family with
\begin{itemize}
\item canonical statistic vector that is the sum of the canonical statistic
    vectors for the elements of the sample,
\item the same canonical parameter vector, and
\item cumulant function that is $n$ times
    the cumulant function for the elements of the sample.
\end{itemize}

A lot of ``addition rules'' for ``brand name distributions''
are special cases.  For example, sum of IID Bernoulli is binomial, sum of
IID Poisson is Poisson, sum of IID exponential is gamma.

The simple math in this section is important for aster models.
The predecessor-is-sample-size property (Section~\ref{sec:piss} above)
requires us to know
$n$-fold convolutions of the distributions associated with arrows at least
well enough to write down the log likelihood.  This section tells how that
is done.

\subsection{Canonical Affine Submodels}
\label{sec:canonical-affine-submodel}

In this section we consider \emph{canonical affine submodels} of exponential
families.  If $\theta$ is the canonical parameter vector, then these
submodel parameterizations have the form
\begin{equation} \label{eq:affine}
   \theta = a + M \beta,
\end{equation}
where $a$ is a known vector and $M$ is a known matrix; $a$ is called the
\emph{offset} vector and $M$ is called the \emph{model matrix} in the
terminology of R functions \texttt{lm} and \texttt{glm}.
$M$ is called the \emph{design matrix} by others.
We use the terminology favored by R.
The submodel parameter vector $\beta$ is called the \emph{coefficients}
vector by R.  We will find another name for it in this section.

The term ``canonical affine submodel'' was introduced by \citet{aster1}.
Usually these are called linear models or generalized linear models or
log-linear models in various settings.

There are two notions of linear used in mathematics.  There is a
sharp dividing line at the beginning of linear algebra.  In calculus
and lower level mathematics (including pre-college) a linear function
is one with a flat graph.  In linear algebra and all higher level mathematics
a linear function is one that preserves the vector space operations.
In particular, if $f$ is a linear function in this higher-level sense,
then $f(0) = 0$.  If one needs the lower-level sense in higher-level
mathematics, it is called an affine function.  An affine function
between vector spaces is
a linear function plus a constant function.  If $a \neq 0$
in \eqref{eq:affine}, then this is a linear change-of-parameter in the
lower-level sense but an affine change-of-parameter in the higher-level sense.
It is unclear (to me) whether the ``linear'' in linear models, generalized
linear models, and log linear models is the lower-level sense allowing
$a \neq 0$ in \eqref{eq:affine} or the higher-level sense assuming $a = 0$
in \eqref{eq:affine}, because $a = 0$ in most applications.
We follow \citet{aster1} in calling these models affine.

The log likelihood for the canonical affine submodel is
\begin{align*}
   l(\beta)
   & =
   \inner{y, a + M \beta} - c(a + M \beta)
   \\
   & =
   \inner{y, a} + \inner{y, M \beta} - c(a + M \beta)
\end{align*}
and the term $\inner{y, a}$ that does not contain the parameter $\beta$
can be dropped from the log likelihood giving
$$
   l(\beta)
   =
   \inner{y, M \beta} - c(a + M \beta)
$$
and, because
$$
   \inner{y, M \beta} = y^T (M \beta) = y^T M \beta = (M^T y)^T \beta
   = \inner{M^T y, \beta}
$$
we can write the submodel log likelihood in exponential family form
\begin{equation} \label{eq:submodel}
   l(\beta)
   =
   \inner{M^T y, \beta} - c_\text{sub}(\beta)
\end{equation}
where
\begin{equation} \label{eq:submodel-cumfun}
   c_\text{sub}(\beta)
   =
   c(a + M \beta), \qquad \text{for all $\beta$}.
\end{equation}
This shows the canonical affine submodel is itself an exponential family with
\begin{itemize}
\item canonical statistic vector $M^T y$,
\item canonical parameter vector $\beta$, and
\item cumulant function $c_\text{sub}$.
\end{itemize}

\subsection{Moment and Cumulant Generating Functions}

The \emph{moment generating function} (MGF) of a random vector $y$ is given by
$$
   M(t) = E\left(e^{\inner{y, t}}\right)
$$
provided that the function $M$ so defined is finite on a neighborhood of
zero; otherwise we say the random vector $y$ does not have an MGF.
Clearly, the MGF only depends on the distribution of $y$,
so we also this is the MGF of this distribution.

The MGF of the distribution of the canonical statistic of an exponential
family corresponding to canonical parameter $\theta$ is given by
\begin{align*}
   M_\theta(t)
   & =
   E_\theta\left(e^{\inner{y, t}}\right)
\end{align*}
We rearrange \eqref{eq:cumfun-expfam} obtaining
$$
   e^{c(\theta) - c(\psi)}
   =
   E_\psi\bigl( e^{\inner{y, \theta - \psi}} \bigr)
$$
and then change $\theta$ to $\theta + t$ and $\psi$ to $\theta$ in that order
obtaining
$$
   e^{c(\theta + t) - c(\theta)}
   =
   E_\theta\bigl( e^{\inner{y, t}} \bigr)
$$
so the MGF of the distribution of $y$ for parameter vector $\theta$ is
$$
   M_\theta(t)
   =
   e^{c(\theta + t) - c(\theta)}
$$
provided $M_\theta$ is finite on a neighborhood of zero,
which is when $c$ is finite on a neighborhood of $\theta$,
which is when $\theta$ is in the interior of the full canonical
parameter space \eqref{eq:full-expfam}.
Hence every distribution in a \emph{regular} full exponential
family has an MGF.

The \emph{cumulant generating function} (CGF) of a random vector $y$ is
the log of the MGF provided the MGF exists.
If a distribution has no MGF, then it has no CGF either.
Hence the CGF of the distribution of $y$ for parameter vector $\theta$ is
$$
   K_\theta(t)
   =
   c(\theta + t) - c(\theta)
$$
provided $K_\theta$ is finite on a neighborhood of zero,
which is when $\theta$ is in the interior of the full canonical
parameter space \eqref{eq:full-expfam}.
Hence every distribution in a \emph{regular} full exponential
family has a CGF.

The MGF is so called because its derivatives evaluated at zero give moments.
The CGF is so called because its derivatives evaluated at zero give cumulants.
Cumulants are polynomial functions of moments and vice versa.
Thus a distribution that has an MGF or a CGF has moments and cumulants of
all orders.
But we will not use moments and cumulants higher than second order
(next section) in our study of aster models.

Observe that derivatives of $K_\theta$ evaluated at zero are derivatives
of $c$ evaluated at $\theta$.  Hence the cumulant function is so called
because its derivatives evaluated at $\theta$ give cumulants.
(Of course, this is only valid when $\theta$ is in the full canonical
parameter space \eqref{eq:full-expfam}).

This also shows that cumulant functions are infinitely differentiable
at every point in the interior of the full canonical
parameter space \eqref{eq:full-expfam}), and
cumulant functions of a regular full exponential are infinitely differentiable
at every point of the full canonical
parameter space \eqref{eq:full-expfam}).

\subsection{Mean and Variance}

First and second order cumulants are mean and variance
\begin{subequations}
\begin{align}
   E_\theta(y) & = c'(\theta)
   \label{eq:cumulant-one}
   \\
   \var_\theta(y) & = c''(\theta)
   \label{eq:cumulant-two}
\end{align}
\end{subequations}
\citep[Theorem~8.1]{barndorff-nielsen}.
Of course, the formulas are valid only where $c$ is differentiable,
which is when $\theta$ is in the interior of the full canonical
parameter space \eqref{eq:full-expfam}.
On the boundary of the full canonical parameter space, the derivatives
on the right-hand sides of these formulas do not exist and the cumulants
on the left-hand sides might or might not exist but in any case are not
given by these formulas.
In a \emph{regular} full exponential family these formulas are good
for all values of $\theta$ in the full canonical
parameter space \eqref{eq:full-expfam}.

In \eqref{eq:cumulant-one} $y$ is a vector and its expectation is a vector
having the same index set: $\mu = E_\theta(y)$ is the vector having components
$\mu_j = E_\theta(y_j)$.
The right-hand side of \eqref{eq:cumulant-one} must also be a vector having
the same index set: its components are $\partial c(\theta) / \partial \theta_j$.

In \eqref{eq:cumulant-two} $y$ is a vector and its variance is a square
symmetric matrix having components $\cov_\theta(y_j, y_k)$.
The right-hand side of \eqref{eq:cumulant-two} must also be a matrix having
the same index set: its components are
$\partial^2 c(\theta) / \partial \theta_j \partial \theta_k$.

Many people do not like our terminology calling the left-hand side
of \eqref{eq:cumulant-two} the \emph{variance} matrix of $y$.
Names in common use are \emph{covariance} matrix (but this is horrible
terminology because it uses up a name that should be used for the covariance
of two random vectors), \emph{variance-covariance} matrix,
and \emph{dispersion} matrix.
We use our terminology, because it denotes the multivariate analog
of the variance of a random variable.
This can be seen everywhere in probability theory.
If you can accept our notation $\var_\theta(y)$ for this mathematical
object, then the same formulas work for univariate and multivariate $y$.

\section{Aster Models and Exponential Families}
\label{sec:aster-expfam}

We finally get to the final axiom of aster models.
Each conditional distribution in the factorization \eqref{eq:factorize}
is actually stands for parametric family of distributions,
and each of these is an exponential family of distributions.
As explained in Section~\ref{sec:piss} above,
the distribution for these conditional families is determined
by the distribution for sample size one, and the distributions
for dependence group $G$ for sample size one form an exponential family having
\begin{itemize}
\item canonical statistic $y_G$,
\item canonical parameter $\theta_G$, and
\item cumulant function $c_G$.
\end{itemize}
This notation means that $\theta$ is a vector with the same index set as
the response vector $y$ and that $\theta_G$ are subvectors of $\theta$
in the same way as $y_G$ are subvectors of $y$.
The fact that the cumulant functions are subscripted means every
dependence group may correspond to a different exponential family,
and we saw this in the examples (Section~\ref{sec:graphs} above).

Now Section~\ref{sec:iid} above tells us the cumulant function for sample
size $n$ is $n$ times the cumulant function for sample size one.
Note that this is valid for $n = 1$ and $n = 0$ too.
For the latter we calculate using \eqref{eq:cumfun-expfam} in the case
that $y$ is the random vector concentrated at zero
$$
   c(\theta) = c(\psi) + \log\left\{ E_\psi(1) \right\}
   = c(\psi) + \log(1)
   = c(\psi)
$$
so the cumulant function of the random vector concentrated at zero is
a constant function ($c(\psi)$ is arbitrary but $\psi$ is a constant
in this equation),
and to obtain agreement with the assertion that the cumulant function for
sample size $n$ is $n$ times the cumulant function for sample size one,
we take this arbitrary constant to be zero, so the cumulant function for
sample size zero is always the zero function, which always has the value zero.

Now the predecessor-is-sample-size property says that the sample size
for dependence group $G$ is $y_{q(G)}$, so the cumulant function for this
sample size is $y_{q(G)}$ times the cumulant function for sample size one.

Because the log of a product is the sum of the logs, the log likelihood
for an aster model is the sum of terms, one term for each term in the
factorization \eqref{eq:factorize}, and each term looks like
\eqref{eq:logl-expfam} except with subscripts for the dependence groups,
that is,
\begin{equation} \label{eq:logl-aster-theta}
   l(\theta)
   =
   \sum_{G \in \mathcal{G}}
   \left[ \inner{y_G, \theta_G} - y_{q(G)} c_G(\theta_G) \right]
\end{equation}
(by the predecessor-is-sample-size property, the conditional distribution
of $y_G$ given $y_{q(G)}$ is the sum of IID random vectors having cumulant
function $c_G$, hence the cumulant function in each term is $y_{q(G)}$
times the cumulant function for sample size one).

As was mentioned in Section~\ref{sec:miss} above, probability theory
just does the Right Thing in case $y_{q(G} = 0$.
By the predecessor-is-sample-size property, this implies $y_G = 0$ too,
so the whole contribution to the log likelihood of the term for dependence
$G$ is zero (when, as we are discussing here, the predecessor is zero).
And zero is the correct contribution because the conditional distribution
of $y_G$ given $y_{q(G)} = 0$ is concentrated at zero,
that is, it is zero with probability one, so the contribution to the
log likelihood should be $\log(1) = 0$.

So there is no need to do anything special about the case where predecessor
equals zero.  All of our formulas are correct in this case too.

\section{Infinitely Divisible Aster Families}
\label{sec:infinitely-divisible}

As stated in Section~\ref{sec:piss} above,
one consequence of the predecessor-is-sample-size property is that
components of the response vector
that are predecessors must be
nonnegative-integer-valued random variables.

Except \citet{aster1} note an exception to this rule.  If the distribution
in question is infinitely divisible, then $r$-fold convolution makes sense
for any $r \ge 0$.  A distribution having moment generating function $m$
is infinitely divisible if and only if $m(\fatdot)^r$ is a moment generating
function for all real $r \ge 0$ \citep[Corollary~4.2.2]{cuppens}.
Then the distribution having moment
generating function $m(\fatdot)^r$ is defined to be the $r$-fold convolution
of the distribution having moment generating function $m$.
Hence a regular full exponential family having cumulant function $c$
is infinitely divisible if and only if $r c(\fatdot)$ is a cumulant
function for all real $r \ge 0$.
R package \code{aster} \citep{aster-package} has several infinitely divisible
families: Poisson, negative binomial, and normal-location
(normal with unknown mean and known variance).
Hence, if the conditional distribution of $y_G$ given $y_{q(G)}$
is infinitely divisible, then
the distribution of $y_{q(G)}$ can be nonnegative-real-valued.

This observation, however, has played no role in applications of aster models
because R packages \code{aster} and \code{aster2} do not implement any
families of nonnegative-valued random variables that are not
also integer-valued.  Such families exist (the gamma distribution,
for example), so if they were implemented, there could be a role for this
observation.

\section{The Aster Transform}
\label{sec:aster-transform}

The aster log likelihood \eqref{eq:logl-aster-theta} does not have
exponential family form \eqref{eq:logl-expfam}.
But since \eqref{eq:logl-aster-theta} is linear in $y$, it must be an
exponential family having canonical statistic vector $y$.  In order
to figure out the canonical parameter and cumulant function for this family
we rewrite \eqref{eq:logl-aster-theta} as follows
\begin{equation} \label{eq:logl-rewrite}
   l(\theta)
   =
   \left(
   \sum_{j \in J} y_j
   \left[ \theta_j - \sum_{\substack{G \in \mathcal{G} \\ q(G) = j}}
   c_G(\theta_G) \right]
   \right)
   -
   \left(
   \sum_{\substack{G \in \mathcal{G} \\ q(G) \notin J}}
   y_{q(G)} c_G(\theta_G) \right)
\end{equation}
and now we see we do have exponential family form with the canonical
parameters (of the exponential family which is the joint distribution of $y$)
being the terms in square brackets (the multipliers of the components of $y$)
\begin{equation} \label{eq:aster-transform}
   \varphi_j
   =
   \theta_j - \sum_{\substack{G \in \mathcal{G} \\ q(G) = j}} c_G(\theta_G),
   \qquad j \in J,
\end{equation}
and the cumulant function being the terms left over
\begin{equation} \label{eq:aster-cumfun}
   c(\varphi)
   =
   \sum_{\substack{G \in \mathcal{G} \\ q(G) \notin J}} y_{q(G)} c_G(\theta_G)
\end{equation}
(the fact that we have $\varphi$ on one side of the equation and $\theta$
on the other will be explained presently).
Note that all of the $y_{q(G)}$ appearing in \eqref{eq:aster-cumfun}
are constant random variables (because they are at initial nodes).
Thus \eqref{eq:aster-cumfun} does define a deterministic (non-random)
function of the parameter, as a cumulant function must be.

This notation means that $\varphi$ is a vector with the same index set as
the response vector $y$, that $\varphi_G$ are subvectors of $\varphi$
in the same way as $y_G$ are subvectors of $y$, and
that $\varphi_j$ are components of $\varphi$
in the same way as $y_j$ are components of $y$.

The map $\theta \mapsto \varphi$ is called the \emph{aster transform}.
We claim this parameter transformation is invertible and both the
transform and its inverse are infinitely differentiable.
We invert the function very simply moving terms from the right-hand
side to the left-hand side
\begin{equation} \label{eq:inverse-aster-transform}
   \theta_j
   =
   \varphi_j + \sum_{\substack{G \in \mathcal{G} \\ q(G) = j}} c_G(\theta_G),
   \qquad j \in J.
\end{equation}
How can this define $\theta$ in terms of $\varphi$ when components of $\theta$
appear on both sides of the equation?
Simply use the equations \eqref{eq:inverse-aster-transform} in any order
that visits successors before predecessors.  Theorem~\ref{th:factorize}
guarantees there is such an order (it guarantees an order among dependence
groups, but we can order components within a dependence group arbitrarily).
Then when we are using \eqref{eq:inverse-aster-transform} to compute
$\theta_j$ we will have already have computed $\theta_k$ for all $k$ that
are successors, successors of successors, and so forth of $j$, and,
in particular, we will already have computed $\theta_G$ for all $G$ such
that $q(G) = j$.

The map $\varphi \mapsto \theta$ is called the \emph{inverse aster transform}.
The aster transform is clearly infinitely differentiable because cumulant
functions are infinitely differentiable (when all of the families for
the dependence groups are regular full exponential families).
The inverse function theorem of real analysis says the inverse function
is differentiable as many times as the function it is the inverse of.
Hence the inverse aster transform is also infinitely differentiable.

Now we see why it is OK for \eqref{eq:aster-cumfun} to have $\varphi$
on the left-hand side and $\theta$ on the right-hand side: $\theta$
is given as a function of $\varphi$ by the inverse aster transform
so the right-hand side of \eqref{eq:aster-cumfun} can be considered
a function of $\varphi$.

Now we can express \eqref{eq:logl-rewrite} in exponential family form
\begin{equation} \label{eq:logl-aster-phi}
   l(\varphi) = \inner{y, \varphi} - c(\varphi)
\end{equation}
where the cumulant function $c$ of the joint distribution of the aster
model is \eqref{eq:aster-cumfun}.

We call
\begin{itemize}
\item $\theta$ the \emph{conditional canonical parameter vector} and
\item $\varphi$ the \emph{unconditional canonical parameter vector}.
\end{itemize}
But this terminology makes these parameters more like two kinds of the
same sort of thing than they really are.
\begin{itemize}
\item The subvectors $\theta_G$ are the canonical parameter vectors of
    the conditional distributions of the dependence groups, but, as
    we have seen (this is the whole point of the aster transform),
    $\theta$ is not the canonical parameter vector of the (unconditional,
    joint) distribution of the aster model.
\item The vector $\varphi$ is the canonical parameter vector of the
    (unconditional, joint)
    distribution of the aster model, but the subvectors $\varphi_G$ or the
    components $\varphi_j$ are not separately canonical for anything.
\end{itemize}
In short the subvectors of $\theta$ are groupwise canonical but
$\theta$ is not vectorwise canonical, and, conversely,
$\varphi$ is vectorwise canonical, but its subvectors are not groupwise
canonical.

