
\chapter{Markov Properties}
\label{app:markov}

Markov properties of graphical models are considered a fundamental part
of the theory \citep[Section~3.2]{lauritzen}.  They are much less important
for aster models, so unimportant that the literature on aster models
does not mention them.
So perhaps most readers will want to skip this appendix.  Nevertheless,
perhaps these ideas might find some future use.  So we do them.

\begin{lemma} \label{lem:markov}
Let $\mathcal{H}$ be any subset of $\mathcal{G}$.  Then the random vectors
$y_H$, $H \in \mathcal{H}$ are conditionally independent given
the random scalars $y_{q(H)}$, $H \in \mathcal{H}$.
\end{lemma}

Note that some $y_j$ can possibly appear among some $H \in \mathcal{H}$
and in some $y_{q(H)}$, $H \in \mathcal{H}$ so we have to say what that means.
Conditioning on a random variable is the same as treating it as constant,
and a constant random variable is independent of any random variables
including itself.  Thus for any sets $A$ and $B$, we have
\begin{equation} \label{eq:before-and-after}
   \pr(y_A \mid y_B)
   =
   \pr(y_{A \setminus B} \mid y_B).
\end{equation}

In \eqref{eq:before-and-after}, the case $A \setminus B = \emptyset$
is possible,
so we have to say what we mean by the random vector $y_\emptyset$.
Our view of vectors and subvectors as really
being functions (Section~\ref{sec:subvector} above) says that $y_\emptyset$
denotes the only function $\emptyset \to \real$, which is the empty function
(which has the empty graph).  Hence $y_\emptyset$ has only one possible
value (the empty function) and must be a constant random vector.
Thus $\pr(y_\emptyset \mid y_j) = 1$ regardless of what $y_j$ is.

This lemma does not say that the components of the random vectors $y_H$ are
conditionally independent.  The components of $y_G$ are dependent given
$y_{q(G)}$.  That is the whole point of dependence groups.

\begin{proof}
Use the total order on $\mathcal{G}$ guaranteed
to exist by Theorem~\ref{th:factorize}
to enumerate $\mathcal{G}$ as $G_1 < G_2 < \cdots < G_n$.
Then $H_j = G_{i_j}$ for $j = 1$, $\ldots,$ $m$,
where $1 \le i_1 < i_2 < \cdots < i_m \le n$.

We integrate out $y_G$ one at a time
in order skipping when $G \in \mathcal{H}$ and also not integrating out
any $y_{q(H)}$ for $H \in \mathcal{H}$.

We start by sum-integrating out $y_{G_1}$ if $G_1 \neq H_1$ obtaining
$$
   \pr(y_{G_2 \cup \cdots \cup G_n})
   =
   \prod_{i = 2}^n \pr(y_{G_i} \mid y_{q(G_i)}).
$$
and keep going repeating this again and again obtaining
\begin{align*}
   \pr(y_{\set{G \in \mathcal{G} : G \ge H_1}})
   & =
   \prod_{\substack{G \in \mathcal{G} \\ G \ge H_1}}
   \pr(y_G \mid y_{q(G)})
   \\
   & =
   \pr(y_{H_1} \mid y_{q(H_1)})
   \prod_{\substack{G \in \mathcal{G} \\ G > H_1}}
   \pr(y_G \mid y_{q(G)})
\end{align*}
(if $G_1 = H_1$ we haven't done anything yet and this is just the same
factorization as \eqref{eq:factorize} in different notation).

Now we have to be careful with our notation.  Define
$$
   Q = \set{ q(H) : H \in \mathcal{H} }
$$
we need to not sum-integrate out any components of $y_Q$.

If $G_{i_1 + 1} \neq H_2$, then we want to sum-integrate out
$y_{G_{i_1 + 1} \setminus Q}$ obtaining
\begin{multline*}
   \pr(y_{H_1 \cup \{q(H_1)\} \cup \set{G \in \mathcal{G} : G > G_{i_1 + 1}}})
   \\
   =
   \pr(y_{H_1} \mid y_{q(H_1)})
   \pr(y_{G_{i_1 + 1} \cap Q} \mid y_{q(G_{i_1 + 1})})
   \prod_{j = i_1 + 2}^n
   \pr(y_{G_i} \mid y_{q(G_i)})
\end{multline*}
(this uses the discussion of $y_\emptyset$ preceding this proof,
since $G_{i_1 + 1} \cap Q$ may or may not be the empty set).

Continuing this process, we obtain
\begin{multline*}
   \pr(y_{H_1 \cup \{q(H_1)\} \cup \{ G_{i_2}, \ldots, G_n \}})
   \\
   =
   \pr(y_{H_1} \mid y_{q(H_1)})
   \prod_{j = i_1 + 1}^{i_2 - 1}
   \pr(y_{G_j \cap Q} \mid y_{q(G_j)})
   \prod_{j = i_2}^n
   \pr(y_{G_i} \mid y_{q(G_i)}).
\end{multline*}
And we can now see how this process continues
$$
   \pr(y_{H_1 \cup H_2 \cup \cdots \cup H_m \cup Q})
   =
   \prod_{H \in \mathcal{H}}
   \pr(y_H \mid y_{q(H)})
   \prod_{G \in \mathcal{G} \setminus \mathcal{H}}
   \pr(y_{G \cap Q} \mid y_{q(G)})
$$
And now integrating out $y_{H \setminus Q}$ in order gives
\begin{align*}
   \pr(y_Q)
   & =
   \prod_{H \in \mathcal{H}}
   \pr(y_{H \cap Q} \mid y_{q(H)})
   \prod_{G \in \mathcal{G} \setminus \mathcal{H}}
   \pr(y_{G \cap Q} \mid y_{q(G)})
\end{align*}
(Note that every $j \in Q$ appears ``in front of the bar'' in
exactly one of these conditional probabilities because $\mathcal{G}$
is a partition.)
So
\begin{align*}
   \pr(y_{H_1 \cup H_2 \cup \cdots \cup H_m} \mid y_Q)
   & =
   \prod_{H \in \mathcal{H}}
   \frac{ \pr(y_H \mid y_{q(H)}) }{ \pr(y_{H \cap Q} \mid y_{q(H)}) }
   \\
   & =
   \prod_{H \in \mathcal{H}}
   \frac{ \pr(y_{H \cup \{q(H)\}}) }{ \pr(y_{(H \cap Q) \cup \{q(H)\}}) }
   \\
   & =
   \prod_{H \in \mathcal{H}}
   \pr( y_{H \setminus Q} \mid y_{Q} )
   \\
   & =
   \prod_{H \in \mathcal{H}}
   \pr( y_H \mid y_{Q} )
\end{align*}
the last step being \eqref{eq:before-and-after}.
\end{proof}

If $\mathcal{G}$ and $\mathcal{H}$ are partitions of a set $J$,
then we say that $\mathcal{G}$ is \emph{finer} than $\mathcal{H}$
if every element of $\mathcal{G}$ is contained in some element
of $\mathcal{H}$.  We also say that $\mathcal{H}$ is \emph{coarser}
than $\mathcal{G}$ to indicate the same concept.

Clearly, every element of $\mathcal{H}$ is the union of elements
of $\mathcal{G}$ it contains (because $\mathcal{G}$ is a partition).

\begin{theorem} \label{th:markov}
Suppose $\mathcal{G}$ and $q$ are as in \eqref{eq:factorize}
and Theorem~\ref{th:factorize}, and suppose $\mathcal{H}$ is a coarser
partition than $\mathcal{G}$.  Define
\begin{equation} \label{eq:q-markov}
   Q = \set{ q(G) : G \in \mathcal{G} \opand H \in \mathcal{H}
   \opand G \subset H \opand q(G) \notin H }
\end{equation}
then the random vectors $y_H$, $H \in \mathcal{H}$, are conditionally
independent given the random vector $y_Q$.
\end{theorem}

Note that \eqref{eq:before-and-after} is being used in this theorem too.
Some $y_j$ may appear in some $y_H$ and also in $y_Q$.

Also we repeat the comment following the lemma.  The theorem does not
assert conditional independence of the components of $y_H$ for any $H$.
The components of $y_G$ being dependent given $y_{q(G)}$ is the whole
point of dependence groups.

\begin{proof}
We prove this by induction. The induction variable is the partition
$\mathcal{H}$. We start with $\mathcal{H} = \mathcal{G}$.
Then we change $\mathcal{H}$ to coarser and coarser
partitions until we get to the $\mathcal{H}$ in the theorem statement.

The base of the induction is the case $\mathcal{H} = \mathcal{G}$
in which case the lemma and the theorem say the same thing.
So that establishes the base of the induction.

In each induction step we decrease the cardinality of $\mathcal{H}$ by one.
This means we take two elements $H'$ and $H''$ of $\mathcal{H}$ and merge
them to make one element of $\mathcal{H}$ after the induction step,
and all other elements of $\mathcal{H}$ remain
unchanged (in this particular induction step). We need to show that if the
assertion of the theorem is true before the induction step, then it is true
after the induction step, when $\mathcal{H}$ is changed as described.

Let $\mathcal{H}_\text{before}$ denote $\mathcal{H}$ before the induction step
and $\mathcal{H}_\text{after}$ denote $\mathcal{H}$ after
the induction step, so all elements of $\mathcal{H}_\text{before}$
and $\mathcal{H}_\text{after}$ are the same except
\begin{itemize}
\item $\mathcal{H}_\text{before}$ has elements $H'$ and $H''$ which are not
    in $\mathcal{H}_\text{after}$ and
\item $\mathcal{H}_\text{after}$ has the element $H' \cup H''$ which is not
    in $\mathcal{H}_\text{before}$.
\end{itemize}
Let $Q_\text{before}$ denote $Q$ before the induction step and $Q_\text{after}$
denote $Q$ after the induction step, so all of the elements
of $Q_\text{before}$ and $Q_\text{after}$ are the same except
\begin{itemize}
\item $q(G)$, $G \in \mathcal{G}$ such that $G \subset H' \cup H''$
    and $q(G) \in H' \cup H''$ are not in $Q_\text{after}$.
    (Some of these may not have been in $Q_\text{before}$ either.)
\end{itemize}

In case $Q_\text{before} = Q_\text{after}$ there is nothing to prove.
Conditional independence of $y_H$, $H \in \mathcal{H}_\text{before}$
given $y_{Q_\text{before}}$ clearly implies conditional independence
of $y_H$, $H \in \mathcal{H}_\text{after}$
given $y_{Q_\text{after}}$ in this case
where $Q_\text{before} = Q_\text{after}$.
The latter statement just forgets part of the assertion of the former.
(It forgets about conditional independence of $y_{H'}$ and $y_{H''}$.)

In case $Q_\text{before} \neq Q_\text{after}$ there is more work to be done.
The induction hypothesis says
$$
   \pr(y \mid y_{Q_\text{before}})
   =
   \prod_{H \in \mathcal{H}_\text{before}}
   \pr(y_H \mid y_{Q_\text{before}}).
$$
First we notice
\begin{equation} \label{eq:notice-too-before-after}
   Q_\text{before} \setminus Q_\text{after} \subset H' \cup H''
\end{equation}
so by the induction hypothesis
\begin{equation} \label{eq:notice-before-after}
   \pr(y_H \mid y_{Q_\text{before}})
   =
   \pr(y_H \mid y_{Q_\text{after}}),
   \qquad
   H \in \mathcal{H}_\text{before} \cap \mathcal{H}_\text{after}.
\end{equation}
Now
\begin{multline*}
   \pr(y_{H'} \mid y_{Q_\text{before}})
   \pr(y_{H''} \mid y_{Q_\text{before}})
   \pr(y_{Q_\text{before}})
   \\
   =
   \pr(y_{H' \cup H''} \mid y_{Q_\text{before}})
   \pr(y_{Q_\text{before}})
   \\
   =
   \pr(y_{H' \cup H'' \cup Q_\text{before}})
   \\
   =
   \pr(y_{H' \cup H'' \cup Q_\text{after}})
\end{multline*}
the first equality being the conditional independence asserted
by the induction hypothesis and the last equality being
\eqref{eq:notice-too-before-after}.  So
\begin{align*}
   \pr(y_{H' \cup H''} \mid  y_{Q_\text{after}})
   & =
   \frac{\pr(y_{H' \cup H'' \cup Q_\text{after}})}{\pr(y_{Q_\text{after}})}
   \\
   & =
   \pr(y_{H'} \mid y_{Q_\text{before}})
   \pr(y_{H''} \mid y_{Q_\text{before}})
   \frac{\pr(y_{Q_\text{before}})}{\pr(y_{Q_\text{after}})}
   \\
   & =
   \pr(y_{H'} \mid y_{Q_\text{before}})
   \pr(y_{H''} \mid y_{Q_\text{before}})
   \pr(y_{Q_\text{before}} \mid y_{Q_\text{after}})
\end{align*}
By a similar argument we have
\begin{align*}
   \pr(y \mid  y_{Q_\text{after}})
   & =
   \pr(y_{Q_\text{before}} \mid y_{Q_\text{after}})
   \prod_{H \in \mathcal{H}_\text{before}}
   \pr(y_H \mid y_{Q_\text{before}})
   \\
   & =
   \pr(y_{H'} \mid y_{Q_\text{before}})
   \pr(y_{H''} \mid y_{Q_\text{before}})
   \pr(y_{Q_\text{before}} \mid y_{Q_\text{after}})
   \\
   & \qquad
   \times
   \prod_{H \in \mathcal{H}_\text{before} \cap \mathcal{H}_\text{after}}
   \pr(y_H \mid y_{Q_\text{before}})
   \\
   & =
   \pr(y_{H' \cup H''} \mid  y_{Q_\text{after}})
   \prod_{H \in \mathcal{H}_\text{before} \cap \mathcal{H}_\text{after}}
   \pr(y_H \mid y_{Q_\text{before}})
   \\
   & =
   \pr(y_{H' \cup H''} \mid  y_{Q_\text{after}})
   \prod_{H \in \mathcal{H}_\text{before} \cap \mathcal{H}_\text{after}}
   \pr(y_H \mid y_{Q_\text{after}})
   \\
   & =
   \prod_{H \in \mathcal{H}_\text{after}}
   \pr(y_H \mid y_{Q_\text{after}})
\end{align*}
where the next-to-last step is \eqref{eq:notice-before-after}.
And reading from end to end gives the assertion that the induction step
must prove.  Hence we are done.
\end{proof}

